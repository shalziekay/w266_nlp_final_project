{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PRO 4-6 DistilBERT_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d5d208ea029a44a9a10593186527b550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_357e949f52074868bc743123d1d4af25",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_280c4d597dbe4a2db6dc3e437f97a6a2",
              "IPY_MODEL_4eebe69906004a57b362dbd13b7a1f3e"
            ]
          }
        },
        "357e949f52074868bc743123d1d4af25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "280c4d597dbe4a2db6dc3e437f97a6a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9a8781553dc345d1bb4e4a7cb5bce358",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_62f8399f515441c880a4408ce770804f"
          }
        },
        "4eebe69906004a57b362dbd13b7a1f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ec69575fdbdf4a13ab34467ede5aee00",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:10&lt;00:00, 22.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_620ae1b001f04766852b08c94d630fa2"
          }
        },
        "9a8781553dc345d1bb4e4a7cb5bce358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "62f8399f515441c880a4408ce770804f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec69575fdbdf4a13ab34467ede5aee00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "620ae1b001f04766852b08c94d630fa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbb64bebc17e44e0b04f39e504b7fdb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_58f0538c55e840ba8cc0915d24d24dcf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d58df1070c7e4c4fbf15968652fb3431",
              "IPY_MODEL_c450dc10138a4946800cc26169110864"
            ]
          }
        },
        "58f0538c55e840ba8cc0915d24d24dcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d58df1070c7e4c4fbf15968652fb3431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b4f8a177bf104cafa035cb05cd41a8ae",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93feac8a434e400489549287df8b14f1"
          }
        },
        "c450dc10138a4946800cc26169110864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_791ffbec2fac4127af5987d0bb6783cc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:05&lt;00:00, 79.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e89533d0c0ca4b039f1b8efedc38ef3a"
          }
        },
        "b4f8a177bf104cafa035cb05cd41a8ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93feac8a434e400489549287df8b14f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "791ffbec2fac4127af5987d0bb6783cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e89533d0c0ca4b039f1b8efedc38ef3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d26f7b2b74f1441cbe4890b1dba26022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b4d7a069b124494399236691cd9d985b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7a2971fd4888433c82dd50ff13c956c4",
              "IPY_MODEL_7baaecc6958a4cc396332b593817536d"
            ]
          }
        },
        "b4d7a069b124494399236691cd9d985b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a2971fd4888433c82dd50ff13c956c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ab2c86249f794446a1822b276da910e7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c4f35b2d0a47420d9c8c65a104573c4e"
          }
        },
        "7baaecc6958a4cc396332b593817536d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f701058279db413ebb3cdcd87b8064d8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:05&lt;00:00, 5.02B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5ef8f532f5a348f292a7a31c880f2463"
          }
        },
        "ab2c86249f794446a1822b276da910e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c4f35b2d0a47420d9c8c65a104573c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f701058279db413ebb3cdcd87b8064d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5ef8f532f5a348f292a7a31c880f2463": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5849321855804ac4956f983d2c2cf04b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8f6ba503c4c54fb698f298edc120c42c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_50bea3c1be59455e915415d0043be95b",
              "IPY_MODEL_7459c010cb8e43568382c1fbcd960142"
            ]
          }
        },
        "8f6ba503c4c54fb698f298edc120c42c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50bea3c1be59455e915415d0043be95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3f5d9b6a581d4a3cb5db6a5fbc9714ac",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 363423424,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 363423424,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d279141d98dc4fa29ad8ae188d4194a6"
          }
        },
        "7459c010cb8e43568382c1fbcd960142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_35a2cf6475134ac6be25bcb89676e584",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 363M/363M [00:08&lt;00:00, 44.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d16a8f77f0b945be944c14e151b21f09"
          }
        },
        "3f5d9b6a581d4a3cb5db6a5fbc9714ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d279141d98dc4fa29ad8ae188d4194a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "35a2cf6475134ac6be25bcb89676e584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d16a8f77f0b945be944c14e151b21f09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrFK2QV22X9N",
        "outputId": "50842304-bc9b-4e71-8a3c-ab66c673cf4f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGMFzgSdUMc3",
        "outputId": "f4f653d7-5b43-4524-e844-722f15b8d79b"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr  7 19:52:19 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0    39W / 250W |   2465MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD9MC9OP2ufC",
        "outputId": "1babff80-5aa3-46e4-9235-6ced350d2e99"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.44)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVK7DgyC201T"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ACV5v9rUw-g"
      },
      "source": [
        "referring to this article: https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx1crfXk3nUn"
      },
      "source": [
        "Read the data in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko-3OTNFmVqV"
      },
      "source": [
        "lav_path = '/content/gdrive/MyDrive/W266Project_Lav_Shalz/train-balanced-sarcasm.csv'\n",
        "shalz_path = '/content/gdrive/MyDrive/Colab Notebooks/train-balanced-sarcasm.csv'"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBIqTyoV2w8q"
      },
      "source": [
        "df = pd.read_csv(shalz_path)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMCrcfn04CE0",
        "outputId": "9ab15df1-80e5-46d1-e519-d0a61395071d"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1010826, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yCgNSLq4D_N",
        "outputId": "d621da5c-31f9-4d1d-f1d4-555e11798395"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label              0\n",
              "comment           53\n",
              "author             0\n",
              "subreddit          0\n",
              "score              0\n",
              "ups                0\n",
              "downs              0\n",
              "date               0\n",
              "created_utc        0\n",
              "parent_comment     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0TcQqEV4Fbd",
        "outputId": "62866373-f300-4b03-acf0-974941d432d5"
      },
      "source": [
        "df = df[df['comment'].notna()]\n",
        "df.isna().sum()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label             0\n",
              "comment           0\n",
              "author            0\n",
              "subreddit         0\n",
              "score             0\n",
              "ups               0\n",
              "downs             0\n",
              "date              0\n",
              "created_utc       0\n",
              "parent_comment    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PUVC-zV4HB_",
        "outputId": "29b6f67f-91eb-4758-dc52-c306d1564cbf"
      },
      "source": [
        "# check label distribution after removing NA\n",
        "df['label'].value_counts()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    505405\n",
              "1    505368\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKO0DyLdpGIR"
      },
      "source": [
        "In this notebook, we want to do hyperparameter tuning in order to improve our model. This means that unlike before, we want to be able to use all of the data we have to build the model and then tune the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FbuHcrh4IjN"
      },
      "source": [
        "# # select a fraction of the data\n",
        "# s0 = df.label[df.label.eq(0)].sample(505368).index\n",
        "# s1 = df.label[df.label.eq(1)].sample(505368).index \n",
        "\n",
        "# df = df.loc[s0.union(s1)]\n",
        "# df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "d5d208ea029a44a9a10593186527b550",
            "357e949f52074868bc743123d1d4af25",
            "280c4d597dbe4a2db6dc3e437f97a6a2",
            "4eebe69906004a57b362dbd13b7a1f3e",
            "9a8781553dc345d1bb4e4a7cb5bce358",
            "62f8399f515441c880a4408ce770804f",
            "ec69575fdbdf4a13ab34467ede5aee00",
            "620ae1b001f04766852b08c94d630fa2",
            "bbb64bebc17e44e0b04f39e504b7fdb2",
            "58f0538c55e840ba8cc0915d24d24dcf",
            "d58df1070c7e4c4fbf15968652fb3431",
            "c450dc10138a4946800cc26169110864",
            "b4f8a177bf104cafa035cb05cd41a8ae",
            "93feac8a434e400489549287df8b14f1",
            "791ffbec2fac4127af5987d0bb6783cc",
            "e89533d0c0ca4b039f1b8efedc38ef3a",
            "d26f7b2b74f1441cbe4890b1dba26022",
            "b4d7a069b124494399236691cd9d985b",
            "7a2971fd4888433c82dd50ff13c956c4",
            "7baaecc6958a4cc396332b593817536d",
            "ab2c86249f794446a1822b276da910e7",
            "c4f35b2d0a47420d9c8c65a104573c4e",
            "f701058279db413ebb3cdcd87b8064d8",
            "5ef8f532f5a348f292a7a31c880f2463"
          ]
        },
        "id": "lixVjl304KLY",
        "outputId": "65bdee3e-e14c-4dc7-f28b-30c2d06be4a8"
      },
      "source": [
        "tokenizer_case = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
        "tokenizer_uncase = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5d208ea029a44a9a10593186527b550",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbb64bebc17e44e0b04f39e504b7fdb2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d26f7b2b74f1441cbe4890b1dba26022",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYlJux8f_Gnv"
      },
      "source": [
        "# check what max length should be based on the sentence lengths in the full data\n",
        "\n",
        "comment_lengths = list(df['comment'].str.split().apply(len))\n",
        "parent_comment_lengths = list(df['parent_comment'].str.split().apply(len))\n",
        "total_comment_lengths = [a + b for a, b in zip(comment_lengths, parent_comment_lengths)]\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8pRN1T2Gntz",
        "outputId": "12b00f2d-5183-45ec-eba3-6f0eda03bfc3"
      },
      "source": [
        "print(\"Comment Length Distribution\")\n",
        "print(min(comment_lengths))\n",
        "print(np.percentile(comment_lengths, [25, 50, 75]))\n",
        "print(max(comment_lengths))\n",
        "\n",
        "\n",
        "print(\"Parent Comment Length Distribution\")\n",
        "print(min(parent_comment_lengths))\n",
        "print(np.percentile(parent_comment_lengths, [25, 50, 75]))\n",
        "print(max(parent_comment_lengths))\n",
        "\n",
        "\n",
        "print(\"Total Comment Length Distribution\")\n",
        "print(min(total_comment_lengths))\n",
        "print(np.percentile(total_comment_lengths, [25, 50, 75]))\n",
        "print(max(total_comment_lengths))\n",
        "\n",
        "\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Comment Length Distribution\n",
            "1\n",
            "[ 5.  9. 14.]\n",
            "2222\n",
            "Parent Comment Length Distribution\n",
            "1\n",
            "[ 8. 14. 26.]\n",
            "4198\n",
            "Total Comment Length Distribution\n",
            "2\n",
            "[16. 24. 40.]\n",
            "4444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npfChVayH3R8"
      },
      "source": [
        "We have some sentences that are very long, but most of the data (75% percentile) is below 50 so we will use this as our max_length value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZspzq0K7nAY"
      },
      "source": [
        "Trying out the tokenizer in order to the two methods we want to try out: \n",
        "\n",
        "`Approach A: [CLS] [comment] [SEP] [Masking]\n",
        "id: 0`\n",
        "\n",
        "\n",
        "`Approach B: [CLS] [parent_comment] [SEP] [comment] [SEP] [Masking]`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkG2V4bwCUBt",
        "outputId": "d61748fd-d841-48ad-dfbe-a648a3668774"
      },
      "source": [
        "# understanding the tokenizer\n",
        "temp_sentence = df[\"comment\"][10]\n",
        "temp_parent_comment = df[\"parent_comment\"][10]\n",
        "print(temp_sentence)\n",
        "print(temp_parent_comment)\n",
        "temp_tokens = tokenizer.tokenize(temp_sentence)\n",
        "print(temp_tokens)\n",
        "print(temp_parent_comment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I think a significant amount would be against spending their tax dollars on other people.\n",
            "I bet if that money was poured into college debt or health debt relief, 81% of Americans would have been for it instead.\n",
            "['I', 'think', 'a', 'significant', 'amount', 'would', 'be', 'against', 'spending', 'their', 'tax', 'dollars', 'on', 'other', 'people', '.']\n",
            "I bet if that money was poured into college debt or health debt relief, 81% of Americans would have been for it instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zyL3NfC9Btu",
        "outputId": "4ffe12e2-ec07-43ee-ef12-c0a9b10c6ae2"
      },
      "source": [
        "inputs = tokenizer(temp_sentence,\n",
        "          padding = 'max_length', max_length = 50, truncation = True)\n",
        "\n",
        "inputs\n",
        "# 101 at the beginning is the CLS token\n",
        "# 102 in between comment and parent comment is SEP token\n",
        "# 0 is padding based on the max_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 146, 1341, 170, 2418, 2971, 1156, 1129, 1222, 5369, 1147, 3641, 5860, 1113, 1168, 1234, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ZZf_28rWIXXg",
        "outputId": "470fb48e-e718-4a62-f935-2807f68f7b71"
      },
      "source": [
        "encoded_sequence = inputs[\"input_ids\"]\n",
        "encoded_sequence\n",
        "decoded_sequence = tokenizer.decode(encoded_sequence)\n",
        "decoded_sequence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] I think a significant amount would be against spending their tax dollars on other people. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pR-AcWlInLQ",
        "outputId": "ab6de523-477c-4e82-ea4c-db2628d563a6"
      },
      "source": [
        "inputs = tokenizer([[temp_sentence, temp_parent_comment]],\n",
        "          padding = 'max_length', max_length = 50, truncation = True)\n",
        "inputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 146, 1341, 170, 2418, 2971, 1156, 1129, 1222, 5369, 1147, 3641, 5860, 1113, 1168, 1234, 119, 102, 146, 7023, 1191, 1115, 1948, 1108, 8168, 1154, 2134, 6695, 1137, 2332, 6695, 3893, 117, 5615, 110, 1104, 4038, 1156, 1138, 1151, 1111, 1122, 1939, 119, 102, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msIiOrEgNDuq"
      },
      "source": [
        "Diff version of tokenizing that we would do below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCMa2Uv1JRVw",
        "outputId": "b0d97148-0eab-4b9f-df24-42aa0cdeefe3"
      },
      "source": [
        "MAX_LENGTH = 30\n",
        "tokenizer.batch_encode_plus(temp_tokens,\n",
        "                            max_length=MAX_LENGTH,\n",
        "                            padding='longest', #implements dynamic padding\n",
        "                            truncation=True,\n",
        "                            return_attention_mask=True,\n",
        "                            return_token_type_ids=False\n",
        "                            )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 146, 102], [101, 1341, 102], [101, 170, 102], [101, 2418, 102], [101, 2971, 102], [101, 1156, 102], [101, 1129, 102], [101, 1222, 102], [101, 5369, 102], [101, 1147, 102], [101, 3641, 102], [101, 5860, 102], [101, 1113, 102], [101, 1168, 102], [101, 1234, 102], [101, 119, 102]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKW29H9zDA2Q",
        "outputId": "38b531e6-269f-4957-e44e-909b5f88563c"
      },
      "source": [
        "tokenizer(temp_sentence,\n",
        "          padding = 'max_length', \n",
        "          max_length = MAX_LENGTH, \n",
        "          truncation = True,\n",
        "          return_attention_mask=True,\n",
        "          return_token_type_ids=False\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 146, 1341, 170, 2418, 2971, 1156, 1129, 1222, 5369, 1147, 3641, 5860, 1113, 1168, 1234, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdZty739Fuke"
      },
      "source": [
        "## Train Test Split\n",
        "Let's split the data in train, val, test and then tokenize all of it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BnOEuHk4Pd3"
      },
      "source": [
        "X_train, temp_text, y_train, temp_labels = train_test_split(df['comment'], df['label'], \n",
        "                                                                    random_state=0, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "X_val, X_test, y_val, y_test = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=0, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scgD-GG66RG7"
      },
      "source": [
        "MAX_LENGTH = 50"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "111Gpwj88zl6"
      },
      "source": [
        "def batch_encode(tokenizer, texts, batch_size=256, max_length=MAX_LENGTH):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        # inputs = tokenizer.batch_encode_plus(batch,\n",
        "        #                                      max_length=max_length,\n",
        "        #                                      padding='longest', #implements dynamic padding\n",
        "        #                                      truncation=True,\n",
        "        #                                      return_attention_mask=True,\n",
        "        #                                      return_token_type_ids=False\n",
        "        #                                      )\n",
        "\n",
        "        inputs = tokenizer(batch,\n",
        "                          padding = 'max_length', \n",
        "                          max_length = MAX_LENGTH, \n",
        "                          truncation = True,\n",
        "                          return_attention_mask=True,\n",
        "                          return_token_type_ids=False\n",
        "                          )\n",
        "        \n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "  "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ918eOn8zjy"
      },
      "source": [
        "### This cell takes a few minutes to run\n",
        "\n",
        "# Encode X_train\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, X_train.tolist())\n",
        "\n",
        "# Encode X_valid\n",
        "X_val_ids, X_val_attention = batch_encode(tokenizer, X_val.tolist())\n",
        "\n",
        "# Encode X_test\n",
        "X_test_ids, X_test_attention = batch_encode(tokenizer, X_test.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "5849321855804ac4956f983d2c2cf04b",
            "8f6ba503c4c54fb698f298edc120c42c",
            "50bea3c1be59455e915415d0043be95b",
            "7459c010cb8e43568382c1fbcd960142",
            "3f5d9b6a581d4a3cb5db6a5fbc9714ac",
            "d279141d98dc4fa29ad8ae188d4194a6",
            "35a2cf6475134ac6be25bcb89676e584",
            "d16a8f77f0b945be944c14e151b21f09"
          ]
        },
        "id": "rrHTrnlg8zhu",
        "outputId": "20f26f49-9b7f-4483-8d9b-accd97303f7f"
      },
      "source": [
        "DISTILBERT_DROPOUT = 0.2\n",
        "DISTILBERT_ATT_DROPOUT = 0.2\n",
        " \n",
        "# Configure DistilBERT's initialization\n",
        "config = DistilBertConfig(dropout=DISTILBERT_DROPOUT, \n",
        "                          attention_dropout=DISTILBERT_ATT_DROPOUT, \n",
        "                          output_hidden_states=True)\n",
        "                          \n",
        "# The bare, pre-trained DistilBERT transformer model outputting raw hidden-states \n",
        "# and without any specific head on top.\n",
        "distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
        "\n",
        "# Make DistilBERT layers untrainable\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5849321855804ac4956f983d2c2cf04b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilOjuQJjq4-7"
      },
      "source": [
        "Trying to understand the DistilBERT model layers a bit more"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INVIjMelquxi",
        "outputId": "4f68e22c-76bd-4efc-dceb-4237081c2c93"
      },
      "source": [
        "distilBERT.layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer at 0x7fc2d849c050>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8ol64fyq-lE",
        "outputId": "675203f6-164b-4aa4-bdad-c673acc9c9af"
      },
      "source": [
        "len(distilBERT.layers[0].weights)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkTWvOj6rC8O"
      },
      "source": [
        "We have 100 layers in the model, let's look at the first 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN9m1lf3rGlv",
        "outputId": "5abfe48d-3ef6-4d82-f659-59ece758b810"
      },
      "source": [
        "for layer in range(10):\n",
        "    print(layer)\n",
        "    print('Layer name: \\t', distilBERT.layers[0].weights[layer].name)\n",
        "    print('Layer shape: \\t', distilBERT.layers[0].weights[layer].shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Layer name: \t tf_distil_bert_model/distilbert/embeddings/word_embeddings/weight:0\n",
            "Layer shape: \t (30522, 768)\n",
            "1\n",
            "Layer name: \t tf_distil_bert_model/distilbert/embeddings/position_embeddings/embeddings:0\n",
            "Layer shape: \t (512, 768)\n",
            "2\n",
            "Layer name: \t tf_distil_bert_model/distilbert/embeddings/LayerNorm/gamma:0\n",
            "Layer shape: \t (768,)\n",
            "3\n",
            "Layer name: \t tf_distil_bert_model/distilbert/embeddings/LayerNorm/beta:0\n",
            "Layer shape: \t (768,)\n",
            "4\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/kernel:0\n",
            "Layer shape: \t (768, 768)\n",
            "5\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/bias:0\n",
            "Layer shape: \t (768,)\n",
            "6\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/kernel:0\n",
            "Layer shape: \t (768, 768)\n",
            "7\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/bias:0\n",
            "Layer shape: \t (768,)\n",
            "8\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/kernel:0\n",
            "Layer shape: \t (768, 768)\n",
            "9\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/bias:0\n",
            "Layer shape: \t (768,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWZR5H-grSmU"
      },
      "source": [
        "Last 5 layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iXo8or0rRuz",
        "outputId": "b537b4ad-f8a0-4b12-a2cd-14c85ba29b9a"
      },
      "source": [
        "for layer in [99, 98, 97, 96, 95]:\n",
        "    print(layer)\n",
        "    print('Layer name: \\t', distilBERT.layers[0].weights[layer].name)\n",
        "    print('Layer shape: \\t', distilBERT.layers[0].weights[layer].shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/beta:0\n",
            "Layer shape: \t (768,)\n",
            "98\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/gamma:0\n",
            "Layer shape: \t (768,)\n",
            "97\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/bias:0\n",
            "Layer shape: \t (768,)\n",
            "96\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/kernel:0\n",
            "Layer shape: \t (3072, 768)\n",
            "95\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin1/bias:0\n",
            "Layer shape: \t (3072,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goWXf-YwtIps"
      },
      "source": [
        "We see the embedding layer which maps the token id to a 768 dim vector.\n",
        "Next is the positional encoding which encodes the 512 BERT input positions. \n",
        "Layers 5-10 hold the weights and biases for the first self-attention layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3n6Mfo48zfZ"
      },
      "source": [
        "LAYER_DROPOUT = 0.2\n",
        "LEARNING_RATE = 5e-5\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "def build_model(transformer, max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Template for building a model off of the BERT or DistilBERT architecture\n",
        "    for a binary classification task.\n",
        "    \n",
        "    Input:\n",
        "      - transformer:  a base Hugging Face transformer model object (BERT or DistilBERT)\n",
        "                      with no added classification head attached.\n",
        "      - max_length:   integer controlling the maximum number of encoded tokens \n",
        "                      in a given sequence.\n",
        "    \n",
        "    Output:\n",
        "      - model:        a compiled tf.keras.Model with added classification layers \n",
        "                      on top of the base pre-trained model architecture.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define weight initializer with a random seed to ensure reproducibility\n",
        "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
        "    \n",
        "    # Define input layers\n",
        "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                            name='input_ids', \n",
        "                                            dtype='int32')\n",
        "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                                  name='input_attention', \n",
        "                                                  dtype='int32')\n",
        "    \n",
        "    # DistilBERT outputs a tuple where the first element at index 0\n",
        "    # represents the hidden-state at the output of the model's last layer.\n",
        "    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n",
        "    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n",
        "    \n",
        "    # We only care about DistilBERT's output for the [CLS] token, \n",
        "    # which is located at index 0 of every encoded sequence.  \n",
        "    # Splicing out the [CLS] tokens gives us 2D data.\n",
        "    cls_token = last_hidden_state[:, 0, :]\n",
        "    \n",
        "    ##                                                 ##\n",
        "    ## Define additional dropout and dense layers here ##\n",
        "    ##                                                 ##\n",
        "    \n",
        "    # Define a single node that makes up the output layer (for binary classification)\n",
        "    output = tf.keras.layers.Dense(1, \n",
        "                                   activation='sigmoid',\n",
        "                                   kernel_initializer=weight_initializer,  \n",
        "                                   kernel_constraint=None,\n",
        "                                   bias_initializer='zeros'\n",
        "                                   )(cls_token)\n",
        "    \n",
        "    # Define the model\n",
        "    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), \n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfMV7E5LOcdR",
        "outputId": "8b1f8bda-ac1f-43bb-d635-24f2e28b2d60"
      },
      "source": [
        "model = build_model(distilBERT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fc33a663d70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fc33a663d70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fc355f10c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7fc355f10c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KJ7L6hAOcbH",
        "outputId": "97c1c66c-757b-4461-b0a1-919dbed494a4"
      },
      "source": [
        "EPOCHS = 6\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index) // BATCH_SIZE\n",
        "\n",
        "# Train the model\n",
        "train_history1 = model.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 929s - loss: 0.6894 - accuracy: 0.5331 - val_loss: 0.6826 - val_accuracy: 0.5583\n",
            "Epoch 2/6\n",
            "11055/11055 - 920s - loss: 0.6832 - accuracy: 0.5550 - val_loss: 0.6786 - val_accuracy: 0.5692\n",
            "Epoch 3/6\n",
            "11055/11055 - 920s - loss: 0.6810 - accuracy: 0.5604 - val_loss: 0.6766 - val_accuracy: 0.5741\n",
            "Epoch 4/6\n",
            "11055/11055 - 925s - loss: 0.6797 - accuracy: 0.5637 - val_loss: 0.6757 - val_accuracy: 0.5759\n",
            "Epoch 5/6\n",
            "11055/11055 - 928s - loss: 0.6787 - accuracy: 0.5666 - val_loss: 0.6742 - val_accuracy: 0.5795\n",
            "Epoch 6/6\n",
            "11055/11055 - 928s - loss: 0.6778 - accuracy: 0.5685 - val_loss: 0.6731 - val_accuracy: 0.5817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UAchTWJwjVC"
      },
      "source": [
        "pd.DataFrame.from_dict(train_history1.history).to_csv('PRO_history1.csv',index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcOnCoC3OcYY",
        "outputId": "a1aa7e6b-43d1-4fbd-8ba8-5573bdf028a5"
      },
      "source": [
        "FT_EPOCHS = 7\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index)\n",
        "\n",
        "# Unfreeze distilBERT layers and make available for training\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = True\n",
        "    \n",
        "# Recompile model after unfreezing\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "train_history2 = model.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = FT_EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4952787 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "707541/707541 - 14672s - loss: 0.4802 - accuracy: 0.7646 - val_loss: 0.6034 - val_accuracy: 0.7331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkYkmP4lNh6V"
      },
      "source": [
        "pd.DataFrame.from_dict(train_history2.history).to_csv('PRO_history2.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh89IBPrOcV5"
      },
      "source": [
        "tf.keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MWMboupagb5"
      },
      "source": [
        "## Adding additional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJt059xcOcTf"
      },
      "source": [
        "# resetting some of the global parameters here\n",
        "LAYER_DROPOUT = 0.2\n",
        "LEARNING_RATE = 5e-5\n",
        "RANDOM_STATE = 42\n",
        "DISTILBERT_DROPOUT = 0.2\n",
        "DISTILBERT_ATT_DROPOUT = 0.2\n",
        "MAX_LENGTH = 50\n",
        "L2REG = 0.01"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fx0SbYaOcQp"
      },
      "source": [
        "def build_model2(transformer, max_length=MAX_LENGTH, l2reg = L2REG, dropout_rate = LAYER_DROPOUT):\n",
        "    \"\"\"\n",
        "    Template for building a model off of the BERT or DistilBERT architecture\n",
        "    for a binary classification task.\n",
        "    \n",
        "    Input:\n",
        "      - transformer:  a base Hugging Face transformer model object (BERT or DistilBERT)\n",
        "                      with no added classification head attached.\n",
        "      - max_length:   integer controlling the maximum number of encoded tokens \n",
        "                      in a given sequence.\n",
        "    \n",
        "    Output:\n",
        "      - model:        a compiled tf.keras.Model with added classification layers \n",
        "                      on top of the base pre-trained model architecture.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define weight initializer with a random seed to ensure reproducibility\n",
        "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
        "    \n",
        "    # Define input layers\n",
        "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                            name='input_ids', \n",
        "                                            dtype='int32')\n",
        "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                                  name='input_attention', \n",
        "                                                  dtype='int32')\n",
        "    \n",
        "    # DistilBERT outputs a tuple where the first element at index 0\n",
        "    # represents the hidden-state at the output of the model's last layer.\n",
        "    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n",
        "    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n",
        "    \n",
        "    # We only care about DistilBERT's output for the [CLS] token, \n",
        "    # which is located at index 0 of every encoded sequence.  \n",
        "    # Splicing out the [CLS] tokens gives us 2D data.\n",
        "    cls_token = last_hidden_state[:, 0, :]\n",
        "    \n",
        "    ##                                                 ##\n",
        "    ## Define additional dropout and dense layers here ##\n",
        "    ##                                                 ##\n",
        "    dense = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer= tf.keras.regularizers.l2(l2reg))(cls_token)\n",
        "    dropout= tf.keras.layers.Dropout(dropout_rate)(dense)\n",
        "    \n",
        "    # Define a single node that makes up the output layer (for binary classification)\n",
        "    output = tf.keras.layers.Dense(1, \n",
        "                                   activation='sigmoid',\n",
        "                                   kernel_initializer=weight_initializer,  \n",
        "                                   kernel_constraint=None,\n",
        "                                   bias_initializer='zeros'\n",
        "                                   )(dropout)\n",
        "    \n",
        "    # Define the model\n",
        "    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), \n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGk8GQ3wOcOM",
        "outputId": "5853c25c-7b93-49da-be7e-369abaa2ca46"
      },
      "source": [
        "model2 = build_model2(distilBERT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X21mqvCEOcLf"
      },
      "source": [
        "EPOCHS = 7\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index) // BATCH_SIZE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9EAKfzTOcIn",
        "outputId": "5fb9ae7a-ec61-41f4-a4fb-eaaa4434f2d8"
      },
      "source": [
        "# Train the model\n",
        "train_history3 = model2.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 879s - loss: 0.9772 - accuracy: 0.5378 - val_loss: 0.6909 - val_accuracy: 0.5607\n",
            "Epoch 2/7\n",
            "11055/11055 - 871s - loss: 0.6917 - accuracy: 0.5468 - val_loss: 0.6880 - val_accuracy: 0.5526\n",
            "Epoch 3/7\n",
            "11055/11055 - 873s - loss: 0.6889 - accuracy: 0.5511 - val_loss: 0.6847 - val_accuracy: 0.5650\n",
            "Epoch 4/7\n",
            "11055/11055 - 870s - loss: 0.6877 - accuracy: 0.5535 - val_loss: 0.6896 - val_accuracy: 0.5416\n",
            "Epoch 5/7\n",
            "11055/11055 - 870s - loss: 0.6869 - accuracy: 0.5550 - val_loss: 0.6841 - val_accuracy: 0.5615\n",
            "Epoch 6/7\n",
            "11055/11055 - 870s - loss: 0.6863 - accuracy: 0.5566 - val_loss: 0.6822 - val_accuracy: 0.5737\n",
            "Epoch 7/7\n",
            "11055/11055 - 869s - loss: 0.6859 - accuracy: 0.5581 - val_loss: 0.6857 - val_accuracy: 0.5520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtK0zrDwEWLR"
      },
      "source": [
        "pd.DataFrame.from_dict(train_history3.history).to_csv('PRO_history3.csv',index=False)\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download('PRO_history3.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvHMpg-jOcFO"
      },
      "source": [
        "# try with diff hyper parameters\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9PZiwtvAOVI"
      },
      "source": [
        "## Add the parent comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XzteZ5uB-tx"
      },
      "source": [
        "tf.keras.backend.clear_session()\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OV5q5B6CAIP"
      },
      "source": [
        "X_train_p, temp_text, y_train_p, temp_labels = train_test_split(df[['comment', 'parent_comment']], df['label'], \n",
        "                                                                    random_state=0, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "X_val_p, X_test_p, y_val_p, y_test_p = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=0, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "P90ZwSKiCfdT",
        "outputId": "284220e3-ff91-4c8c-b750-41dafe8be3b1"
      },
      "source": [
        "X_train_p.head()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>parent_comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>715875</th>\n",
              "      <td>They do in neutral!</td>\n",
              "      <td>The only main thing you'll notice (vs driving ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348688</th>\n",
              "      <td>But but... He was on Howard Stern voting for t...</td>\n",
              "      <td>Trump was against the war from the very beginn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323243</th>\n",
              "      <td>Kvothe, is that you?</td>\n",
              "      <td>Now imagine a hoodie with a great number of po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56970</th>\n",
              "      <td>Never \"meta\" in pro scene so it must be a shit...</td>\n",
              "      <td>What's wrong with scarab?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267644</th>\n",
              "      <td>I'm almost shocked not to see TJ McConnell here</td>\n",
              "      <td>Best and Worst Catch &amp;amp; Shooters</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  comment                                     parent_comment\n",
              "715875                                They do in neutral!  The only main thing you'll notice (vs driving ...\n",
              "348688  But but... He was on Howard Stern voting for t...  Trump was against the war from the very beginn...\n",
              "323243                               Kvothe, is that you?  Now imagine a hoodie with a great number of po...\n",
              "56970   Never \"meta\" in pro scene so it must be a shit...                          What's wrong with scarab?\n",
              "267644    I'm almost shocked not to see TJ McConnell here                Best and Worst Catch &amp; Shooters"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL8oZ8WEAPyK"
      },
      "source": [
        "MAX_LENGTH = 30"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNgu1WUvabei"
      },
      "source": [
        "# try with parent comment connected\n",
        "def batch_encode_parent(tokenizer, texts, parent, batch_size=256, max_length=MAX_LENGTH):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        parent_batch = parent[i:i+batch_size]\n",
        "\n",
        "        combined = [list(i) for i in zip(parent_batch, batch)]\n",
        "\n",
        "\n",
        "        inputs = tokenizer(combined,\n",
        "                          padding = 'max_length', \n",
        "                          max_length = MAX_LENGTH, \n",
        "                          truncation = True,\n",
        "                          return_attention_mask=True,\n",
        "                          return_token_type_ids=False\n",
        "                          )\n",
        "        \n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "  "
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li8R0XVzHE8W",
        "outputId": "b75329c7-8e93-445d-a308-359f44bbd169"
      },
      "source": [
        "# temp1 = X_train_p[\"comment\"].head(2).tolist()\n",
        "# temp2 = X_train_p[\"parent_comment\"].head(2).tolist()\n",
        "\n",
        "# [[temp2, temp1]]\n",
        "\n",
        "# print(temp1)\n",
        "# print(temp2)\n",
        "\n",
        "# # zip(temp1, temp2)\n",
        "\n",
        "\n",
        "# [list(i) for i in zip(temp1, temp2)]\n",
        "\n",
        "\n",
        "# # batch_encode_parent(tokenizer, temp1, temp2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['They do in neutral!', \"But but... He was on Howard Stern voting for the war and that's even worse than if he were a sitting senator doing the same... Because reasons...\"]\n",
            "[\"The only main thing you'll notice (vs driving a car with automatic transmission) is that the car will slow down a lot more quickly than an automatic would, manuals generally dont coast as freely.\", \"Trump was against the war from the very beginning too. Hillary literally voted for it. It is always such doublethink with the left it's unreal.\"]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['They do in neutral!',\n",
              "  \"The only main thing you'll notice (vs driving a car with automatic transmission) is that the car will slow down a lot more quickly than an automatic would, manuals generally dont coast as freely.\"],\n",
              " [\"But but... He was on Howard Stern voting for the war and that's even worse than if he were a sitting senator doing the same... Because reasons...\",\n",
              "  \"Trump was against the war from the very beginning too. Hillary literally voted for it. It is always such doublethink with the left it's unreal.\"]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLqE2NdBabXY"
      },
      "source": [
        "### This cell takes a few minutes to run\n",
        "\n",
        "# tokenizer_case\n",
        "# tokenizer_uncase\n",
        "\n",
        "# Encode X_train\n",
        "X_train_ids_p, X_train_attention_p = batch_encode_parent(tokenizer_uncase, \n",
        "                                                  X_train_p[\"comment\"].tolist(), \n",
        "                                                  X_train_p[\"parent_comment\"].tolist())\n",
        "\n",
        "# Encode X_valid\n",
        "X_val_ids_p, X_val_attention_p = batch_encode_parent(tokenizer_uncase, \n",
        "                                              X_val_p[\"comment\"].tolist(), \n",
        "                                              X_val_p[\"parent_comment\"].tolist())\n",
        "\n",
        "# Encode X_test\n",
        "X_test_ids_p, X_test_attention_p = batch_encode_parent(tokenizer_uncase, \n",
        "                                                X_test_p[\"comment\"].tolist(), \n",
        "                                                X_test_p[\"parent_comment\"].tolist())\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvuNdcCrbHIp",
        "outputId": "9756c96f-0ed6-460a-eb54-778f3525a893"
      },
      "source": [
        "DISTILBERT_DROPOUT = 0.2\n",
        "DISTILBERT_ATT_DROPOUT = 0.2\n",
        " \n",
        "# Configure DistilBERT's initialization\n",
        "config = DistilBertConfig(dropout=DISTILBERT_DROPOUT, \n",
        "                          attention_dropout=DISTILBERT_ATT_DROPOUT, \n",
        "                          output_hidden_states=True)\n",
        "                          \n",
        "# The bare, pre-trained DistilBERT transformer model outputting raw hidden-states \n",
        "# and without any specific head on top.\n",
        "distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
        "\n",
        "# Make DistilBERT layers untrainable\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = False\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6x9lwTqbHBL"
      },
      "source": [
        "LAYER_DROPOUT = 0.2\n",
        "LEARNING_RATE = 5e-5\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "def build_model_parent(transformer, max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Template for building a model off of the BERT or DistilBERT architecture\n",
        "    for a binary classification task.\n",
        "    \n",
        "    Input:\n",
        "      - transformer:  a base Hugging Face transformer model object (BERT or DistilBERT)\n",
        "                      with no added classification head attached.\n",
        "      - max_length:   integer controlling the maximum number of encoded tokens \n",
        "                      in a given sequence.\n",
        "    \n",
        "    Output:\n",
        "      - model:        a compiled tf.keras.Model with added classification layers \n",
        "                      on top of the base pre-trained model architecture.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define weight initializer with a random seed to ensure reproducibility\n",
        "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
        "    \n",
        "    # Define input layers\n",
        "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                            name='input_ids', \n",
        "                                            dtype='int32')\n",
        "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                                  name='input_attention', \n",
        "                                                  dtype='int32')\n",
        "    \n",
        "    # DistilBERT outputs a tuple where the first element at index 0\n",
        "    # represents the hidden-state at the output of the model's last layer.\n",
        "    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n",
        "    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n",
        "    \n",
        "    # We only care about DistilBERT's output for the [CLS] token, \n",
        "    # which is located at index 0 of every encoded sequence.  \n",
        "    # Splicing out the [CLS] tokens gives us 2D data.\n",
        "    cls_token = last_hidden_state[:, 0, :]\n",
        "    \n",
        "    ##                                                 ##\n",
        "    ## Define additional dropout and dense layers here ##\n",
        "    ##                                                 ##\n",
        "    \n",
        "    # Define a single node that makes up the output layer (for binary classification)\n",
        "    output = tf.keras.layers.Dense(1, \n",
        "                                   activation='sigmoid',\n",
        "                                   kernel_initializer=weight_initializer,  \n",
        "                                   kernel_constraint=None,\n",
        "                                   bias_initializer='zeros'\n",
        "                                   )(cls_token)\n",
        "    \n",
        "    # Define the model\n",
        "    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), \n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CUSicjgbG4k",
        "outputId": "951b5341-6266-4055-8172-d113d0e4f357"
      },
      "source": [
        "model_parent = build_model_parent(distilBERT)\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS57r-UjabNm",
        "outputId": "46d82968-06d2-4833-a208-6a2a9d1672ec"
      },
      "source": [
        "EPOCHS = 6\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index) // BATCH_SIZE\n",
        "\n",
        "# Train the model\n",
        "train_history_parent1 = model_parent.fit(\n",
        "    x = [X_train_ids_p, X_train_attention_p],\n",
        "    y = y_train_p.to_numpy(),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids_p, X_val_attention_p], y_val_p.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 571s - loss: 0.6684 - accuracy: 0.5900 - val_loss: 0.6555 - val_accuracy: 0.6137\n",
            "Epoch 2/6\n",
            "11055/11055 - 564s - loss: 0.6563 - accuracy: 0.6112 - val_loss: 0.6505 - val_accuracy: 0.6205\n",
            "Epoch 3/6\n",
            "11055/11055 - 563s - loss: 0.6532 - accuracy: 0.6156 - val_loss: 0.6472 - val_accuracy: 0.6241\n",
            "Epoch 4/6\n",
            "11055/11055 - 562s - loss: 0.6514 - accuracy: 0.6177 - val_loss: 0.6455 - val_accuracy: 0.6261\n",
            "Epoch 5/6\n",
            "11055/11055 - 562s - loss: 0.6502 - accuracy: 0.6194 - val_loss: 0.6439 - val_accuracy: 0.6274\n",
            "Epoch 6/6\n",
            "11055/11055 - 564s - loss: 0.6489 - accuracy: 0.6218 - val_loss: 0.6429 - val_accuracy: 0.6285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONCALbZLOb6p"
      },
      "source": [
        "pd.DataFrame.from_dict(train_history_parent1.history).to_csv('PRO_parenthistory1.csv',index=False)\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m1JzBIO_cgd",
        "outputId": "15574eee-0d42-493e-8fa6-d22ba05f2696"
      },
      "source": [
        "FT_EPOCHS = 4\n",
        "BATCH_SIZE = 32\n",
        "NUM_STEPS = len(X_train.index)\n",
        "\n",
        "\n",
        "\n",
        "# Unfreeze distilBERT layers and make available for training\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = True\n",
        "    \n",
        "# Recompile model after unfreezing\n",
        "model_parent.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_history_parent2 = model_parent.fit(\n",
        "    x = [X_train_ids_p, X_train_attention_p],\n",
        "    y = y_train_p.to_numpy(),\n",
        "    epochs = FT_EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    \n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids_p, X_val_attention_p], y_val_p.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2830164 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "707541/707541 - 6498s - loss: 0.4629 - accuracy: 0.7775 - val_loss: 0.5353 - val_accuracy: 0.7594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVt9eyaUD_BC"
      },
      "source": [
        "pd.DataFrame.from_dict(train_history_parent2.history).to_csv('PRO_train_history_parent2.csv',index=False)\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dPvKJnfhrZW",
        "outputId": "b6de6573-2b2f-4879-c07e-dcd459572ccb"
      },
      "source": [
        "train_history_parent22 = model_parent.fit(\n",
        "    x = [X_train_ids_p, X_train_attention_p],\n",
        "    y = y_train_p.to_numpy(),\n",
        "    epochs = FT_EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    \n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids_p, X_val_attention_p], y_val_p.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2830164 batches). You may need to use the repeat() function when building your dataset.\n",
            "707541/707541 - 6480s - loss: 0.2982 - accuracy: 0.8722 - val_loss: 0.7303 - val_accuracy: 0.7470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_2cwiSy6kUI",
        "outputId": "1065f347-d181-4eb6-cb39-9322405c1d99"
      },
      "source": [
        "train_history_parent222 = model_parent.fit(\n",
        "    x = [X_train_ids_p, X_train_attention_p],\n",
        "    y = y_train_p.to_numpy(),\n",
        "    epochs = FT_EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    \n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids_p, X_val_attention_p], y_val_p.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2830164 batches). You may need to use the repeat() function when building your dataset.\n",
            "707541/707541 - 6481s - loss: 0.1752 - accuracy: 0.9299 - val_loss: 1.0220 - val_accuracy: 0.7431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vusb2Yc5D--x",
        "outputId": "6129323e-1d46-43db-ed2a-6582703afd19"
      },
      "source": [
        "# maybe change hyperparameters\n",
        "FT_EPOCHS = 4\n",
        "BATCH_SIZE = 32\n",
        "NUM_STEPS = len(X_train.index)\n",
        "\n",
        "\n",
        "\n",
        "# Unfreeze distilBERT layers and make available for training\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = True\n",
        "    \n",
        "# Recompile model after unfreezing\n",
        "model_parent.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-3), \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_history_parent3 = model_parent.fit(\n",
        "    x = [X_train_ids_p, X_train_attention_p],\n",
        "    y = y_train_p.to_numpy(),\n",
        "    epochs = FT_EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    \n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids_p, X_val_attention_p], y_val_p.to_numpy()),\n",
        "    verbose=2\n",
        ")\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2830164 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "707541/707541 - 6492s - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5iAJLhqqI03"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2NvshLuqIyd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3jLJIhCqIwX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdeGvenKqItx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfb7f8vmqIr0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dkboqs5qIpQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvp9WaN6qImt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRhVqpo7D-3H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKsImoEED-0W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgq1d8x7GIo6",
        "outputId": "8da37703-4e5f-4ad6-c264-8110556ac148"
      },
      "source": [
        "model = build_model(distilBERT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f7626466ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f7626466ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f7641d17c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f7641d17c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ12ibAH8zY7",
        "outputId": "e6fee038-27a1-4d93-8c03-efe4d33b521f"
      },
      "source": [
        "EPOCHS = 6\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index) // BATCH_SIZE\n",
        "\n",
        "# Train the model\n",
        "train_history1 = model.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 869s - loss: 0.6538 - accuracy: 0.6120 - val_loss: 0.6299 - val_accuracy: 0.6437\n",
            "Epoch 2/6\n",
            "11055/11055 - 861s - loss: 0.6352 - accuracy: 0.6382 - val_loss: 0.6207 - val_accuracy: 0.6545\n",
            "Epoch 3/6\n",
            "11055/11055 - 861s - loss: 0.6301 - accuracy: 0.6442 - val_loss: 0.6157 - val_accuracy: 0.6595\n",
            "Epoch 4/6\n",
            "11055/11055 - 861s - loss: 0.6274 - accuracy: 0.6468 - val_loss: 0.6127 - val_accuracy: 0.6627\n",
            "Epoch 5/6\n",
            "11055/11055 - 861s - loss: 0.6253 - accuracy: 0.6492 - val_loss: 0.6103 - val_accuracy: 0.6650\n",
            "Epoch 6/6\n",
            "11055/11055 - 862s - loss: 0.6240 - accuracy: 0.6513 - val_loss: 0.6083 - val_accuracy: 0.6658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLFpuUKF8zWN",
        "outputId": "b48dd612-8ed5-40c7-8004-0a5af624c5e5"
      },
      "source": [
        "FT_EPOCHS = 4\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index)\n",
        "\n",
        "# Unfreeze distilBERT layers and make available for training\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = True\n",
        "    \n",
        "# Recompile model after unfreezing\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "train_history2 = model.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = FT_EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2830164 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "707541/707541 - 8453s - loss: 0.4618 - accuracy: 0.7783 - val_loss: 0.5063 - val_accuracy: 0.7696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHhs-Y5t8fE2"
      },
      "source": [
        "All of the above code was run the full data using random parameters. We get a training accuracy of 77.83% and a validation accuracy of 76.96%, which are good signs we are not overfitting.\n",
        "\n",
        "Next, let's try changing the model architecture slightly (adding some dense layers) and hyperparameter tuning the model. Based on some research, the parameters that lead to the biggest change in accuracy are: learning rate, dropout, and batch size so I am going to focus on these parameters. It may also be good to try adding more dense/dropout layers to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlHOongwz-TJ"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_sY-k0iApVk"
      },
      "source": [
        "## Add additional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ULtBTBuHpu0"
      },
      "source": [
        "# resetting some of the global parameters here\n",
        "LAYER_DROPOUT = 0.2\n",
        "LEARNING_RATE = 5e-5\n",
        "RANDOM_STATE = 42\n",
        "DISTILBERT_DROPOUT = 0.2\n",
        "DISTILBERT_ATT_DROPOUT = 0.2\n",
        "MAX_LENGTH = 30\n",
        "L2REG = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5KDpgozAk66"
      },
      "source": [
        "def build_model2(transformer, max_length=MAX_LENGTH, l2reg = L2REG, dropout_rate = LAYER_DROPOUT):\n",
        "    \"\"\"\n",
        "    Template for building a model off of the BERT or DistilBERT architecture\n",
        "    for a binary classification task.\n",
        "    \n",
        "    Input:\n",
        "      - transformer:  a base Hugging Face transformer model object (BERT or DistilBERT)\n",
        "                      with no added classification head attached.\n",
        "      - max_length:   integer controlling the maximum number of encoded tokens \n",
        "                      in a given sequence.\n",
        "    \n",
        "    Output:\n",
        "      - model:        a compiled tf.keras.Model with added classification layers \n",
        "                      on top of the base pre-trained model architecture.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define weight initializer with a random seed to ensure reproducibility\n",
        "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
        "    \n",
        "    # Define input layers\n",
        "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                            name='input_ids', \n",
        "                                            dtype='int32')\n",
        "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                                  name='input_attention', \n",
        "                                                  dtype='int32')\n",
        "    \n",
        "    # DistilBERT outputs a tuple where the first element at index 0\n",
        "    # represents the hidden-state at the output of the model's last layer.\n",
        "    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n",
        "    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n",
        "    \n",
        "    # We only care about DistilBERT's output for the [CLS] token, \n",
        "    # which is located at index 0 of every encoded sequence.  \n",
        "    # Splicing out the [CLS] tokens gives us 2D data.\n",
        "    cls_token = last_hidden_state[:, 0, :]\n",
        "    \n",
        "    ##                                                 ##\n",
        "    ## Define additional dropout and dense layers here ##\n",
        "    ##                                                 ##\n",
        "    dense = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer= tf.keras.regularizers.l2(l2reg))(cls_token)\n",
        "    dropout= tf.keras.layers.Dropout(dropout_rate)(dense)\n",
        "    \n",
        "    # Define a single node that makes up the output layer (for binary classification)\n",
        "    output = tf.keras.layers.Dense(1, \n",
        "                                   activation='sigmoid',\n",
        "                                   kernel_initializer=weight_initializer,  \n",
        "                                   kernel_constraint=None,\n",
        "                                   bias_initializer='zeros'\n",
        "                                   )(dropout)\n",
        "    \n",
        "    # Define the model\n",
        "    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), \n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "    # inps = Input(shape = (max_len,), dtype='int64')\n",
        "    # masks= Input(shape = (max_len,), dtype='int64')\n",
        "    # dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
        "    # dense = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
        "    # dropout= Dropout(0.5)(dense)\n",
        "    # pred = Dense(num_classes, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "    # model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
        "    # print(model.summary())\n",
        "    # return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xHlfFUJI74f",
        "outputId": "5c45cc49-3d5a-4301-f9e5-3525229a3ac6"
      },
      "source": [
        "model2 = build_model2(distilBERT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fb3dea35ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fb3dea35ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fb3fa2e6c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7fb3fa2e6c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_OFU9zRZqvj"
      },
      "source": [
        "EPOCHS = 6\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index) // BATCH_SIZE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q95uv8l2Jgku",
        "outputId": "151150fb-f9ab-405a-ecc5-fbd14fdb7f2e"
      },
      "source": [
        "\n",
        "# Train the model\n",
        "train_history3 = model2.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 2234s - loss: 0.7305 - accuracy: 0.8047 - val_loss: 0.5110 - val_accuracy: 0.7629\n",
            "Epoch 2/6\n",
            "11055/11055 - 2226s - loss: 0.3899 - accuracy: 0.8259 - val_loss: 0.5051 - val_accuracy: 0.7584\n",
            "Epoch 3/6\n",
            "11055/11055 - 2226s - loss: 0.3469 - accuracy: 0.8492 - val_loss: 0.5824 - val_accuracy: 0.7495\n",
            "Epoch 4/6\n",
            "11055/11055 - 2223s - loss: 0.3076 - accuracy: 0.8692 - val_loss: 0.5959 - val_accuracy: 0.7534\n",
            "Epoch 5/6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jihnv3zQYKvI"
      },
      "source": [
        "with additional dense + dropout layers\n",
        "\n",
        "\n",
        "11055/11055 - 2234s - loss: 0.7305 - accuracy: 0.8047 - val_loss: 0.5110 - val_accuracy: 0.7629\n",
        "Epoch 2/6\n",
        "11055/11055 - 2226s - loss: 0.3899 - accuracy: 0.8259 - val_loss: 0.5051 - val_accuracy: 0.7584\n",
        "Epoch 3/6\n",
        "11055/11055 - 2226s - loss: 0.3469 - accuracy: 0.8492 - val_loss: 0.5824 - val_accuracy: 0.7495\n",
        "Epoch 4/6\n",
        "11055/11055 - 2223s - loss: 0.3076 - accuracy: 0.8692 - val_loss: 0.5959 - val_accuracy: 0.7534\n",
        "Epoch 5/6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbEHs1bjZ2Gp"
      },
      "source": [
        "FT_EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbywJ9C-JHZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d635e61-0025-47c4-8360-e3869750222a"
      },
      "source": [
        "# Unfreeze distilBERT layers and make available for training\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = True\n",
        "    \n",
        "# Recompile model after unfreezing\n",
        "model2.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "train_history4 = model2.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = FT_EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 2283s - loss: 0.3801 - accuracy: 0.8299 - val_loss: 0.5387 - val_accuracy: 0.7676\n",
            "Epoch 2/10\n",
            "11055/11055 - 2290s - loss: 0.3486 - accuracy: 0.8474 - val_loss: 0.5328 - val_accuracy: 0.7625\n",
            "Epoch 3/10\n",
            "11055/11055 - 2283s - loss: 0.3183 - accuracy: 0.8637 - val_loss: 0.5681 - val_accuracy: 0.7627\n",
            "Epoch 4/10\n",
            "11055/11055 - 2287s - loss: 0.2878 - accuracy: 0.8786 - val_loss: 0.5962 - val_accuracy: 0.7601\n",
            "Epoch 5/10\n",
            "11055/11055 - 2280s - loss: 0.2621 - accuracy: 0.8912 - val_loss: 0.6502 - val_accuracy: 0.7570\n",
            "Epoch 6/10\n",
            "11055/11055 - 2286s - loss: 0.2371 - accuracy: 0.9028 - val_loss: 0.6628 - val_accuracy: 0.7553\n",
            "Epoch 7/10\n",
            "11055/11055 - 2286s - loss: 0.2152 - accuracy: 0.9131 - val_loss: 0.7298 - val_accuracy: 0.7532\n",
            "Epoch 8/10\n",
            "11055/11055 - 2287s - loss: 0.1963 - accuracy: 0.9218 - val_loss: 0.7284 - val_accuracy: 0.7539\n",
            "Epoch 9/10\n",
            "11055/11055 - 2285s - loss: 0.1788 - accuracy: 0.9290 - val_loss: 0.7987 - val_accuracy: 0.7509\n",
            "Epoch 10/10\n",
            "11055/11055 - 2283s - loss: 0.1641 - accuracy: 0.9355 - val_loss: 0.8397 - val_accuracy: 0.7494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvTTPSr09nmm",
        "outputId": "e5511616-a646-4cdc-c20a-706e815cac6c"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_attention (InputLayer)    [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
            "                                                                 input_attention[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem (Slici (None, 768)          0           tf_distil_bert_model[0][7]       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          393728      tf.__operators__.getitem[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 512)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            513         dropout_19[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 66,757,121\n",
            "Trainable params: 66,757,121\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4guV1gaKUFyd"
      },
      "source": [
        "# save this model\n",
        "# !mkdir -p distilbert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "lP9nMIkAd-E8",
        "outputId": "5dc1dc26-93dd-4ae6-eec2-c42461c915c3"
      },
      "source": [
        "import torch\n",
        "torch.save(model2, 'model2_saved')\n",
        "\n",
        "# saved_model = torch.load('path/to/model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-99b1b503fc3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model2_saved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# saved_model = torch.load('path/to/model')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWAlDVF9YHJF"
      },
      "source": [
        "# hyper parameter values to try\n",
        "lr_vals = [5e-7, 5e-6, 5e-5, 5e-4, 5e-3, 5e-2, 5e-1] \n",
        "EPOCHS = 6\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index) // BATCH_SIZE\n",
        "\n",
        "\n",
        "LEARNING_RATE_TUNE0 = lr_vals[0]\n",
        "\n",
        "def build_model_tune0(transformer, max_length=MAX_LENGTH, l2reg = L2REG, dropout_rate = LAYER_DROPOUT, lr = LEARNING_RATE_TUNE0):\n",
        "    \"\"\"\n",
        "    Template for building a model off of the BERT or DistilBERT architecture\n",
        "    for a binary classification task.\n",
        "    \n",
        "    Input:\n",
        "      - transformer:  a base Hugging Face transformer model object (BERT or DistilBERT)\n",
        "                      with no added classification head attached.\n",
        "      - max_length:   integer controlling the maximum number of encoded tokens \n",
        "                      in a given sequence.\n",
        "    \n",
        "    Output:\n",
        "      - model:        a compiled tf.keras.Model with added classification layers \n",
        "                      on top of the base pre-trained model architecture.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define weight initializer with a random seed to ensure reproducibility\n",
        "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
        "    \n",
        "    # Define input layers\n",
        "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                            name='input_ids', \n",
        "                                            dtype='int32')\n",
        "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                                  name='input_attention', \n",
        "                                                  dtype='int32')\n",
        "    \n",
        "    # DistilBERT outputs a tuple where the first element at index 0\n",
        "    # represents the hidden-state at the output of the model's last layer.\n",
        "    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n",
        "    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n",
        "    \n",
        "    # We only care about DistilBERT's output for the [CLS] token, \n",
        "    # which is located at index 0 of every encoded sequence.  \n",
        "    # Splicing out the [CLS] tokens gives us 2D data.\n",
        "    cls_token = last_hidden_state[:, 0, :]\n",
        "    \n",
        "    ##                                                 ##\n",
        "    ## Define additional dropout and dense layers here ##\n",
        "    ##                                                 ##\n",
        "    dense = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer= tf.keras.regularizers.l2(l2reg))(cls_token)\n",
        "    dropout= tf.keras.layers.Dropout(dropout_rate)(dense)\n",
        "    \n",
        "    # Define a single node that makes up the output layer (for binary classification)\n",
        "    output = tf.keras.layers.Dense(1, \n",
        "                                   activation='sigmoid',\n",
        "                                   kernel_initializer=weight_initializer,  \n",
        "                                   kernel_constraint=None,\n",
        "                                   bias_initializer='zeros'\n",
        "                                   )(dropout)\n",
        "    \n",
        "    # Define the model\n",
        "    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=lr), \n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UmhklWkMZc0",
        "outputId": "58c90795-cf31-4e0a-8b31-8206cb75b9da"
      },
      "source": [
        "hyperparam_models = []\n",
        "train_histories = []\n",
        "\n",
        "for lr in lr_vals:\n",
        "  model3 = build_model_tune0(distilBERT, max_length=MAX_LENGTH, l2reg = L2REG, dropout_rate = LAYER_DROPOUT, lr = lr)\n",
        "\n",
        "  train_history3 = model3.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")\n",
        "  \n",
        "  hyperparam_models.append(model3)\n",
        "  train_histories.append(train_history3)\n",
        "\n",
        "  print(\"Learning Rate: \", lr)\n",
        "  print()\n",
        "  print(model3.summary())\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8e50d33ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8e50d33ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f8e6c5e4c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f8e6c5e4c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convertWARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "\n",
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 871s - loss: 6.2524 - accuracy: 0.5203 - val_loss: 5.6675 - val_accuracy: 0.5885\n",
            "Epoch 2/6\n",
            "11055/11055 - 867s - loss: 5.1588 - accuracy: 0.5595 - val_loss: 4.6573 - val_accuracy: 0.6148\n",
            "Epoch 3/6\n",
            "11055/11055 - 873s - loss: 4.2299 - accuracy: 0.5814 - val_loss: 3.8060 - val_accuracy: 0.6243\n",
            "Epoch 4/6\n",
            "11055/11055 - 872s - loss: 3.4535 - accuracy: 0.5938 - val_loss: 3.1005 - val_accuracy: 0.6289\n",
            "Epoch 5/6\n",
            "11055/11055 - 869s - loss: 2.8154 - accuracy: 0.6017 - val_loss: 2.5265 - val_accuracy: 0.6314\n",
            "Epoch 6/6\n",
            "11055/11055 - 869s - loss: 2.3005 - accuracy: 0.6086 - val_loss: 2.0686 - val_accuracy: 0.6345\n",
            "Learning Rate:  5e-07\n",
            "\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_attention (InputLayer)    [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
            "                                                                 input_attention[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem (Slici (None, 768)          0           tf_distil_bert_model[0][7]       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          393728      tf.__operators__.getitem[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 512)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            513         dropout_19[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 66,757,121\n",
            "Trainable params: 394,241\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 874s - loss: 3.1214 - accuracy: 0.5941 - val_loss: 1.1826 - val_accuracy: 0.6368\n",
            "Epoch 2/6\n",
            "11055/11055 - 868s - loss: 0.8834 - accuracy: 0.6261 - val_loss: 0.7391 - val_accuracy: 0.6489\n",
            "Epoch 3/6\n",
            "11055/11055 - 867s - loss: 0.7175 - accuracy: 0.6324 - val_loss: 0.6803 - val_accuracy: 0.6512\n",
            "Epoch 4/6\n",
            "11055/11055 - 868s - loss: 0.6835 - accuracy: 0.6354 - val_loss: 0.6610 - val_accuracy: 0.6536\n",
            "Epoch 5/6\n",
            "11055/11055 - 868s - loss: 0.6703 - accuracy: 0.6374 - val_loss: 0.6521 - val_accuracy: 0.6546\n",
            "Epoch 6/6\n",
            "11055/11055 - 868s - loss: 0.6634 - accuracy: 0.6389 - val_loss: 0.6467 - val_accuracy: 0.6552\n",
            "Learning Rate:  5e-06\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_attention (InputLayer)    [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
            "                                                                 input_attention[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_1 (Sli (None, 768)          0           tf_distil_bert_model[1][7]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512)          393728      tf.__operators__.getitem_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 512)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            513         dropout_20[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 66,757,121\n",
            "Trainable params: 394,241\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 874s - loss: 0.9514 - accuracy: 0.6282 - val_loss: 0.6422 - val_accuracy: 0.6487\n",
            "Epoch 2/6\n",
            "11055/11055 - 869s - loss: 0.6489 - accuracy: 0.6403 - val_loss: 0.6281 - val_accuracy: 0.6607\n",
            "Epoch 3/6\n",
            "11055/11055 - 867s - loss: 0.6411 - accuracy: 0.6459 - val_loss: 0.6259 - val_accuracy: 0.6589\n",
            "Epoch 4/6\n",
            "11055/11055 - 866s - loss: 0.6362 - accuracy: 0.6500 - val_loss: 0.6172 - val_accuracy: 0.6723\n",
            "Epoch 5/6\n",
            "11055/11055 - 868s - loss: 0.6332 - accuracy: 0.6534 - val_loss: 0.6174 - val_accuracy: 0.6681\n",
            "Epoch 6/6\n",
            "11055/11055 - 869s - loss: 0.6306 - accuracy: 0.6560 - val_loss: 0.6131 - val_accuracy: 0.6753\n",
            "Learning Rate:  5e-05\n",
            "\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_attention (InputLayer)    [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
            "                                                                 input_attention[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_2 (Sli (None, 768)          0           tf_distil_bert_model[2][7]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          393728      tf.__operators__.getitem_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 512)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            513         dropout_21[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 66,757,121\n",
            "Trainable params: 394,241\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 879s - loss: 0.6919 - accuracy: 0.6226 - val_loss: 0.6373 - val_accuracy: 0.6535\n",
            "Epoch 2/6\n",
            "11055/11055 - 872s - loss: 0.6508 - accuracy: 0.6359 - val_loss: 0.6312 - val_accuracy: 0.6553\n",
            "Epoch 3/6\n",
            "11055/11055 - 873s - loss: 0.6462 - accuracy: 0.6406 - val_loss: 0.6316 - val_accuracy: 0.6574\n",
            "Epoch 4/6\n",
            "11055/11055 - 874s - loss: 0.6442 - accuracy: 0.6425 - val_loss: 0.6334 - val_accuracy: 0.6528\n",
            "Epoch 5/6\n",
            "11055/11055 - 874s - loss: 0.6442 - accuracy: 0.6418 - val_loss: 0.6188 - val_accuracy: 0.6708\n",
            "Epoch 6/6\n",
            "11055/11055 - 871s - loss: 0.6433 - accuracy: 0.6419 - val_loss: 0.6263 - val_accuracy: 0.6599\n",
            "Learning Rate:  0.0005\n",
            "\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_attention (InputLayer)    [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
            "                                                                 input_attention[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3 (Sli (None, 768)          0           tf_distil_bert_model[3][7]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 512)          393728      tf.__operators__.getitem_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 512)          0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1)            513         dropout_22[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 66,757,121\n",
            "Trainable params: 394,241\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 880s - loss: 0.6977 - accuracy: 0.5073 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 2/6\n",
            "11055/11055 - 874s - loss: 0.6932 - accuracy: 0.4994 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 3/6\n",
            "11055/11055 - 873s - loss: 0.6932 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 4/6\n",
            "11055/11055 - 874s - loss: 0.6932 - accuracy: 0.4997 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 5/6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wg5XI1uTFl1"
      },
      "source": [
        "# had to rerun some of them\n",
        "lr_vals = [5e-4, 5e-3, 5e-2, 5e-1]\n",
        "\n",
        "\n",
        "hyperparam_models1 = []\n",
        "train_histories1 = []\n",
        "\n",
        "for lr in lr_vals:\n",
        "  model3 = build_model_tune0(distilBERT, max_length=MAX_LENGTH, l2reg = L2REG, dropout_rate = LAYER_DROPOUT, lr = lr)\n",
        "\n",
        "  train_history3 = model3.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")\n",
        "  \n",
        "  hyperparam_models1.append(model3)\n",
        "  train_histories1.append(train_history3)\n",
        "\n",
        "  print(\"Learning Rate: \", lr)\n",
        "  print()\n",
        "  print(model3.summary())\n",
        "\n",
        "\n",
        "\n",
        "# which learning rate was the best? \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFT07yn862Xz"
      },
      "source": [
        "# add other metrics - accuracy, precision, recall, f1\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}