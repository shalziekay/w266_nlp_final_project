{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-2 DistilBERT_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08ac1d13fd744dfda1cdd16d56fed7f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_087924bbeffa481682c1971e35254665",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_10316054cdd749fea235913a0930ae6a",
              "IPY_MODEL_e8e94338603d4ca68b3c2832e4b31525"
            ]
          }
        },
        "087924bbeffa481682c1971e35254665": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "10316054cdd749fea235913a0930ae6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e231579631584c61aa96e0413381cd26",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8551bd3d533e431797b4038e81698758"
          }
        },
        "e8e94338603d4ca68b3c2832e4b31525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_355702c0f5a045a8bcf4c334ac163bed",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:04&lt;00:00, 50.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67258beb65c74e68a447d873a758a3b7"
          }
        },
        "e231579631584c61aa96e0413381cd26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8551bd3d533e431797b4038e81698758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "355702c0f5a045a8bcf4c334ac163bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67258beb65c74e68a447d873a758a3b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0edc4308a23481cb1bf5c2d2b7de47c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bcf45be380ac4091b5080fb7652a2fda",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_250f0cf375564f5dafaa51e6643ce65b",
              "IPY_MODEL_bd8cbba2506f4e3388d43ef18ac565a3"
            ]
          }
        },
        "bcf45be380ac4091b5080fb7652a2fda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "250f0cf375564f5dafaa51e6643ce65b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e650c14bae6d4875bbce3a09c1040b72",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33aa5bc2ad0546279c1bd38612d19af8"
          }
        },
        "bd8cbba2506f4e3388d43ef18ac565a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_842eda05dba04128a3dd61346f470efd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:04&lt;00:00, 104kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6116a9aeecc49aea33f2b82f7a8540a"
          }
        },
        "e650c14bae6d4875bbce3a09c1040b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33aa5bc2ad0546279c1bd38612d19af8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "842eda05dba04128a3dd61346f470efd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6116a9aeecc49aea33f2b82f7a8540a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "47fbd7f94ba5437d9c3f1ddf59b8f036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3a16026d2bbd4482bb5db79680957450",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4b0ca8d0b44043d1918c39c642d8397b",
              "IPY_MODEL_61ca65795a0f438083bdb4250fa2f968"
            ]
          }
        },
        "3a16026d2bbd4482bb5db79680957450": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b0ca8d0b44043d1918c39c642d8397b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0ba438db4d174170a127b9ec0576abf8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_679a3f218773432e8d87752cc7a5a41d"
          }
        },
        "61ca65795a0f438083bdb4250fa2f968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e0d45b66f9ca4646b890200fc6d06f6a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:04&lt;00:00, 6.46B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8650c1a460f44ddbfe1e308749872bc"
          }
        },
        "0ba438db4d174170a127b9ec0576abf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "679a3f218773432e8d87752cc7a5a41d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0d45b66f9ca4646b890200fc6d06f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8650c1a460f44ddbfe1e308749872bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01de4c429b704516838f187999b04470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_da016dedd0cb478f8e412ff0d83d1012",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_02fd3614f992421f919c9ed80183a963",
              "IPY_MODEL_50fe9ca791464a5d9e5d79a7274fce24"
            ]
          }
        },
        "da016dedd0cb478f8e412ff0d83d1012": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "02fd3614f992421f919c9ed80183a963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c06b233653d74211b2bccff6d4d665e3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 363423424,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 363423424,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93c3d4340fae4dff8320f62b3cd4ab2c"
          }
        },
        "50fe9ca791464a5d9e5d79a7274fce24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d21d73fc8e6842f78f56799e9fba880c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 363M/363M [00:06&lt;00:00, 53.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dc54061797eb43e485a9323d7d5ee303"
          }
        },
        "c06b233653d74211b2bccff6d4d665e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93c3d4340fae4dff8320f62b3cd4ab2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d21d73fc8e6842f78f56799e9fba880c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dc54061797eb43e485a9323d7d5ee303": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06ae658c272449b685afe385194a78c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b14d35d3df334305bcf9d486ce7056a2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dacda2bfcf564b26b960593cff800fee",
              "IPY_MODEL_545e57b7f03440f2b019acc0b0073c8a"
            ]
          }
        },
        "b14d35d3df334305bcf9d486ce7056a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dacda2bfcf564b26b960593cff800fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d77e650dbd78474b98ff425429148341",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 363423424,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 363423424,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1e43b241eef94b7b99be0b1dc68e486e"
          }
        },
        "545e57b7f03440f2b019acc0b0073c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fd7b8a07187746619c38ddbdf7df8917",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 363M/363M [00:07&lt;00:00, 49.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1cc944645f8740b2bb6862106d8294fc"
          }
        },
        "d77e650dbd78474b98ff425429148341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1e43b241eef94b7b99be0b1dc68e486e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd7b8a07187746619c38ddbdf7df8917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1cc944645f8740b2bb6862106d8294fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrFK2QV22X9N",
        "outputId": "c9428fe4-869c-44e5-e440-d59d7df22d64"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD9MC9OP2ufC",
        "outputId": "f5442526-d087-4ca1-d549-609ea1029cf7"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\r\u001b[K     |▏                               | 10kB 24.9MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 32.3MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 22.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 26.0MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 24.1MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 26.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71kB 17.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81kB 19.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 92kB 18.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 102kB 18.3MB/s eta 0:00:01\r\u001b[K     |█▉                              | 112kB 18.3MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 18.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 133kB 18.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 143kB 18.3MB/s eta 0:00:01\r\u001b[K     |██▌                             | 153kB 18.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 163kB 18.3MB/s eta 0:00:01\r\u001b[K     |██▉                             | 174kB 18.3MB/s eta 0:00:01\r\u001b[K     |███                             | 184kB 18.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 194kB 18.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 204kB 18.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 215kB 18.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 225kB 18.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 235kB 18.3MB/s eta 0:00:01\r\u001b[K     |████                            | 245kB 18.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 256kB 18.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 266kB 18.3MB/s eta 0:00:01\r\u001b[K     |████▌                           | 276kB 18.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 286kB 18.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 296kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 307kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 317kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 327kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 337kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 348kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 358kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 368kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 378kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 389kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 399kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 409kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 419kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 430kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 440kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 450kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 460kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 471kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 481kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 491kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 501kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 512kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 522kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 532kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 542kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 552kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 563kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 573kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 583kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 593kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 604kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 614kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 624kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 634kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 645kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 655kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 665kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 675kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 686kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 696kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 706kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 716kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 727kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 737kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 747kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 757kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 768kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 778kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 788kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 798kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 808kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 819kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 829kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 839kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 849kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 860kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 870kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 880kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 890kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 901kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 911kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 921kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 931kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 942kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 952kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 962kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 972kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 983kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 993kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.0MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.0MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.0MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.0MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.0MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.1MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.1MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.1MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.1MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.1MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.1MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.1MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.1MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.2MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.2MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.2MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.2MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.2MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.2MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.2MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.2MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.3MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.3MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.3MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.3MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.3MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.3MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.3MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.3MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.3MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.4MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.4MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.4MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.4MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.4MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.4MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.4MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.4MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.5MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.5MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.5MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.5MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.5MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.5MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.5MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.5MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.6MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.6MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.6MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.6MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.6MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.6MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.6MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.6MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.6MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.6MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.7MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.7MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.7MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.7MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.7MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.7MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.7MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.8MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.8MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.8MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.8MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.8MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.8MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.8MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.8MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.8MB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.8MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.9MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.9MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.9MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.9MB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.9MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.9MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.9MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.9MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.9MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.0MB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.0MB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.0MB 18.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 49.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=97e11b86bc65262fadbc733fd94ceaa4631ccfbfcce70fb0e16780e6980314ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVK7DgyC201T"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ACV5v9rUw-g"
      },
      "source": [
        "referring to this article: https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx1crfXk3nUn"
      },
      "source": [
        "Read the data in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko-3OTNFmVqV"
      },
      "source": [
        "lav_path = '/content/gdrive/MyDrive/W266Project_Lav_Shalz/train-balanced-sarcasm.csv'\n",
        "shalz_path = '/content/gdrive/MyDrive/Colab Notebooks/train-balanced-sarcasm.csv'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBIqTyoV2w8q"
      },
      "source": [
        "df = pd.read_csv(shalz_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMCrcfn04CE0",
        "outputId": "c0f45709-a424-4f74-f6d1-99cf41959913"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1010826, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yCgNSLq4D_N",
        "outputId": "b2515743-b412-4ad4-bd61-53087c245b2d"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label              0\n",
              "comment           53\n",
              "author             0\n",
              "subreddit          0\n",
              "score              0\n",
              "ups                0\n",
              "downs              0\n",
              "date               0\n",
              "created_utc        0\n",
              "parent_comment     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0TcQqEV4Fbd",
        "outputId": "736a9f74-42b5-4700-a669-ac5c0631e891"
      },
      "source": [
        "df = df[df['comment'].notna()]\n",
        "df.isna().sum()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label             0\n",
              "comment           0\n",
              "author            0\n",
              "subreddit         0\n",
              "score             0\n",
              "ups               0\n",
              "downs             0\n",
              "date              0\n",
              "created_utc       0\n",
              "parent_comment    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PUVC-zV4HB_",
        "outputId": "90b24d9a-80d5-4ae4-98f1-5856d552d358"
      },
      "source": [
        "# check label distribution after removing NA\n",
        "df['label'].value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    505405\n",
              "1    505368\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKO0DyLdpGIR"
      },
      "source": [
        "In this notebook, we want to do hyperparameter tuning in order to improve our model. This means that unlike before, we want to be able to use all of the data we have to build the model and then tune the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FbuHcrh4IjN"
      },
      "source": [
        "# # select a fraction of the data\n",
        "# s0 = df.label[df.label.eq(0)].sample(505368).index\n",
        "# s1 = df.label[df.label.eq(1)].sample(505368).index \n",
        "\n",
        "# df = df.loc[s0.union(s1)]\n",
        "# df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "08ac1d13fd744dfda1cdd16d56fed7f6",
            "087924bbeffa481682c1971e35254665",
            "10316054cdd749fea235913a0930ae6a",
            "e8e94338603d4ca68b3c2832e4b31525",
            "e231579631584c61aa96e0413381cd26",
            "8551bd3d533e431797b4038e81698758",
            "355702c0f5a045a8bcf4c334ac163bed",
            "67258beb65c74e68a447d873a758a3b7",
            "f0edc4308a23481cb1bf5c2d2b7de47c",
            "bcf45be380ac4091b5080fb7652a2fda",
            "250f0cf375564f5dafaa51e6643ce65b",
            "bd8cbba2506f4e3388d43ef18ac565a3",
            "e650c14bae6d4875bbce3a09c1040b72",
            "33aa5bc2ad0546279c1bd38612d19af8",
            "842eda05dba04128a3dd61346f470efd",
            "a6116a9aeecc49aea33f2b82f7a8540a",
            "47fbd7f94ba5437d9c3f1ddf59b8f036",
            "3a16026d2bbd4482bb5db79680957450",
            "4b0ca8d0b44043d1918c39c642d8397b",
            "61ca65795a0f438083bdb4250fa2f968",
            "0ba438db4d174170a127b9ec0576abf8",
            "679a3f218773432e8d87752cc7a5a41d",
            "e0d45b66f9ca4646b890200fc6d06f6a",
            "c8650c1a460f44ddbfe1e308749872bc"
          ]
        },
        "id": "lixVjl304KLY",
        "outputId": "8fa4d277-1466-4f1f-b72c-653e813cb809"
      },
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "# model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "# model.layers\n",
        "\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08ac1d13fd744dfda1cdd16d56fed7f6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0edc4308a23481cb1bf5c2d2b7de47c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47fbd7f94ba5437d9c3f1ddf59b8f036",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYlJux8f_Gnv"
      },
      "source": [
        "# check what max length should be based on the sentence lengths in the full data\n",
        "\n",
        "comment_lengths = list(df['comment'].str.split().apply(len))\n",
        "parent_comment_lengths = list(df['parent_comment'].str.split().apply(len))\n",
        "total_comment_lengths = [a + b for a, b in zip(comment_lengths, parent_comment_lengths)]\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8pRN1T2Gntz",
        "outputId": "bddda793-90da-454c-84e9-c4ea48f6124b"
      },
      "source": [
        "print(\"Comment Length Distribution\")\n",
        "print(min(comment_lengths))\n",
        "print(np.percentile(comment_lengths, [25, 50, 75]))\n",
        "print(max(comment_lengths))\n",
        "\n",
        "\n",
        "print(\"Parent Comment Length Distribution\")\n",
        "print(min(parent_comment_lengths))\n",
        "print(np.percentile(parent_comment_lengths, [25, 50, 75]))\n",
        "print(max(parent_comment_lengths))\n",
        "\n",
        "\n",
        "print(\"Total Comment Length Distribution\")\n",
        "print(min(total_comment_lengths))\n",
        "print(np.percentile(total_comment_lengths, [25, 50, 75]))\n",
        "print(max(total_comment_lengths))\n",
        "\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Comment Length Distribution\n",
            "1\n",
            "[ 5.  9. 14.]\n",
            "2222\n",
            "Parent Comment Length Distribution\n",
            "1\n",
            "[ 8. 14. 26.]\n",
            "4198\n",
            "Total Comment Length Distribution\n",
            "2\n",
            "[16. 24. 40.]\n",
            "4444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npfChVayH3R8"
      },
      "source": [
        "We have some sentences that are very long, but most of the data (75% percentile) is below 50 so we will use this as our max_length value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZspzq0K7nAY"
      },
      "source": [
        "Trying out the tokenizer in order to the two methods we want to try out: \n",
        "\n",
        "`Approach A: [CLS] [comment] [SEP] [Masking]\n",
        "id: 0`\n",
        "\n",
        "\n",
        "`Approach B: [CLS] [parent_comment] [SEP] [comment] [SEP] [Masking]`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkG2V4bwCUBt",
        "outputId": "2c20bdde-fdf4-40e1-9b7d-8d0ec28f0277"
      },
      "source": [
        "# understanding the tokenizer\n",
        "temp_sentence = df[\"comment\"][10]\n",
        "temp_parent_comment = df[\"parent_comment\"][10]\n",
        "print(temp_sentence)\n",
        "print(temp_parent_comment)\n",
        "temp_tokens = tokenizer.tokenize(temp_sentence)\n",
        "print(temp_tokens)\n",
        "print(temp_parent_comment)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I think a significant amount would be against spending their tax dollars on other people.\n",
            "I bet if that money was poured into college debt or health debt relief, 81% of Americans would have been for it instead.\n",
            "['i', 'think', 'a', 'significant', 'amount', 'would', 'be', 'against', 'spending', 'their', 'tax', 'dollars', 'on', 'other', 'people', '.']\n",
            "I bet if that money was poured into college debt or health debt relief, 81% of Americans would have been for it instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zyL3NfC9Btu",
        "outputId": "fc1a9d9f-9c84-4473-b17e-cb7ae65bf979"
      },
      "source": [
        "inputs = tokenizer(temp_sentence,\n",
        "          padding = 'max_length', max_length = 50, truncation = True)\n",
        "\n",
        "inputs\n",
        "# 101 at the beginning is the CLS token\n",
        "# 102 in between comment and parent comment is SEP token\n",
        "# 0 is padding based on the max_length"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1045, 2228, 1037, 3278, 3815, 2052, 2022, 2114, 5938, 2037, 4171, 6363, 2006, 2060, 2111, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "ZZf_28rWIXXg",
        "outputId": "867e5f43-6487-4adf-cddb-8e8896ee7654"
      },
      "source": [
        "encoded_sequence = inputs[\"input_ids\"]\n",
        "encoded_sequence\n",
        "decoded_sequence = tokenizer.decode(encoded_sequence)\n",
        "decoded_sequence"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] i think a significant amount would be against spending their tax dollars on other people. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pR-AcWlInLQ",
        "outputId": "52047878-221a-4b6a-890d-1d1a78e18308"
      },
      "source": [
        "inputs = tokenizer([[temp_sentence, temp_parent_comment]],\n",
        "          padding = 'max_length', max_length = 50, truncation = True)\n",
        "inputs"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 1045, 2228, 1037, 3278, 3815, 2052, 2022, 2114, 5938, 2037, 4171, 6363, 2006, 2060, 2111, 1012, 102, 1045, 6655, 2065, 2008, 2769, 2001, 8542, 2046, 2267, 7016, 2030, 2740, 7016, 4335, 1010, 6282, 1003, 1997, 4841, 2052, 2031, 2042, 2005, 2009, 2612, 1012, 102, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msIiOrEgNDuq"
      },
      "source": [
        "Diff version of tokenizing that we would do below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCMa2Uv1JRVw",
        "outputId": "b22210ee-c464-4e7e-8b93-488ea1ca6ef5"
      },
      "source": [
        "MAX_LENGTH = 30\n",
        "tokenizer.batch_encode_plus(temp_tokens,\n",
        "                            max_length=MAX_LENGTH,\n",
        "                            padding='longest', #implements dynamic padding\n",
        "                            truncation=True,\n",
        "                            return_attention_mask=True,\n",
        "                            return_token_type_ids=False\n",
        "                            )"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 1045, 102], [101, 2228, 102], [101, 1037, 102], [101, 3278, 102], [101, 3815, 102], [101, 2052, 102], [101, 2022, 102], [101, 2114, 102], [101, 5938, 102], [101, 2037, 102], [101, 4171, 102], [101, 6363, 102], [101, 2006, 102], [101, 2060, 102], [101, 2111, 102], [101, 1012, 102]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKW29H9zDA2Q",
        "outputId": "e5bf0293-9d00-43a4-9d69-d21a88bfbeb9"
      },
      "source": [
        "tokenizer(temp_sentence,\n",
        "          padding = 'max_length', \n",
        "          max_length = MAX_LENGTH, \n",
        "          truncation = True,\n",
        "          return_attention_mask=True,\n",
        "          return_token_type_ids=False\n",
        "          )"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1045, 2228, 1037, 3278, 3815, 2052, 2022, 2114, 5938, 2037, 4171, 6363, 2006, 2060, 2111, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdZty739Fuke"
      },
      "source": [
        "Let's split the data in train, val, test and then tokenize all of it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BnOEuHk4Pd3"
      },
      "source": [
        "X_train, temp_text, y_train, temp_labels = train_test_split(df['comment'], df['label'], \n",
        "                                                                    random_state=0, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "X_val, X_test, y_val, y_test = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=0, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scgD-GG66RG7"
      },
      "source": [
        "MAX_LENGTH = 50"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "111Gpwj88zl6"
      },
      "source": [
        "def batch_encode(tokenizer, texts, batch_size=256, max_length=MAX_LENGTH):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        # inputs = tokenizer.batch_encode_plus(batch,\n",
        "        #                                      max_length=max_length,\n",
        "        #                                      padding='longest', #implements dynamic padding\n",
        "        #                                      truncation=True,\n",
        "        #                                      return_attention_mask=True,\n",
        "        #                                      return_token_type_ids=False\n",
        "        #                                      )\n",
        "\n",
        "        inputs = tokenizer(batch,\n",
        "                          padding = 'max_length', \n",
        "                          max_length = MAX_LENGTH, \n",
        "                          truncation = True,\n",
        "                          return_attention_mask=True,\n",
        "                          return_token_type_ids=False\n",
        "                          )\n",
        "        \n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "  "
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ918eOn8zjy"
      },
      "source": [
        "### This cell takes a few minutes to run\n",
        "\n",
        "# Encode X_train\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, X_train.tolist())\n",
        "\n",
        "# Encode X_valid\n",
        "X_val_ids, X_val_attention = batch_encode(tokenizer, X_val.tolist())\n",
        "\n",
        "# Encode X_test\n",
        "X_test_ids, X_test_attention = batch_encode(tokenizer, X_test.tolist())"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "01de4c429b704516838f187999b04470",
            "da016dedd0cb478f8e412ff0d83d1012",
            "02fd3614f992421f919c9ed80183a963",
            "50fe9ca791464a5d9e5d79a7274fce24",
            "c06b233653d74211b2bccff6d4d665e3",
            "93c3d4340fae4dff8320f62b3cd4ab2c",
            "d21d73fc8e6842f78f56799e9fba880c",
            "dc54061797eb43e485a9323d7d5ee303"
          ]
        },
        "id": "rrHTrnlg8zhu",
        "outputId": "e77ca8a8-5cee-4a6a-9066-2611a8580e05"
      },
      "source": [
        "DISTILBERT_DROPOUT = 0.2\n",
        "DISTILBERT_ATT_DROPOUT = 0.2\n",
        " \n",
        "# Configure DistilBERT's initialization\n",
        "config = DistilBertConfig(dropout=DISTILBERT_DROPOUT, \n",
        "                          attention_dropout=DISTILBERT_ATT_DROPOUT, \n",
        "                          output_hidden_states=True)\n",
        "                          \n",
        "# The bare, pre-trained DistilBERT transformer model outputting raw hidden-states \n",
        "# and without any specific head on top.\n",
        "distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
        "\n",
        "# Make DistilBERT layers untrainable\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01de4c429b704516838f187999b04470",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilOjuQJjq4-7"
      },
      "source": [
        "Trying to understand the DistilBERT model layers a bit more"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INVIjMelquxi",
        "outputId": "b05fe7a3-10e2-4b76-d3ce-0f9f419d825b"
      },
      "source": [
        "distilBERT.layers"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer at 0x7f864e154ed0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8ol64fyq-lE",
        "outputId": "0cac5e89-76bf-4fef-f834-62a8c394baf6"
      },
      "source": [
        "len(distilBERT.layers[0].weights)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkTWvOj6rC8O"
      },
      "source": [
        "We have 100 layers in the model, let's look at the first 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN9m1lf3rGlv",
        "outputId": "40040fbd-6142-4b6f-819b-a6f9c8486165"
      },
      "source": [
        "for layer in range(10):\n",
        "    print(layer)\n",
        "    print('Layer name: \\t', distilBERT.layers[0].weights[layer].name)\n",
        "    print('Layer shape: \\t', distilBERT.layers[0].weights[layer].shape)\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Layer name: \t tf_distil_bert_model/distilbert/embeddings/word_embeddings/weight:0\n",
            "Layer shape: \t (30522, 768)\n",
            "1\n",
            "Layer name: \t tf_distil_bert_model/distilbert/embeddings/position_embeddings/embeddings:0\n",
            "Layer shape: \t (512, 768)\n",
            "2\n",
            "Layer name: \t tf_distil_bert_model/distilbert/embeddings/LayerNorm/gamma:0\n",
            "Layer shape: \t (768,)\n",
            "3\n",
            "Layer name: \t tf_distil_bert_model/distilbert/embeddings/LayerNorm/beta:0\n",
            "Layer shape: \t (768,)\n",
            "4\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/kernel:0\n",
            "Layer shape: \t (768, 768)\n",
            "5\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/bias:0\n",
            "Layer shape: \t (768,)\n",
            "6\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/kernel:0\n",
            "Layer shape: \t (768, 768)\n",
            "7\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/bias:0\n",
            "Layer shape: \t (768,)\n",
            "8\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/kernel:0\n",
            "Layer shape: \t (768, 768)\n",
            "9\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/bias:0\n",
            "Layer shape: \t (768,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWZR5H-grSmU"
      },
      "source": [
        "Last 5 layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iXo8or0rRuz",
        "outputId": "52b0ec3a-5d36-4772-f73b-f03732f53dec"
      },
      "source": [
        "for layer in [99, 98, 97, 96, 95]:\n",
        "    print(layer)\n",
        "    print('Layer name: \\t', distilBERT.layers[0].weights[layer].name)\n",
        "    print('Layer shape: \\t', distilBERT.layers[0].weights[layer].shape)\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/beta:0\n",
            "Layer shape: \t (768,)\n",
            "98\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/gamma:0\n",
            "Layer shape: \t (768,)\n",
            "97\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/bias:0\n",
            "Layer shape: \t (768,)\n",
            "96\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/kernel:0\n",
            "Layer shape: \t (3072, 768)\n",
            "95\n",
            "Layer name: \t tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin1/bias:0\n",
            "Layer shape: \t (3072,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goWXf-YwtIps"
      },
      "source": [
        "We see the embedding layer which maps the token id to a 768 dim vector.\n",
        "Next is the positional encoding which encodes the 512 BERT input positions. \n",
        "Layers 5-10 hold the weights and biases for the first self-attention layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3n6Mfo48zfZ"
      },
      "source": [
        "LAYER_DROPOUT = 0.2\n",
        "LEARNING_RATE = 5e-5\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "def build_model(transformer, max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Template for building a model off of the BERT or DistilBERT architecture\n",
        "    for a binary classification task.\n",
        "    \n",
        "    Input:\n",
        "      - transformer:  a base Hugging Face transformer model object (BERT or DistilBERT)\n",
        "                      with no added classification head attached.\n",
        "      - max_length:   integer controlling the maximum number of encoded tokens \n",
        "                      in a given sequence.\n",
        "    \n",
        "    Output:\n",
        "      - model:        a compiled tf.keras.Model with added classification layers \n",
        "                      on top of the base pre-trained model architecture.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define weight initializer with a random seed to ensure reproducibility\n",
        "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
        "    \n",
        "    # Define input layers\n",
        "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                            name='input_ids', \n",
        "                                            dtype='int32')\n",
        "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                                  name='input_attention', \n",
        "                                                  dtype='int32')\n",
        "    \n",
        "    # DistilBERT outputs a tuple where the first element at index 0\n",
        "    # represents the hidden-state at the output of the model's last layer.\n",
        "    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n",
        "    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n",
        "    \n",
        "    # We only care about DistilBERT's output for the [CLS] token, \n",
        "    # which is located at index 0 of every encoded sequence.  \n",
        "    # Splicing out the [CLS] tokens gives us 2D data.\n",
        "    cls_token = last_hidden_state[:, 0, :]\n",
        "    \n",
        "    ##                                                 ##\n",
        "    ## Define additional dropout and dense layers here ##\n",
        "    ##                                                 ##\n",
        "    \n",
        "    # Define a single node that makes up the output layer (for binary classification)\n",
        "    output = tf.keras.layers.Dense(1, \n",
        "                                   activation='sigmoid',\n",
        "                                   kernel_initializer=weight_initializer,  \n",
        "                                   kernel_constraint=None,\n",
        "                                   bias_initializer='zeros'\n",
        "                                   )(cls_token)\n",
        "    \n",
        "    # Define the model\n",
        "    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), \n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfMV7E5LOcdR",
        "outputId": "4912e0b9-97b2-4a19-d28a-4cedb1d9569a"
      },
      "source": [
        "model = build_model(distilBERT)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8743b84d70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8743b84d70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f875f431c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f875f431c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KJ7L6hAOcbH",
        "outputId": "653c9822-5d8d-4cc6-a277-5e0e9cca19a6"
      },
      "source": [
        "EPOCHS = 6\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index) // BATCH_SIZE\n",
        "\n",
        "# Train the model\n",
        "train_history1 = model.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")\n",
        "\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 1427s - loss: 0.6525 - accuracy: 0.6149 - val_loss: 0.6288 - val_accuracy: 0.6450\n",
            "Epoch 2/6\n",
            "11055/11055 - 1419s - loss: 0.6338 - accuracy: 0.6401 - val_loss: 0.6190 - val_accuracy: 0.6563\n",
            "Epoch 3/6\n",
            "11055/11055 - 1419s - loss: 0.6281 - accuracy: 0.6467 - val_loss: 0.6133 - val_accuracy: 0.6615\n",
            "Epoch 4/6\n",
            "11055/11055 - 1420s - loss: 0.6248 - accuracy: 0.6498 - val_loss: 0.6102 - val_accuracy: 0.6655\n",
            "Epoch 5/6\n",
            "11055/11055 - 1420s - loss: 0.6229 - accuracy: 0.6518 - val_loss: 0.6075 - val_accuracy: 0.6677\n",
            "Epoch 6/6\n",
            "11055/11055 - 1420s - loss: 0.6214 - accuracy: 0.6537 - val_loss: 0.6056 - val_accuracy: 0.6689\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UAchTWJwjVC"
      },
      "source": [
        "pd.DataFrame.from_dict(train_history1.history).to_csv('history1.csv',index=False)\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "HcOnCoC3OcYY",
        "outputId": "4ad89ae2-e3eb-4277-abca-2213f7e02ba9"
      },
      "source": [
        "\n",
        "FT_EPOCHS = 4\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index)\n",
        "\n",
        "\n",
        "\n",
        "# Unfreeze distilBERT layers and make available for training\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = True\n",
        "    \n",
        "# Recompile model after unfreezing\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "train_history2 = model.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = FT_EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-0b591aa12eaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_STEPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_val_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_attention\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh89IBPrOcV5"
      },
      "source": [
        "tf.keras.backend.clear_session()\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MWMboupagb5"
      },
      "source": [
        "## Adding additional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJt059xcOcTf"
      },
      "source": [
        "# resetting some of the global parameters here\n",
        "LAYER_DROPOUT = 0.2\n",
        "LEARNING_RATE = 5e-5\n",
        "RANDOM_STATE = 42\n",
        "DISTILBERT_DROPOUT = 0.2\n",
        "DISTILBERT_ATT_DROPOUT = 0.2\n",
        "MAX_LENGTH = 40\n",
        "L2REG = 0.01"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fx0SbYaOcQp"
      },
      "source": [
        "def build_model2(transformer, max_length=MAX_LENGTH, l2reg = L2REG, dropout_rate = LAYER_DROPOUT):\n",
        "    \"\"\"\n",
        "    Template for building a model off of the BERT or DistilBERT architecture\n",
        "    for a binary classification task.\n",
        "    \n",
        "    Input:\n",
        "      - transformer:  a base Hugging Face transformer model object (BERT or DistilBERT)\n",
        "                      with no added classification head attached.\n",
        "      - max_length:   integer controlling the maximum number of encoded tokens \n",
        "                      in a given sequence.\n",
        "    \n",
        "    Output:\n",
        "      - model:        a compiled tf.keras.Model with added classification layers \n",
        "                      on top of the base pre-trained model architecture.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define weight initializer with a random seed to ensure reproducibility\n",
        "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
        "    \n",
        "    # Define input layers\n",
        "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                            name='input_ids', \n",
        "                                            dtype='int32')\n",
        "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                                  name='input_attention', \n",
        "                                                  dtype='int32')\n",
        "    \n",
        "    # DistilBERT outputs a tuple where the first element at index 0\n",
        "    # represents the hidden-state at the output of the model's last layer.\n",
        "    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n",
        "    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n",
        "    \n",
        "    # We only care about DistilBERT's output for the [CLS] token, \n",
        "    # which is located at index 0 of every encoded sequence.  \n",
        "    # Splicing out the [CLS] tokens gives us 2D data.\n",
        "    cls_token = last_hidden_state[:, 0, :]\n",
        "    \n",
        "    ##                                                 ##\n",
        "    ## Define additional dropout and dense layers here ##\n",
        "    ##                                                 ##\n",
        "    dense = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer= tf.keras.regularizers.l2(l2reg))(cls_token)\n",
        "    dropout= tf.keras.layers.Dropout(dropout_rate)(dense)\n",
        "    \n",
        "    # Define a single node that makes up the output layer (for binary classification)\n",
        "    output = tf.keras.layers.Dense(1, \n",
        "                                   activation='sigmoid',\n",
        "                                   kernel_initializer=weight_initializer,  \n",
        "                                   kernel_constraint=None,\n",
        "                                   bias_initializer='zeros'\n",
        "                                   )(dropout)\n",
        "    \n",
        "    # Define the model\n",
        "    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), \n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGk8GQ3wOcOM",
        "outputId": "b4072f85-cc88-4426-fa2f-5960adefaf49"
      },
      "source": [
        "model2 = build_model2(distilBERT)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X21mqvCEOcLf"
      },
      "source": [
        "EPOCHS = 6\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index) // BATCH_SIZE\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9EAKfzTOcIn",
        "outputId": "b9250bc6-f6db-4373-8479-2dfcf665bdbb"
      },
      "source": [
        "# Train the model\n",
        "train_history3 = model2.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 3578s - loss: 0.7288 - accuracy: 0.7964 - val_loss: 0.5064 - val_accuracy: 0.7680\n",
            "Epoch 2/6\n",
            "11055/11055 - 3573s - loss: 0.4023 - accuracy: 0.8194 - val_loss: 0.5135 - val_accuracy: 0.7654\n",
            "Epoch 3/6\n",
            "11055/11055 - 3572s - loss: 0.3594 - accuracy: 0.8423 - val_loss: 0.5569 - val_accuracy: 0.7602\n",
            "Epoch 4/6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "QtK0zrDwEWLR",
        "outputId": "12bfe7ac-67e2-485f-8f8f-ef0dd7706043"
      },
      "source": [
        "pd.DataFrame.from_dict(train_history3.history).to_csv('history2.csv',index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('history2.csv') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e11cae85de3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_history3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'history2.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'history2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvHMpg-jOcFO"
      },
      "source": [
        "# try with diff hyper parameters\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9PZiwtvAOVI"
      },
      "source": [
        "## Add the parent comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XzteZ5uB-tx"
      },
      "source": [
        "tf.keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OV5q5B6CAIP"
      },
      "source": [
        "X_train_p, temp_text, y_train_p, temp_labels = train_test_split(df[['comment', 'parent_comment']], df['label'], \n",
        "                                                                    random_state=0, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "X_val_p, X_test_p, y_val_p, y_test_p = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=0, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "P90ZwSKiCfdT",
        "outputId": "a1f094bf-0977-4a57-d2ea-6af061431f01"
      },
      "source": [
        "X_train_p.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>parent_comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>715875</th>\n",
              "      <td>They do in neutral!</td>\n",
              "      <td>The only main thing you'll notice (vs driving ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348688</th>\n",
              "      <td>But but... He was on Howard Stern voting for t...</td>\n",
              "      <td>Trump was against the war from the very beginn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323243</th>\n",
              "      <td>Kvothe, is that you?</td>\n",
              "      <td>Now imagine a hoodie with a great number of po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56970</th>\n",
              "      <td>Never \"meta\" in pro scene so it must be a shit...</td>\n",
              "      <td>What's wrong with scarab?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267644</th>\n",
              "      <td>I'm almost shocked not to see TJ McConnell here</td>\n",
              "      <td>Best and Worst Catch &amp;amp; Shooters</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  comment                                     parent_comment\n",
              "715875                                They do in neutral!  The only main thing you'll notice (vs driving ...\n",
              "348688  But but... He was on Howard Stern voting for t...  Trump was against the war from the very beginn...\n",
              "323243                               Kvothe, is that you?  Now imagine a hoodie with a great number of po...\n",
              "56970   Never \"meta\" in pro scene so it must be a shit...                          What's wrong with scarab?\n",
              "267644    I'm almost shocked not to see TJ McConnell here                Best and Worst Catch &amp; Shooters"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL8oZ8WEAPyK"
      },
      "source": [
        "MAX_LENGTH = 30"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNgu1WUvabei"
      },
      "source": [
        "# try with parent comment connected\n",
        "def batch_encode_parent(tokenizer, texts, parent, batch_size=256, max_length=MAX_LENGTH):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        parent_batch = parent[i:i+batch_size]\n",
        "\n",
        "        combined = [list(i) for i in zip(parent_batch, batch)]\n",
        "\n",
        "\n",
        "        inputs = tokenizer(combined,\n",
        "                          padding = 'max_length', \n",
        "                          max_length = MAX_LENGTH, \n",
        "                          truncation = True,\n",
        "                          return_attention_mask=True,\n",
        "                          return_token_type_ids=False\n",
        "                          )\n",
        "        \n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "  "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li8R0XVzHE8W",
        "outputId": "b75329c7-8e93-445d-a308-359f44bbd169"
      },
      "source": [
        "# temp1 = X_train_p[\"comment\"].head(2).tolist()\n",
        "# temp2 = X_train_p[\"parent_comment\"].head(2).tolist()\n",
        "\n",
        "# [[temp2, temp1]]\n",
        "\n",
        "# print(temp1)\n",
        "# print(temp2)\n",
        "\n",
        "# # zip(temp1, temp2)\n",
        "\n",
        "\n",
        "# [list(i) for i in zip(temp1, temp2)]\n",
        "\n",
        "\n",
        "# # batch_encode_parent(tokenizer, temp1, temp2)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['They do in neutral!', \"But but... He was on Howard Stern voting for the war and that's even worse than if he were a sitting senator doing the same... Because reasons...\"]\n",
            "[\"The only main thing you'll notice (vs driving a car with automatic transmission) is that the car will slow down a lot more quickly than an automatic would, manuals generally dont coast as freely.\", \"Trump was against the war from the very beginning too. Hillary literally voted for it. It is always such doublethink with the left it's unreal.\"]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['They do in neutral!',\n",
              "  \"The only main thing you'll notice (vs driving a car with automatic transmission) is that the car will slow down a lot more quickly than an automatic would, manuals generally dont coast as freely.\"],\n",
              " [\"But but... He was on Howard Stern voting for the war and that's even worse than if he were a sitting senator doing the same... Because reasons...\",\n",
              "  \"Trump was against the war from the very beginning too. Hillary literally voted for it. It is always such doublethink with the left it's unreal.\"]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLqE2NdBabXY"
      },
      "source": [
        "### This cell takes a few minutes to run\n",
        "\n",
        "# Encode X_train\n",
        "X_train_ids_p, X_train_attention_p = batch_encode_parent(tokenizer, \n",
        "                                                  X_train_p[\"comment\"].tolist(), \n",
        "                                                  X_train_p[\"parent_comment\"].tolist())\n",
        "\n",
        "# Encode X_valid\n",
        "X_val_ids_p, X_val_attention_p = batch_encode_parent(tokenizer, \n",
        "                                              X_val_p[\"comment\"].tolist(), \n",
        "                                              X_val_p[\"parent_comment\"].tolist())\n",
        "\n",
        "# Encode X_test\n",
        "X_test_ids_p, X_test_attention_p = batch_encode_parent(tokenizer, \n",
        "                                                X_test_p[\"comment\"].tolist(), \n",
        "                                                X_test_p[\"parent_comment\"].tolist())\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "06ae658c272449b685afe385194a78c2",
            "b14d35d3df334305bcf9d486ce7056a2",
            "dacda2bfcf564b26b960593cff800fee",
            "545e57b7f03440f2b019acc0b0073c8a",
            "d77e650dbd78474b98ff425429148341",
            "1e43b241eef94b7b99be0b1dc68e486e",
            "fd7b8a07187746619c38ddbdf7df8917",
            "1cc944645f8740b2bb6862106d8294fc"
          ]
        },
        "id": "MvuNdcCrbHIp",
        "outputId": "168dc991-124e-4db4-f8b4-ecaeab77ae12"
      },
      "source": [
        "DISTILBERT_DROPOUT = 0.2\n",
        "DISTILBERT_ATT_DROPOUT = 0.2\n",
        " \n",
        "# Configure DistilBERT's initialization\n",
        "config = DistilBertConfig(dropout=DISTILBERT_DROPOUT, \n",
        "                          attention_dropout=DISTILBERT_ATT_DROPOUT, \n",
        "                          output_hidden_states=True)\n",
        "                          \n",
        "# The bare, pre-trained DistilBERT transformer model outputting raw hidden-states \n",
        "# and without any specific head on top.\n",
        "distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
        "\n",
        "# Make DistilBERT layers untrainable\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = False\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06ae658c272449b685afe385194a78c2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6x9lwTqbHBL"
      },
      "source": [
        "LAYER_DROPOUT = 0.2\n",
        "LEARNING_RATE = 5e-5\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "def build_model_parent(transformer, max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Template for building a model off of the BERT or DistilBERT architecture\n",
        "    for a binary classification task.\n",
        "    \n",
        "    Input:\n",
        "      - transformer:  a base Hugging Face transformer model object (BERT or DistilBERT)\n",
        "                      with no added classification head attached.\n",
        "      - max_length:   integer controlling the maximum number of encoded tokens \n",
        "                      in a given sequence.\n",
        "    \n",
        "    Output:\n",
        "      - model:        a compiled tf.keras.Model with added classification layers \n",
        "                      on top of the base pre-trained model architecture.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define weight initializer with a random seed to ensure reproducibility\n",
        "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
        "    \n",
        "    # Define input layers\n",
        "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                            name='input_ids', \n",
        "                                            dtype='int32')\n",
        "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                                  name='input_attention', \n",
        "                                                  dtype='int32')\n",
        "    \n",
        "    # DistilBERT outputs a tuple where the first element at index 0\n",
        "    # represents the hidden-state at the output of the model's last layer.\n",
        "    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n",
        "    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n",
        "    \n",
        "    # We only care about DistilBERT's output for the [CLS] token, \n",
        "    # which is located at index 0 of every encoded sequence.  \n",
        "    # Splicing out the [CLS] tokens gives us 2D data.\n",
        "    cls_token = last_hidden_state[:, 0, :]\n",
        "    \n",
        "    ##                                                 ##\n",
        "    ## Define additional dropout and dense layers here ##\n",
        "    ##                                                 ##\n",
        "    \n",
        "    # Define a single node that makes up the output layer (for binary classification)\n",
        "    output = tf.keras.layers.Dense(1, \n",
        "                                   activation='sigmoid',\n",
        "                                   kernel_initializer=weight_initializer,  \n",
        "                                   kernel_constraint=None,\n",
        "                                   bias_initializer='zeros'\n",
        "                                   )(cls_token)\n",
        "    \n",
        "    # Define the model\n",
        "    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), \n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CUSicjgbG4k",
        "outputId": "96232fae-4337-4890-ac66-420b78f6b9a8"
      },
      "source": [
        "model_parent = build_model_parent(distilBERT)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f095578ed70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f095578ed70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f097103bc20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f097103bc20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS57r-UjabNm",
        "outputId": "ac7ea420-ea58-43b1-a6d4-8873aab118d5"
      },
      "source": [
        "EPOCHS = 6\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index) // BATCH_SIZE\n",
        "\n",
        "# Train the model\n",
        "train_history_parent1 = model_parent.fit(\n",
        "    x = [X_train_ids_p, X_train_attention_p],\n",
        "    y = y_train_p.to_numpy(),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids_p, X_val_attention_p], y_val_p.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 890s - loss: 0.6686 - accuracy: 0.5892 - val_loss: 0.6557 - val_accuracy: 0.6136\n",
            "Epoch 2/6\n",
            "11055/11055 - 882s - loss: 0.6567 - accuracy: 0.6106 - val_loss: 0.6505 - val_accuracy: 0.6204\n",
            "Epoch 3/6\n",
            "11055/11055 - 882s - loss: 0.6529 - accuracy: 0.6159 - val_loss: 0.6475 - val_accuracy: 0.6238\n",
            "Epoch 4/6\n",
            "11055/11055 - 881s - loss: 0.6511 - accuracy: 0.6187 - val_loss: 0.6454 - val_accuracy: 0.6263\n",
            "Epoch 5/6\n",
            "11055/11055 - 880s - loss: 0.6499 - accuracy: 0.6196 - val_loss: 0.6438 - val_accuracy: 0.6274\n",
            "Epoch 6/6\n",
            "11055/11055 - 880s - loss: 0.6492 - accuracy: 0.6209 - val_loss: 0.6428 - val_accuracy: 0.6284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONCALbZLOb6p"
      },
      "source": [
        "pd.DataFrame.from_dict(train_history_parent1.history).to_csv('parenthistory1.csv',index=False)\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m1JzBIO_cgd",
        "outputId": "d8fe1d85-f8dc-4620-d800-a952b94aefd7"
      },
      "source": [
        "FT_EPOCHS = 4\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index)\n",
        "\n",
        "\n",
        "\n",
        "# Unfreeze distilBERT layers and make available for training\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = True\n",
        "    \n",
        "# Recompile model after unfreezing\n",
        "model_parent.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_history_parent2 = model_parent.fit(\n",
        "    x = [X_train_ids_p, X_train_attention_p],\n",
        "    y = y_train_p.to_numpy(),\n",
        "    epochs = FT_EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    \n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids_p, X_val_attention_p], y_val_p.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2830164 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "707541/707541 - 8796s - loss: 0.4690 - accuracy: 0.7736 - val_loss: 0.5134 - val_accuracy: 0.7647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVt9eyaUD_BC"
      },
      "source": [
        "pd.DataFrame.from_dict(train_history_parent2.history).to_csv('train_history_parent2.csv',index=False)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vusb2Yc5D--x",
        "outputId": "aba841dd-7322-4cde-8c11-0109aaf2e6d0"
      },
      "source": [
        "# maybe change hyperparameters\n",
        "FT_EPOCHS = 4\n",
        "BATCH_SIZE = 32\n",
        "NUM_STEPS = len(X_train.index)\n",
        "\n",
        "\n",
        "\n",
        "# Unfreeze distilBERT layers and make available for training\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = True\n",
        "    \n",
        "# Recompile model after unfreezing\n",
        "model_parent.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-3), \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_history_parent3 = model_parent.fit(\n",
        "    x = [X_train_ids_p, X_train_attention_p],\n",
        "    y = y_train_p.to_numpy(),\n",
        "    epochs = FT_EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    \n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids_p, X_val_attention_p], y_val_p.to_numpy()),\n",
        "    verbose=2\n",
        ")\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2830164 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "707541/707541 - 9831s - loss: 0.6940 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5iAJLhqqI03"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2NvshLuqIyd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3jLJIhCqIwX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdeGvenKqItx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfb7f8vmqIr0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dkboqs5qIpQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvp9WaN6qImt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRhVqpo7D-3H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKsImoEED-0W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgq1d8x7GIo6",
        "outputId": "8da37703-4e5f-4ad6-c264-8110556ac148"
      },
      "source": [
        "model = build_model(distilBERT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f7626466ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f7626466ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f7641d17c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f7641d17c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ12ibAH8zY7",
        "outputId": "e6fee038-27a1-4d93-8c03-efe4d33b521f"
      },
      "source": [
        "EPOCHS = 6\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index) // BATCH_SIZE\n",
        "\n",
        "# Train the model\n",
        "train_history1 = model.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 869s - loss: 0.6538 - accuracy: 0.6120 - val_loss: 0.6299 - val_accuracy: 0.6437\n",
            "Epoch 2/6\n",
            "11055/11055 - 861s - loss: 0.6352 - accuracy: 0.6382 - val_loss: 0.6207 - val_accuracy: 0.6545\n",
            "Epoch 3/6\n",
            "11055/11055 - 861s - loss: 0.6301 - accuracy: 0.6442 - val_loss: 0.6157 - val_accuracy: 0.6595\n",
            "Epoch 4/6\n",
            "11055/11055 - 861s - loss: 0.6274 - accuracy: 0.6468 - val_loss: 0.6127 - val_accuracy: 0.6627\n",
            "Epoch 5/6\n",
            "11055/11055 - 861s - loss: 0.6253 - accuracy: 0.6492 - val_loss: 0.6103 - val_accuracy: 0.6650\n",
            "Epoch 6/6\n",
            "11055/11055 - 862s - loss: 0.6240 - accuracy: 0.6513 - val_loss: 0.6083 - val_accuracy: 0.6658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLFpuUKF8zWN",
        "outputId": "b48dd612-8ed5-40c7-8004-0a5af624c5e5"
      },
      "source": [
        "FT_EPOCHS = 4\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index)\n",
        "\n",
        "# Unfreeze distilBERT layers and make available for training\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = True\n",
        "    \n",
        "# Recompile model after unfreezing\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "train_history2 = model.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = FT_EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2830164 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "707541/707541 - 8453s - loss: 0.4618 - accuracy: 0.7783 - val_loss: 0.5063 - val_accuracy: 0.7696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHhs-Y5t8fE2"
      },
      "source": [
        "All of the above code was run the full data using random parameters. We get a training accuracy of 77.83% and a validation accuracy of 76.96%, which are good signs we are not overfitting.\n",
        "\n",
        "Next, let's try changing the model architecture slightly (adding some dense layers) and hyperparameter tuning the model. Based on some research, the parameters that lead to the biggest change in accuracy are: learning rate, dropout, and batch size so I am going to focus on these parameters. It may also be good to try adding more dense/dropout layers to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlHOongwz-TJ"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_sY-k0iApVk"
      },
      "source": [
        "## Add additional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ULtBTBuHpu0"
      },
      "source": [
        "# resetting some of the global parameters here\n",
        "LAYER_DROPOUT = 0.2\n",
        "LEARNING_RATE = 5e-5\n",
        "RANDOM_STATE = 42\n",
        "DISTILBERT_DROPOUT = 0.2\n",
        "DISTILBERT_ATT_DROPOUT = 0.2\n",
        "MAX_LENGTH = 30\n",
        "L2REG = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5KDpgozAk66"
      },
      "source": [
        "def build_model2(transformer, max_length=MAX_LENGTH, l2reg = L2REG, dropout_rate = LAYER_DROPOUT):\n",
        "    \"\"\"\n",
        "    Template for building a model off of the BERT or DistilBERT architecture\n",
        "    for a binary classification task.\n",
        "    \n",
        "    Input:\n",
        "      - transformer:  a base Hugging Face transformer model object (BERT or DistilBERT)\n",
        "                      with no added classification head attached.\n",
        "      - max_length:   integer controlling the maximum number of encoded tokens \n",
        "                      in a given sequence.\n",
        "    \n",
        "    Output:\n",
        "      - model:        a compiled tf.keras.Model with added classification layers \n",
        "                      on top of the base pre-trained model architecture.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define weight initializer with a random seed to ensure reproducibility\n",
        "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
        "    \n",
        "    # Define input layers\n",
        "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                            name='input_ids', \n",
        "                                            dtype='int32')\n",
        "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                                  name='input_attention', \n",
        "                                                  dtype='int32')\n",
        "    \n",
        "    # DistilBERT outputs a tuple where the first element at index 0\n",
        "    # represents the hidden-state at the output of the model's last layer.\n",
        "    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n",
        "    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n",
        "    \n",
        "    # We only care about DistilBERT's output for the [CLS] token, \n",
        "    # which is located at index 0 of every encoded sequence.  \n",
        "    # Splicing out the [CLS] tokens gives us 2D data.\n",
        "    cls_token = last_hidden_state[:, 0, :]\n",
        "    \n",
        "    ##                                                 ##\n",
        "    ## Define additional dropout and dense layers here ##\n",
        "    ##                                                 ##\n",
        "    dense = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer= tf.keras.regularizers.l2(l2reg))(cls_token)\n",
        "    dropout= tf.keras.layers.Dropout(dropout_rate)(dense)\n",
        "    \n",
        "    # Define a single node that makes up the output layer (for binary classification)\n",
        "    output = tf.keras.layers.Dense(1, \n",
        "                                   activation='sigmoid',\n",
        "                                   kernel_initializer=weight_initializer,  \n",
        "                                   kernel_constraint=None,\n",
        "                                   bias_initializer='zeros'\n",
        "                                   )(dropout)\n",
        "    \n",
        "    # Define the model\n",
        "    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), \n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "    # inps = Input(shape = (max_len,), dtype='int64')\n",
        "    # masks= Input(shape = (max_len,), dtype='int64')\n",
        "    # dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
        "    # dense = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
        "    # dropout= Dropout(0.5)(dense)\n",
        "    # pred = Dense(num_classes, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "    # model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
        "    # print(model.summary())\n",
        "    # return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xHlfFUJI74f",
        "outputId": "5c45cc49-3d5a-4301-f9e5-3525229a3ac6"
      },
      "source": [
        "model2 = build_model2(distilBERT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fb3dea35ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fb3dea35ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fb3fa2e6c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7fb3fa2e6c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_OFU9zRZqvj"
      },
      "source": [
        "EPOCHS = 6\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index) // BATCH_SIZE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q95uv8l2Jgku",
        "outputId": "151150fb-f9ab-405a-ecc5-fbd14fdb7f2e"
      },
      "source": [
        "\n",
        "# Train the model\n",
        "train_history3 = model2.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 2234s - loss: 0.7305 - accuracy: 0.8047 - val_loss: 0.5110 - val_accuracy: 0.7629\n",
            "Epoch 2/6\n",
            "11055/11055 - 2226s - loss: 0.3899 - accuracy: 0.8259 - val_loss: 0.5051 - val_accuracy: 0.7584\n",
            "Epoch 3/6\n",
            "11055/11055 - 2226s - loss: 0.3469 - accuracy: 0.8492 - val_loss: 0.5824 - val_accuracy: 0.7495\n",
            "Epoch 4/6\n",
            "11055/11055 - 2223s - loss: 0.3076 - accuracy: 0.8692 - val_loss: 0.5959 - val_accuracy: 0.7534\n",
            "Epoch 5/6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jihnv3zQYKvI"
      },
      "source": [
        "with additional dense + dropout layers\n",
        "\n",
        "\n",
        "11055/11055 - 2234s - loss: 0.7305 - accuracy: 0.8047 - val_loss: 0.5110 - val_accuracy: 0.7629\n",
        "Epoch 2/6\n",
        "11055/11055 - 2226s - loss: 0.3899 - accuracy: 0.8259 - val_loss: 0.5051 - val_accuracy: 0.7584\n",
        "Epoch 3/6\n",
        "11055/11055 - 2226s - loss: 0.3469 - accuracy: 0.8492 - val_loss: 0.5824 - val_accuracy: 0.7495\n",
        "Epoch 4/6\n",
        "11055/11055 - 2223s - loss: 0.3076 - accuracy: 0.8692 - val_loss: 0.5959 - val_accuracy: 0.7534\n",
        "Epoch 5/6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbEHs1bjZ2Gp"
      },
      "source": [
        "FT_EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbywJ9C-JHZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d635e61-0025-47c4-8360-e3869750222a"
      },
      "source": [
        "# Unfreeze distilBERT layers and make available for training\n",
        "for layer in distilBERT.layers:\n",
        "    layer.trainable = True\n",
        "    \n",
        "# Recompile model after unfreezing\n",
        "model2.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "train_history4 = model2.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = FT_EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 2283s - loss: 0.3801 - accuracy: 0.8299 - val_loss: 0.5387 - val_accuracy: 0.7676\n",
            "Epoch 2/10\n",
            "11055/11055 - 2290s - loss: 0.3486 - accuracy: 0.8474 - val_loss: 0.5328 - val_accuracy: 0.7625\n",
            "Epoch 3/10\n",
            "11055/11055 - 2283s - loss: 0.3183 - accuracy: 0.8637 - val_loss: 0.5681 - val_accuracy: 0.7627\n",
            "Epoch 4/10\n",
            "11055/11055 - 2287s - loss: 0.2878 - accuracy: 0.8786 - val_loss: 0.5962 - val_accuracy: 0.7601\n",
            "Epoch 5/10\n",
            "11055/11055 - 2280s - loss: 0.2621 - accuracy: 0.8912 - val_loss: 0.6502 - val_accuracy: 0.7570\n",
            "Epoch 6/10\n",
            "11055/11055 - 2286s - loss: 0.2371 - accuracy: 0.9028 - val_loss: 0.6628 - val_accuracy: 0.7553\n",
            "Epoch 7/10\n",
            "11055/11055 - 2286s - loss: 0.2152 - accuracy: 0.9131 - val_loss: 0.7298 - val_accuracy: 0.7532\n",
            "Epoch 8/10\n",
            "11055/11055 - 2287s - loss: 0.1963 - accuracy: 0.9218 - val_loss: 0.7284 - val_accuracy: 0.7539\n",
            "Epoch 9/10\n",
            "11055/11055 - 2285s - loss: 0.1788 - accuracy: 0.9290 - val_loss: 0.7987 - val_accuracy: 0.7509\n",
            "Epoch 10/10\n",
            "11055/11055 - 2283s - loss: 0.1641 - accuracy: 0.9355 - val_loss: 0.8397 - val_accuracy: 0.7494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvTTPSr09nmm",
        "outputId": "e5511616-a646-4cdc-c20a-706e815cac6c"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_attention (InputLayer)    [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
            "                                                                 input_attention[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem (Slici (None, 768)          0           tf_distil_bert_model[0][7]       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          393728      tf.__operators__.getitem[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 512)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            513         dropout_19[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 66,757,121\n",
            "Trainable params: 66,757,121\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4guV1gaKUFyd"
      },
      "source": [
        "# save this model\n",
        "# !mkdir -p distilbert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "lP9nMIkAd-E8",
        "outputId": "5dc1dc26-93dd-4ae6-eec2-c42461c915c3"
      },
      "source": [
        "import torch\n",
        "torch.save(model2, 'model2_saved')\n",
        "\n",
        "# saved_model = torch.load('path/to/model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-99b1b503fc3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model2_saved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# saved_model = torch.load('path/to/model')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWAlDVF9YHJF"
      },
      "source": [
        "# hyper parameter values to try\n",
        "lr_vals = [5e-7, 5e-6, 5e-5, 5e-4, 5e-3, 5e-2, 5e-1] \n",
        "EPOCHS = 6\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = len(X_train.index) // BATCH_SIZE\n",
        "\n",
        "\n",
        "LEARNING_RATE_TUNE0 = lr_vals[0]\n",
        "\n",
        "def build_model_tune0(transformer, max_length=MAX_LENGTH, l2reg = L2REG, dropout_rate = LAYER_DROPOUT, lr = LEARNING_RATE_TUNE0):\n",
        "    \"\"\"\n",
        "    Template for building a model off of the BERT or DistilBERT architecture\n",
        "    for a binary classification task.\n",
        "    \n",
        "    Input:\n",
        "      - transformer:  a base Hugging Face transformer model object (BERT or DistilBERT)\n",
        "                      with no added classification head attached.\n",
        "      - max_length:   integer controlling the maximum number of encoded tokens \n",
        "                      in a given sequence.\n",
        "    \n",
        "    Output:\n",
        "      - model:        a compiled tf.keras.Model with added classification layers \n",
        "                      on top of the base pre-trained model architecture.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define weight initializer with a random seed to ensure reproducibility\n",
        "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
        "    \n",
        "    # Define input layers\n",
        "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                            name='input_ids', \n",
        "                                            dtype='int32')\n",
        "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
        "                                                  name='input_attention', \n",
        "                                                  dtype='int32')\n",
        "    \n",
        "    # DistilBERT outputs a tuple where the first element at index 0\n",
        "    # represents the hidden-state at the output of the model's last layer.\n",
        "    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n",
        "    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n",
        "    \n",
        "    # We only care about DistilBERT's output for the [CLS] token, \n",
        "    # which is located at index 0 of every encoded sequence.  \n",
        "    # Splicing out the [CLS] tokens gives us 2D data.\n",
        "    cls_token = last_hidden_state[:, 0, :]\n",
        "    \n",
        "    ##                                                 ##\n",
        "    ## Define additional dropout and dense layers here ##\n",
        "    ##                                                 ##\n",
        "    dense = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer= tf.keras.regularizers.l2(l2reg))(cls_token)\n",
        "    dropout= tf.keras.layers.Dropout(dropout_rate)(dense)\n",
        "    \n",
        "    # Define a single node that makes up the output layer (for binary classification)\n",
        "    output = tf.keras.layers.Dense(1, \n",
        "                                   activation='sigmoid',\n",
        "                                   kernel_initializer=weight_initializer,  \n",
        "                                   kernel_constraint=None,\n",
        "                                   bias_initializer='zeros'\n",
        "                                   )(dropout)\n",
        "    \n",
        "    # Define the model\n",
        "    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=lr), \n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UmhklWkMZc0",
        "outputId": "58c90795-cf31-4e0a-8b31-8206cb75b9da"
      },
      "source": [
        "hyperparam_models = []\n",
        "train_histories = []\n",
        "\n",
        "for lr in lr_vals:\n",
        "  model3 = build_model_tune0(distilBERT, max_length=MAX_LENGTH, l2reg = L2REG, dropout_rate = LAYER_DROPOUT, lr = lr)\n",
        "\n",
        "  train_history3 = model3.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")\n",
        "  \n",
        "  hyperparam_models.append(model3)\n",
        "  train_histories.append(train_history3)\n",
        "\n",
        "  print(\"Learning Rate: \", lr)\n",
        "  print()\n",
        "  print(model3.summary())\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8e50d33ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8e50d33ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f8e6c5e4c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f8e6c5e4c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convertWARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "\n",
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 871s - loss: 6.2524 - accuracy: 0.5203 - val_loss: 5.6675 - val_accuracy: 0.5885\n",
            "Epoch 2/6\n",
            "11055/11055 - 867s - loss: 5.1588 - accuracy: 0.5595 - val_loss: 4.6573 - val_accuracy: 0.6148\n",
            "Epoch 3/6\n",
            "11055/11055 - 873s - loss: 4.2299 - accuracy: 0.5814 - val_loss: 3.8060 - val_accuracy: 0.6243\n",
            "Epoch 4/6\n",
            "11055/11055 - 872s - loss: 3.4535 - accuracy: 0.5938 - val_loss: 3.1005 - val_accuracy: 0.6289\n",
            "Epoch 5/6\n",
            "11055/11055 - 869s - loss: 2.8154 - accuracy: 0.6017 - val_loss: 2.5265 - val_accuracy: 0.6314\n",
            "Epoch 6/6\n",
            "11055/11055 - 869s - loss: 2.3005 - accuracy: 0.6086 - val_loss: 2.0686 - val_accuracy: 0.6345\n",
            "Learning Rate:  5e-07\n",
            "\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_attention (InputLayer)    [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
            "                                                                 input_attention[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem (Slici (None, 768)          0           tf_distil_bert_model[0][7]       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          393728      tf.__operators__.getitem[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 512)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            513         dropout_19[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 66,757,121\n",
            "Trainable params: 394,241\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 874s - loss: 3.1214 - accuracy: 0.5941 - val_loss: 1.1826 - val_accuracy: 0.6368\n",
            "Epoch 2/6\n",
            "11055/11055 - 868s - loss: 0.8834 - accuracy: 0.6261 - val_loss: 0.7391 - val_accuracy: 0.6489\n",
            "Epoch 3/6\n",
            "11055/11055 - 867s - loss: 0.7175 - accuracy: 0.6324 - val_loss: 0.6803 - val_accuracy: 0.6512\n",
            "Epoch 4/6\n",
            "11055/11055 - 868s - loss: 0.6835 - accuracy: 0.6354 - val_loss: 0.6610 - val_accuracy: 0.6536\n",
            "Epoch 5/6\n",
            "11055/11055 - 868s - loss: 0.6703 - accuracy: 0.6374 - val_loss: 0.6521 - val_accuracy: 0.6546\n",
            "Epoch 6/6\n",
            "11055/11055 - 868s - loss: 0.6634 - accuracy: 0.6389 - val_loss: 0.6467 - val_accuracy: 0.6552\n",
            "Learning Rate:  5e-06\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_attention (InputLayer)    [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
            "                                                                 input_attention[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_1 (Sli (None, 768)          0           tf_distil_bert_model[1][7]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512)          393728      tf.__operators__.getitem_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 512)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            513         dropout_20[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 66,757,121\n",
            "Trainable params: 394,241\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 874s - loss: 0.9514 - accuracy: 0.6282 - val_loss: 0.6422 - val_accuracy: 0.6487\n",
            "Epoch 2/6\n",
            "11055/11055 - 869s - loss: 0.6489 - accuracy: 0.6403 - val_loss: 0.6281 - val_accuracy: 0.6607\n",
            "Epoch 3/6\n",
            "11055/11055 - 867s - loss: 0.6411 - accuracy: 0.6459 - val_loss: 0.6259 - val_accuracy: 0.6589\n",
            "Epoch 4/6\n",
            "11055/11055 - 866s - loss: 0.6362 - accuracy: 0.6500 - val_loss: 0.6172 - val_accuracy: 0.6723\n",
            "Epoch 5/6\n",
            "11055/11055 - 868s - loss: 0.6332 - accuracy: 0.6534 - val_loss: 0.6174 - val_accuracy: 0.6681\n",
            "Epoch 6/6\n",
            "11055/11055 - 869s - loss: 0.6306 - accuracy: 0.6560 - val_loss: 0.6131 - val_accuracy: 0.6753\n",
            "Learning Rate:  5e-05\n",
            "\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_attention (InputLayer)    [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
            "                                                                 input_attention[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_2 (Sli (None, 768)          0           tf_distil_bert_model[2][7]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          393728      tf.__operators__.getitem_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 512)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            513         dropout_21[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 66,757,121\n",
            "Trainable params: 394,241\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 879s - loss: 0.6919 - accuracy: 0.6226 - val_loss: 0.6373 - val_accuracy: 0.6535\n",
            "Epoch 2/6\n",
            "11055/11055 - 872s - loss: 0.6508 - accuracy: 0.6359 - val_loss: 0.6312 - val_accuracy: 0.6553\n",
            "Epoch 3/6\n",
            "11055/11055 - 873s - loss: 0.6462 - accuracy: 0.6406 - val_loss: 0.6316 - val_accuracy: 0.6574\n",
            "Epoch 4/6\n",
            "11055/11055 - 874s - loss: 0.6442 - accuracy: 0.6425 - val_loss: 0.6334 - val_accuracy: 0.6528\n",
            "Epoch 5/6\n",
            "11055/11055 - 874s - loss: 0.6442 - accuracy: 0.6418 - val_loss: 0.6188 - val_accuracy: 0.6708\n",
            "Epoch 6/6\n",
            "11055/11055 - 871s - loss: 0.6433 - accuracy: 0.6419 - val_loss: 0.6263 - val_accuracy: 0.6599\n",
            "Learning Rate:  0.0005\n",
            "\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_attention (InputLayer)    [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
            "                                                                 input_attention[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3 (Sli (None, 768)          0           tf_distil_bert_model[3][7]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 512)          393728      tf.__operators__.getitem_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 512)          0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1)            513         dropout_22[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 66,757,121\n",
            "Trainable params: 394,241\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "11055/11055 - 880s - loss: 0.6977 - accuracy: 0.5073 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 2/6\n",
            "11055/11055 - 874s - loss: 0.6932 - accuracy: 0.4994 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 3/6\n",
            "11055/11055 - 873s - loss: 0.6932 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 4/6\n",
            "11055/11055 - 874s - loss: 0.6932 - accuracy: 0.4997 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 5/6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wg5XI1uTFl1"
      },
      "source": [
        "# had to rerun some of them\n",
        "lr_vals = [5e-4, 5e-3, 5e-2, 5e-1]\n",
        "\n",
        "\n",
        "hyperparam_models1 = []\n",
        "train_histories1 = []\n",
        "\n",
        "for lr in lr_vals:\n",
        "  model3 = build_model_tune0(distilBERT, max_length=MAX_LENGTH, l2reg = L2REG, dropout_rate = LAYER_DROPOUT, lr = lr)\n",
        "\n",
        "  train_history3 = model3.fit(\n",
        "    x = [X_train_ids, X_train_attention],\n",
        "    y = y_train.to_numpy(),\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    steps_per_epoch = NUM_STEPS,\n",
        "    validation_data = ([X_val_ids, X_val_attention], y_val.to_numpy()),\n",
        "    verbose=2\n",
        ")\n",
        "  \n",
        "  hyperparam_models1.append(model3)\n",
        "  train_histories1.append(train_history3)\n",
        "\n",
        "  print(\"Learning Rate: \", lr)\n",
        "  print()\n",
        "  print(model3.summary())\n",
        "\n",
        "\n",
        "\n",
        "# which learning rate was the best? \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFT07yn862Xz"
      },
      "source": [
        "# add other metrics - accuracy, precision, recall, f1\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}