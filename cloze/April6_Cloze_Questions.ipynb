{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "April6_Cloze_Questions.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Hbz27OryGcXL",
        "p-tVNugrB5Pv",
        "TCj0cYaRwSt8",
        "cMY31u9MwUnz"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bedccbe7fb6a4b6ebe3f28e31caf4441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_02113d929e094648b334e4b31850928e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5bb2a41199bf476aa3c5a5c1ea7f94aa",
              "IPY_MODEL_40f1aa940b36419e8160c407e2e467fb"
            ]
          }
        },
        "02113d929e094648b334e4b31850928e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5bb2a41199bf476aa3c5a5c1ea7f94aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a0c9c92060d44f19b4de2fa99cab868f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b842e2946fa14e1ead4156598b9abd0b"
          }
        },
        "40f1aa940b36419e8160c407e2e467fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c6e848ad6e6d4c0086c8e8646f74395c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 396kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f736894db484fb7b939d87c0f09a46d"
          }
        },
        "a0c9c92060d44f19b4de2fa99cab868f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b842e2946fa14e1ead4156598b9abd0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6e848ad6e6d4c0086c8e8646f74395c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f736894db484fb7b939d87c0f09a46d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33eead376ee647aea35c8348eaa4b0f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0c2fbd54b6264685a9c30bf0a009b543",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d86364cfc64e4da095e0f70f53a4b5de",
              "IPY_MODEL_0ec46773ff7748eb85b553b517dd1d17"
            ]
          }
        },
        "0c2fbd54b6264685a9c30bf0a009b543": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d86364cfc64e4da095e0f70f53a4b5de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9f417986e8cd4e3499f771b5f680997a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 29,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 29,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9fc0a666f2724087a51c2fb9ecd8921e"
          }
        },
        "0ec46773ff7748eb85b553b517dd1d17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_031d7efd66b148bb84bb749a195c0795",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29.0/29.0 [00:00&lt;00:00, 176B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf293c690c864d5d90b3b70a53da1211"
          }
        },
        "9f417986e8cd4e3499f771b5f680997a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9fc0a666f2724087a51c2fb9ecd8921e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "031d7efd66b148bb84bb749a195c0795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf293c690c864d5d90b3b70a53da1211": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "20341b95e58d4d4e9b20e5749a398edf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_70bea917ff82440e9616051b8d8a87b5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9bb053438ffe4599a3571c352b4cfadd",
              "IPY_MODEL_3c2a6291ea624c08aec94d87fa6a2a18"
            ]
          }
        },
        "70bea917ff82440e9616051b8d8a87b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9bb053438ffe4599a3571c352b4cfadd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_892618d50b784cbc9442b8f1977715f4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435797,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435797,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_71b6a80132ab45e683c0fdbed975f334"
          }
        },
        "3c2a6291ea624c08aec94d87fa6a2a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d236d7782ece4ad3882308d5090e9d74",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436k/436k [00:00&lt;00:00, 3.87MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3482dd9196b74f12b798ceaa8a5e7701"
          }
        },
        "892618d50b784cbc9442b8f1977715f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "71b6a80132ab45e683c0fdbed975f334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d236d7782ece4ad3882308d5090e9d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3482dd9196b74f12b798ceaa8a5e7701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a63c8b08102c476aae6b0756e7085bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7bcf9ae6f48d47aabd78c6ce599e7af5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b79dffe021cf42f0986255959353baa4",
              "IPY_MODEL_624e47ecfbfd4cd0bc69e0b002d8105f"
            ]
          }
        },
        "7bcf9ae6f48d47aabd78c6ce599e7af5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b79dffe021cf42f0986255959353baa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6adc4740a04b45c4b1b1386d4c67d459",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e42dc75ff261427991be557403e6b68d"
          }
        },
        "624e47ecfbfd4cd0bc69e0b002d8105f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_265ec2a6cf144bb598b61786359b8e0d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 745B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4823c6f1d1ef45b39123360f632b1a71"
          }
        },
        "6adc4740a04b45c4b1b1386d4c67d459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e42dc75ff261427991be557403e6b68d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "265ec2a6cf144bb598b61786359b8e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4823c6f1d1ef45b39123360f632b1a71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82b5cfad9bf049cf8a9d884b6177f684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8da9948a68e54b8faf72106a5b10824f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e6157ee27af8469185683aa12115a58f",
              "IPY_MODEL_d66c8b46a8f34f2799f3c8d53b350e98"
            ]
          }
        },
        "8da9948a68e54b8faf72106a5b10824f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6157ee27af8469185683aa12115a58f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_489c64c2828c4b5bb87efcafd88bf28e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 526681800,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 526681800,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a1c03088a0d42fb862533f9696393ee"
          }
        },
        "d66c8b46a8f34f2799f3c8d53b350e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2865ab3cf332496bb9a465e0f4b11d9b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 527M/527M [00:13&lt;00:00, 38.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5658f1279276413dbf414d62d3eedc20"
          }
        },
        "489c64c2828c4b5bb87efcafd88bf28e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a1c03088a0d42fb862533f9696393ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2865ab3cf332496bb9a465e0f4b11d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5658f1279276413dbf414d62d3eedc20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FitAHi88OaD2",
        "outputId": "e7ad93bf-cf86-4563-a1e7-6130d10fe8d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4mno6L-OhQB",
        "outputId": "1d4049e4-5e1f-440e-b099-e147b6dcefb7"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.2MB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 22.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=434400cd2bff5305702240133dafaf3dc93ce07fdf8efa662759b96e649972b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD42TYk5OhSe"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools\n",
        "\n",
        "# from transformers import DistilBertTokenizerFast\n",
        "# from transformers import TFDistilBertModel, DistilBertConfig\n",
        "\n",
        "from transformers import BertTokenizer, TFBertForMaskedLM\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNZ-pkw5OhU8"
      },
      "source": [
        "lav_path = '/content/gdrive/MyDrive/W266Project_Lav_Shalz/train-balanced-sarcasm.csv'\n",
        "shalz_path = '/content/gdrive/MyDrive/Colab Notebooks/train-balanced-sarcasm.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ai_WhjJPAw3"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAgdyhmAOhXS"
      },
      "source": [
        "# df = pd.read_csv(lav_path)\n",
        "df = pd.read_csv(shalz_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7dzoxynOhZp",
        "outputId": "fed3a37f-b4d5-47b7-90be-0f2fc3751655"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1010826, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZUFlIfjOhbl",
        "outputId": "ff1b22e8-db6a-4f1b-f4ab-86fcafb899e9"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label              0\n",
              "comment           53\n",
              "author             0\n",
              "subreddit          0\n",
              "score              0\n",
              "ups                0\n",
              "downs              0\n",
              "date               0\n",
              "created_utc        0\n",
              "parent_comment     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsklwFxROhdp",
        "outputId": "8a1b7c26-d692-4310-956e-1ee0bda5a736"
      },
      "source": [
        "df = df[df['comment'].notna()]\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label             0\n",
              "comment           0\n",
              "author            0\n",
              "subreddit         0\n",
              "score             0\n",
              "ups               0\n",
              "downs             0\n",
              "date              0\n",
              "created_utc       0\n",
              "parent_comment    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c2SovWFhpBrt",
        "outputId": "90423ee7-2423-422f-e7d2-6a0c5a567c5e"
      },
      "source": [
        "df[\"parent_comment\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Yeah, I get that argument. At this point, I'd prefer is she lived in NC as well.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MFmMZF0sWKf",
        "outputId": "83b839b4-7569-44cb-99a6-316bc7312ecf"
      },
      "source": [
        "df['subreddit'].value_counts()\n",
        "\n",
        "# pick top 20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AskReddit               65674\n",
              "politics                39493\n",
              "worldnews               26376\n",
              "leagueoflegends         21034\n",
              "pcmasterrace            18987\n",
              "                        ...  \n",
              "worldofnintendoswap         1\n",
              "raw                         1\n",
              "BrotherhoodOfBolland        1\n",
              "shittyactionfigures         1\n",
              "spotthevegan                1\n",
              "Name: subreddit, Length: 14876, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "T-mMhY80tJRR",
        "outputId": "93c210ea-5c41-4b0f-88e7-f059e1d24815"
      },
      "source": [
        "df[df[\"subreddit\"] == \"AskReddit\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>comment</th>\n",
              "      <th>author</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>score</th>\n",
              "      <th>ups</th>\n",
              "      <th>downs</th>\n",
              "      <th>date</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>parent_comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>I don't pay attention to her, but as long as s...</td>\n",
              "      <td>only7inches</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-02 10:35:08</td>\n",
              "      <td>do you find ariana grande sexy ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>Trick or treating in general is just weird...</td>\n",
              "      <td>only7inches</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-10</td>\n",
              "      <td>2016-10-23 21:43:03</td>\n",
              "      <td>What's your weird or unsettling Trick or Treat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>what the fuck</td>\n",
              "      <td>Pishwi</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>22</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-11</td>\n",
              "      <td>2016-11-04 20:10:33</td>\n",
              "      <td>Star Wars, easy. I'm not that bothered about I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0</td>\n",
              "      <td>This would make me cry.</td>\n",
              "      <td>neutralneutrals</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-11</td>\n",
              "      <td>2016-11-07 12:03:16</td>\n",
              "      <td>\"You are like the end piece of bread in a loaf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0</td>\n",
              "      <td>My stuffed animal I've had since I was born.</td>\n",
              "      <td>kn1820</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-10</td>\n",
              "      <td>2016-10-11 11:17:42</td>\n",
              "      <td>Your house is burning down and you only have t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1010778</th>\n",
              "      <td>1</td>\n",
              "      <td>...I was waiting for the</td>\n",
              "      <td>inanimal</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2009-09</td>\n",
              "      <td>2009-09-24 21:00:04</td>\n",
              "      <td>are you kidding the only reason you don't like...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1010779</th>\n",
              "      <td>0</td>\n",
              "      <td>Hi</td>\n",
              "      <td>mercurysquad</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2009-03</td>\n",
              "      <td>2009-03-14 14:22:17</td>\n",
              "      <td>I don't really need a notifier. The envelope i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1010786</th>\n",
              "      <td>1</td>\n",
              "      <td>He meant Freedom and Liberties for him, not fo...</td>\n",
              "      <td>tbutters</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2009-10</td>\n",
              "      <td>2009-10-18 17:44:42</td>\n",
              "      <td>What about your second point... \"Freedom and l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1010793</th>\n",
              "      <td>1</td>\n",
              "      <td>How admirable.</td>\n",
              "      <td>ryksych</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2009-07</td>\n",
              "      <td>2009-07-07 04:40:23</td>\n",
              "      <td>My Catholic high school didn't let a girl part...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1010804</th>\n",
              "      <td>0</td>\n",
              "      <td>so cool.</td>\n",
              "      <td>ImRtarded</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2009-08</td>\n",
              "      <td>2009-08-19 10:52:40</td>\n",
              "      <td>olds not old's</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65674 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         label  ...                                     parent_comment\n",
              "5            0  ...                   do you find ariana grande sexy ?\n",
              "6            0  ...  What's your weird or unsettling Trick or Treat...\n",
              "15           0  ...  Star Wars, easy. I'm not that bothered about I...\n",
              "27           0  ...  \"You are like the end piece of bread in a loaf...\n",
              "37           0  ...  Your house is burning down and you only have t...\n",
              "...        ...  ...                                                ...\n",
              "1010778      1  ...  are you kidding the only reason you don't like...\n",
              "1010779      0  ...  I don't really need a notifier. The envelope i...\n",
              "1010786      1  ...  What about your second point... \"Freedom and l...\n",
              "1010793      1  ...  My Catholic high school didn't let a girl part...\n",
              "1010804      0  ...                                     olds not old's\n",
              "\n",
              "[65674 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se6gYSKwtYXt",
        "outputId": "61748208-1be4-49d2-df14-c5b96060855b"
      },
      "source": [
        "df.iloc[6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label                                                             0\n",
              "comment               Trick or treating in general is just weird...\n",
              "author                                                  only7inches\n",
              "subreddit                                                 AskReddit\n",
              "score                                                             1\n",
              "ups                                                              -1\n",
              "downs                                                            -1\n",
              "date                                                        2016-10\n",
              "created_utc                                     2016-10-23 21:43:03\n",
              "parent_comment    What's your weird or unsettling Trick or Treat...\n",
              "Name: 6, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXS9tTrDPCZF"
      },
      "source": [
        "Masked Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330,
          "referenced_widgets": [
            "bedccbe7fb6a4b6ebe3f28e31caf4441",
            "02113d929e094648b334e4b31850928e",
            "5bb2a41199bf476aa3c5a5c1ea7f94aa",
            "40f1aa940b36419e8160c407e2e467fb",
            "a0c9c92060d44f19b4de2fa99cab868f",
            "b842e2946fa14e1ead4156598b9abd0b",
            "c6e848ad6e6d4c0086c8e8646f74395c",
            "3f736894db484fb7b939d87c0f09a46d",
            "33eead376ee647aea35c8348eaa4b0f1",
            "0c2fbd54b6264685a9c30bf0a009b543",
            "d86364cfc64e4da095e0f70f53a4b5de",
            "0ec46773ff7748eb85b553b517dd1d17",
            "9f417986e8cd4e3499f771b5f680997a",
            "9fc0a666f2724087a51c2fb9ecd8921e",
            "031d7efd66b148bb84bb749a195c0795",
            "cf293c690c864d5d90b3b70a53da1211",
            "20341b95e58d4d4e9b20e5749a398edf",
            "70bea917ff82440e9616051b8d8a87b5",
            "9bb053438ffe4599a3571c352b4cfadd",
            "3c2a6291ea624c08aec94d87fa6a2a18",
            "892618d50b784cbc9442b8f1977715f4",
            "71b6a80132ab45e683c0fdbed975f334",
            "d236d7782ece4ad3882308d5090e9d74",
            "3482dd9196b74f12b798ceaa8a5e7701",
            "a63c8b08102c476aae6b0756e7085bb8",
            "7bcf9ae6f48d47aabd78c6ce599e7af5",
            "b79dffe021cf42f0986255959353baa4",
            "624e47ecfbfd4cd0bc69e0b002d8105f",
            "6adc4740a04b45c4b1b1386d4c67d459",
            "e42dc75ff261427991be557403e6b68d",
            "265ec2a6cf144bb598b61786359b8e0d",
            "4823c6f1d1ef45b39123360f632b1a71",
            "82b5cfad9bf049cf8a9d884b6177f684",
            "8da9948a68e54b8faf72106a5b10824f",
            "e6157ee27af8469185683aa12115a58f",
            "d66c8b46a8f34f2799f3c8d53b350e98",
            "489c64c2828c4b5bb87efcafd88bf28e",
            "6a1c03088a0d42fb862533f9696393ee",
            "2865ab3cf332496bb9a465e0f4b11d9b",
            "5658f1279276413dbf414d62d3eedc20"
          ]
        },
        "id": "dCEXjceMPEZV",
        "outputId": "582f85fe-d378-43f2-948c-a96b7925185c"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model = TFBertForMaskedLM.from_pretrained('bert-base-cased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bedccbe7fb6a4b6ebe3f28e31caf4441",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33eead376ee647aea35c8348eaa4b0f1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_wâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20341b95e58d4d4e9b20e5749a398edf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a63c8b08102c476aae6b0756e7085bb8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82b5cfad9bf049cf8a9d884b6177f684",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=526681800.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzVzGhuGPFNN"
      },
      "source": [
        "# test it out\n",
        "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"tf\")\n",
        "mask_position = np.where(inputs['input_ids'] == 103)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQhaeBrSLUTq",
        "outputId": "3a3ed759-5763-4f6f-ee97-efbb233dd2b6"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWXYP4hK_ICz"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gofw31jePFPZ",
        "outputId": "00161bc0-a58d-41d5-e055-c20f6362c383"
      },
      "source": [
        "mask_position"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0]), array([6]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyKsOFVxPNev",
        "outputId": "695212b7-7ffb-40d3-ee91-1e1fa2535ab7"
      },
      "source": [
        "np.argmax(model(inputs)[0][0, mask_position[1][0]])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2123"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "W0Yk9wbbPNkO",
        "outputId": "2dca8ee6-da9f-4d66-cd49-90f919875638"
      },
      "source": [
        "tokenizer.convert_ids_to_tokens(2123)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Paris'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSvIvawUuA-9"
      },
      "source": [
        "# # test out with one example\n",
        "\n",
        "# test_comment = df.iloc[6, 1]\n",
        "# test_parent_comment = df.iloc[6, 9]\n",
        "# test_label = df.iloc[6, 0]\n",
        "# test_phrase = \" I am [MASK]!\"\n",
        "\n",
        "# # # can't put three things in because it expects a pair\n",
        "# tokenizer([[test_parent_comment, test_comment, test_phrase]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ-wQRi8PNot",
        "outputId": "32886317-ed6f-4cef-e3c7-c309c2750e6f"
      },
      "source": [
        "#test_phrase = \" I had a [MASK] experience.\"\n",
        "# test_phrase = \" The Reddit comment was [MASK].\"\n",
        "# test_phrase = \" The previous comment was [MASK].\"\n",
        "# test_phrase = \" The previous sentence was [MASK].\"\n",
        "\n",
        "test_phrase1 = \" I am [MASK]!\"\n",
        "test_phrase2 = \" I am being [MASK]!\"\n",
        "test_phrase3 = \" OP is [MASK]!\"\n",
        "test_phrase3 = \" OP is [MASK].\"\n",
        "\n",
        "test_phrases = [test_phrase1, test_phrase2, test_phrase3, test_phrase3]\n",
        "\n",
        "# word1 = ['joking', 'ironic', 'satirical', 'exaggerating', 'sarcastic']\n",
        "# word2 = ['serious', 'honest', 'blunt', 'transparent', 'unsarcastic']\n",
        "\n",
        "word1 = ['joking', 'ironic', 'satirical', 'sarcastic' ,'kidding']\n",
        "word2 = ['serious', 'honest']\n",
        "\n",
        "#pairs =  list(itertools.product(word1, word2))\n",
        "pairs = (('joking', 'serious'), ('ironic', 'serious'), ('satirical', 'honest'), ('kidding', 'serious'))\n",
        "\n",
        "#for phrase in test_phrases:\n",
        "phrase = test_phrase1\n",
        "for pair in pairs:\n",
        "  # phrase_list = [phrase]*20\n",
        "  comments_pet = [comment + phrase for comment in comments]\n",
        "  combined = [list(i) for i in zip(parent_comments, comments_pet)]\n",
        "\n",
        "  # # for a single example\n",
        "  # comments_pet = comments[1] + phrase\n",
        "  # combined = [[parent_comments[1], comments_pet]]\n",
        "  \n",
        "\n",
        "  inputs = tokenizer(combined, padding=True, return_tensors=\"tf\", max_length=512)\n",
        "\n",
        "  mask_positions = np.where(inputs['input_ids'] == 103)\n",
        "\n",
        "  out = model(inputs)\n",
        "\n",
        "  pair_tokens = tokenizer.convert_tokens_to_ids(pair)\n",
        "\n",
        "  for example_nr, (context, example) in enumerate(combined):\n",
        "      print('\"' + context + '\"')\n",
        "      print('\"' + example + \" \")\n",
        "      print(\"Label is: \", labels[example_nr])\n",
        "      print('Logits:')\n",
        "\n",
        "      print('\\t' + pair[0] + ': ', out[0][example_nr, mask_positions[1][example_nr]][pair_tokens[0]].numpy())\n",
        "      print('\\t' + pair[1] + ': ', out[0][example_nr, mask_positions[1][example_nr]][pair_tokens[1]].numpy())\n",
        "      print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"I KNOW THEY DIDNT TAKE OUT THE CHICKEN NOISE WHEN YOU GO TO PRESTIGE BUT THEN BACK OUT\"\n",
            "\"Your title suggests they didn't remove it but your joke about it now being unplayable suggests they did remove it, which is it? I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tjoking:  7.446527\n",
            "\tserious:  8.699382\n",
            "\n",
            "\"What is your go to home remedy for bad sinuses and a sore throat?\"\n",
            "\"Lemon, honey and ginger I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tjoking:  3.8863106\n",
            "\tserious:  3.986889\n",
            "\n",
            "\"tipo cose considerate come i capisaldi del rap italiano, se ce ne sono\"\n",
            "\"Fedez I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tjoking:  2.9425082\n",
            "\tserious:  5.0033016\n",
            "\n",
            "\"No mention of the dark lord nestle.\"\n",
            "\"Oh there not that bad, it's not like they use child slavery to facilitate these mass extinctions. I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tjoking:  5.6756773\n",
            "\tserious:  7.5164146\n",
            "\n",
            "\"Exakt\"\n",
            "\"Tja so viel Pech muss man erstmal haben xD I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tjoking:  4.3441653\n",
            "\tserious:  5.6929\n",
            "\n",
            "\"Tyson Fury on Twitter: \"Don't be fooled guys, just look at these Zionist trying to mess up my career, Wake up, world,\"\"\n",
            "\"I'm sure he has the final solution to this, that he'll elaborate on next time. I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tjoking:  6.161847\n",
            "\tserious:  7.9114313\n",
            "\n",
            "\"Here we go again... Medical issue holding up Bruce trade\"\n",
            "\"Maybe the issue is potentially having 3 corner OFers? I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tjoking:  6.1856127\n",
            "\tserious:  8.112812\n",
            "\n",
            "\"I love this game more than any other Moba before And I really enjoy playing even if I lose. But then comes that moment when I am on reddit again be it in the Diablo or Hearthstone subreddit, that I realize at some point this will just die like all blizzard games that don't gemerate enough money anymore. With WoW being the only exception because it brings in cash all the time. And that makes me sad. And the same will happen with overwatch that's just the way blizz does things\"\n",
            "\"But everyone talks crap about it so it must suck! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tjoking:  6.8574257\n",
            "\tserious:  7.5469556\n",
            "\n",
            "\"No, that's not it.\"\n",
            "\"Of course not, how would I know anything about growing up around young Maori kids. I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tjoking:  5.189449\n",
            "\tserious:  6.845493\n",
            "\n",
            "\"Feel the new player experience!\"\n",
            "\"3x Br100 and still 2.18 KD... I'm fine with him learning on Koltyr. I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tjoking:  4.126628\n",
            "\tserious:  6.2439456\n",
            "\n",
            "\"And I now need to make that my next hunt.\"\n",
            "\"Yeah they both really look fantastic I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tjoking:  4.395604\n",
            "\tserious:  6.507118\n",
            "\n",
            "\"I would seriously love to hear the rationale for Gameday choosing ISU-KU over Duke-Louisville.\"\n",
            "\"Typical B12 / midwest bias on the part of ESPN! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tjoking:  5.4129786\n",
            "\tserious:  6.887866\n",
            "\n",
            "\"\"std::string is the worst string class ever conceived\" why?\"\n",
            "\"It's bulky, it does many unneeded allocs and reallocs, etc. I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tjoking:  5.75651\n",
            "\tserious:  7.0148067\n",
            "\n",
            "\"How old are you if I may ask?\"\n",
            "\"I am 17, basically just turned I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tjoking:  0.36108443\n",
            "\tserious:  2.3610065\n",
            "\n",
            "\"I mean if it goes as far as Rick pulling a gun then I'd like Daryl to restrain him as opposed to Rick getting hit in the head with a rock by Michonne. It's probably the wrong attitude, and the moment will be awesome I've been wrong a number of times with this show.\"\n",
            "\"Put that way, the idea of Michonne suddenly taking a rock to Rick's head seems preposterous: very cartoonish, and unnecessary in a well-supplied town like this, with other resources for restraint and other people to help. I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tjoking:  6.5091896\n",
            "\tserious:  8.2172985\n",
            "\n",
            "\"You can also switch servers\"\n",
            "\"To Toontown Fellowship! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tjoking:  1.6361972\n",
            "\tserious:  3.8989797\n",
            "\n",
            "\"I too measure human worth by the number and complexity of extant orreries they created. Let's make this into a club!\"\n",
            "\"Yeah, and then beat someone with it! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tjoking:  3.8816268\n",
            "\tserious:  6.3349385\n",
            "\n",
            "\"Will only take 5 days\"\n",
            "\"Really, it's that easy? I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tjoking:  6.7032585\n",
            "\tserious:  7.871636\n",
            "\n",
            "\"Wut, are you talking about the Nil greatsword powerstanced L2 move? That shit ain't OP.\"\n",
            "\"O.O DONT YOU KNOW OP = PRETTY TO LOOK AT I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tjoking:  3.3846784\n",
            "\tserious:  4.434394\n",
            "\n",
            "\"Reality Check: 1 on 1 With President Obama, How Does He Justify A Kill List?\"\n",
            "\"He needs to get his own news network! I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tjoking:  5.9501114\n",
            "\tserious:  7.579418\n",
            "\n",
            "\"I KNOW THEY DIDNT TAKE OUT THE CHICKEN NOISE WHEN YOU GO TO PRESTIGE BUT THEN BACK OUT\"\n",
            "\"Your title suggests they didn't remove it but your joke about it now being unplayable suggests they did remove it, which is it? I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tironic:  1.647527\n",
            "\tserious:  8.699382\n",
            "\n",
            "\"What is your go to home remedy for bad sinuses and a sore throat?\"\n",
            "\"Lemon, honey and ginger I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tironic:  -2.3829226\n",
            "\tserious:  3.986889\n",
            "\n",
            "\"tipo cose considerate come i capisaldi del rap italiano, se ce ne sono\"\n",
            "\"Fedez I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tironic:  -0.47031948\n",
            "\tserious:  5.0033016\n",
            "\n",
            "\"No mention of the dark lord nestle.\"\n",
            "\"Oh there not that bad, it's not like they use child slavery to facilitate these mass extinctions. I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tironic:  -0.39335215\n",
            "\tserious:  7.5164146\n",
            "\n",
            "\"Exakt\"\n",
            "\"Tja so viel Pech muss man erstmal haben xD I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tironic:  -0.73611224\n",
            "\tserious:  5.6929\n",
            "\n",
            "\"Tyson Fury on Twitter: \"Don't be fooled guys, just look at these Zionist trying to mess up my career, Wake up, world,\"\"\n",
            "\"I'm sure he has the final solution to this, that he'll elaborate on next time. I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tironic:  0.38116962\n",
            "\tserious:  7.9114313\n",
            "\n",
            "\"Here we go again... Medical issue holding up Bruce trade\"\n",
            "\"Maybe the issue is potentially having 3 corner OFers? I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tironic:  -0.031222627\n",
            "\tserious:  8.112812\n",
            "\n",
            "\"I love this game more than any other Moba before And I really enjoy playing even if I lose. But then comes that moment when I am on reddit again be it in the Diablo or Hearthstone subreddit, that I realize at some point this will just die like all blizzard games that don't gemerate enough money anymore. With WoW being the only exception because it brings in cash all the time. And that makes me sad. And the same will happen with overwatch that's just the way blizz does things\"\n",
            "\"But everyone talks crap about it so it must suck! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tironic:  2.7723424\n",
            "\tserious:  7.5469556\n",
            "\n",
            "\"No, that's not it.\"\n",
            "\"Of course not, how would I know anything about growing up around young Maori kids. I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tironic:  -0.07449259\n",
            "\tserious:  6.845493\n",
            "\n",
            "\"Feel the new player experience!\"\n",
            "\"3x Br100 and still 2.18 KD... I'm fine with him learning on Koltyr. I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tironic:  -1.7360649\n",
            "\tserious:  6.2439456\n",
            "\n",
            "\"And I now need to make that my next hunt.\"\n",
            "\"Yeah they both really look fantastic I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tironic:  -1.2314817\n",
            "\tserious:  6.507118\n",
            "\n",
            "\"I would seriously love to hear the rationale for Gameday choosing ISU-KU over Duke-Louisville.\"\n",
            "\"Typical B12 / midwest bias on the part of ESPN! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tironic:  0.30621743\n",
            "\tserious:  6.887866\n",
            "\n",
            "\"\"std::string is the worst string class ever conceived\" why?\"\n",
            "\"It's bulky, it does many unneeded allocs and reallocs, etc. I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tironic:  0.8360779\n",
            "\tserious:  7.0148067\n",
            "\n",
            "\"How old are you if I may ask?\"\n",
            "\"I am 17, basically just turned I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tironic:  -3.1758778\n",
            "\tserious:  2.3610065\n",
            "\n",
            "\"I mean if it goes as far as Rick pulling a gun then I'd like Daryl to restrain him as opposed to Rick getting hit in the head with a rock by Michonne. It's probably the wrong attitude, and the moment will be awesome I've been wrong a number of times with this show.\"\n",
            "\"Put that way, the idea of Michonne suddenly taking a rock to Rick's head seems preposterous: very cartoonish, and unnecessary in a well-supplied town like this, with other resources for restraint and other people to help. I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tironic:  0.6047085\n",
            "\tserious:  8.2172985\n",
            "\n",
            "\"You can also switch servers\"\n",
            "\"To Toontown Fellowship! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tironic:  -1.995639\n",
            "\tserious:  3.8989797\n",
            "\n",
            "\"I too measure human worth by the number and complexity of extant orreries they created. Let's make this into a club!\"\n",
            "\"Yeah, and then beat someone with it! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tironic:  -0.11870139\n",
            "\tserious:  6.3349385\n",
            "\n",
            "\"Will only take 5 days\"\n",
            "\"Really, it's that easy? I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tironic:  -0.030035503\n",
            "\tserious:  7.871636\n",
            "\n",
            "\"Wut, are you talking about the Nil greatsword powerstanced L2 move? That shit ain't OP.\"\n",
            "\"O.O DONT YOU KNOW OP = PRETTY TO LOOK AT I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tironic:  -1.861545\n",
            "\tserious:  4.434394\n",
            "\n",
            "\"Reality Check: 1 on 1 With President Obama, How Does He Justify A Kill List?\"\n",
            "\"He needs to get his own news network! I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tironic:  -0.14202222\n",
            "\tserious:  7.579418\n",
            "\n",
            "\"I KNOW THEY DIDNT TAKE OUT THE CHICKEN NOISE WHEN YOU GO TO PRESTIGE BUT THEN BACK OUT\"\n",
            "\"Your title suggests they didn't remove it but your joke about it now being unplayable suggests they did remove it, which is it? I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tsatirical:  1.8181875\n",
            "\thonest:  4.52376\n",
            "\n",
            "\"What is your go to home remedy for bad sinuses and a sore throat?\"\n",
            "\"Lemon, honey and ginger I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tsatirical:  -1.8794612\n",
            "\thonest:  4.833343\n",
            "\n",
            "\"tipo cose considerate come i capisaldi del rap italiano, se ce ne sono\"\n",
            "\"Fedez I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tsatirical:  -0.89769906\n",
            "\thonest:  3.641914\n",
            "\n",
            "\"No mention of the dark lord nestle.\"\n",
            "\"Oh there not that bad, it's not like they use child slavery to facilitate these mass extinctions. I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tsatirical:  -1.0234872\n",
            "\thonest:  3.4689608\n",
            "\n",
            "\"Exakt\"\n",
            "\"Tja so viel Pech muss man erstmal haben xD I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tsatirical:  -0.7436362\n",
            "\thonest:  3.563901\n",
            "\n",
            "\"Tyson Fury on Twitter: \"Don't be fooled guys, just look at these Zionist trying to mess up my career, Wake up, world,\"\"\n",
            "\"I'm sure he has the final solution to this, that he'll elaborate on next time. I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tsatirical:  -0.18954647\n",
            "\thonest:  3.7127736\n",
            "\n",
            "\"Here we go again... Medical issue holding up Bruce trade\"\n",
            "\"Maybe the issue is potentially having 3 corner OFers? I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tsatirical:  -0.12856904\n",
            "\thonest:  3.5356054\n",
            "\n",
            "\"I love this game more than any other Moba before And I really enjoy playing even if I lose. But then comes that moment when I am on reddit again be it in the Diablo or Hearthstone subreddit, that I realize at some point this will just die like all blizzard games that don't gemerate enough money anymore. With WoW being the only exception because it brings in cash all the time. And that makes me sad. And the same will happen with overwatch that's just the way blizz does things\"\n",
            "\"But everyone talks crap about it so it must suck! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tsatirical:  0.5626636\n",
            "\thonest:  4.2264895\n",
            "\n",
            "\"No, that's not it.\"\n",
            "\"Of course not, how would I know anything about growing up around young Maori kids. I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tsatirical:  -0.927138\n",
            "\thonest:  2.6150308\n",
            "\n",
            "\"Feel the new player experience!\"\n",
            "\"3x Br100 and still 2.18 KD... I'm fine with him learning on Koltyr. I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tsatirical:  -2.0554845\n",
            "\thonest:  4.6463795\n",
            "\n",
            "\"And I now need to make that my next hunt.\"\n",
            "\"Yeah they both really look fantastic I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tsatirical:  -1.6208004\n",
            "\thonest:  5.8033304\n",
            "\n",
            "\"I would seriously love to hear the rationale for Gameday choosing ISU-KU over Duke-Louisville.\"\n",
            "\"Typical B12 / midwest bias on the part of ESPN! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tsatirical:  0.6450443\n",
            "\thonest:  2.0938592\n",
            "\n",
            "\"\"std::string is the worst string class ever conceived\" why?\"\n",
            "\"It's bulky, it does many unneeded allocs and reallocs, etc. I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tsatirical:  0.8062155\n",
            "\thonest:  3.134034\n",
            "\n",
            "\"How old are you if I may ask?\"\n",
            "\"I am 17, basically just turned I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tsatirical:  -3.0938523\n",
            "\thonest:  0.4922531\n",
            "\n",
            "\"I mean if it goes as far as Rick pulling a gun then I'd like Daryl to restrain him as opposed to Rick getting hit in the head with a rock by Michonne. It's probably the wrong attitude, and the moment will be awesome I've been wrong a number of times with this show.\"\n",
            "\"Put that way, the idea of Michonne suddenly taking a rock to Rick's head seems preposterous: very cartoonish, and unnecessary in a well-supplied town like this, with other resources for restraint and other people to help. I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tsatirical:  0.6653864\n",
            "\thonest:  3.7427576\n",
            "\n",
            "\"You can also switch servers\"\n",
            "\"To Toontown Fellowship! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tsatirical:  -2.0274732\n",
            "\thonest:  2.0078976\n",
            "\n",
            "\"I too measure human worth by the number and complexity of extant orreries they created. Let's make this into a club!\"\n",
            "\"Yeah, and then beat someone with it! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tsatirical:  -0.30983967\n",
            "\thonest:  3.7938328\n",
            "\n",
            "\"Will only take 5 days\"\n",
            "\"Really, it's that easy? I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tsatirical:  -0.7151676\n",
            "\thonest:  4.3949037\n",
            "\n",
            "\"Wut, are you talking about the Nil greatsword powerstanced L2 move? That shit ain't OP.\"\n",
            "\"O.O DONT YOU KNOW OP = PRETTY TO LOOK AT I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tsatirical:  -1.841044\n",
            "\thonest:  2.6173801\n",
            "\n",
            "\"Reality Check: 1 on 1 With President Obama, How Does He Justify A Kill List?\"\n",
            "\"He needs to get his own news network! I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tsatirical:  1.0419217\n",
            "\thonest:  3.3645868\n",
            "\n",
            "\"I KNOW THEY DIDNT TAKE OUT THE CHICKEN NOISE WHEN YOU GO TO PRESTIGE BUT THEN BACK OUT\"\n",
            "\"Your title suggests they didn't remove it but your joke about it now being unplayable suggests they did remove it, which is it? I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tkidding:  7.5293474\n",
            "\tserious:  8.699382\n",
            "\n",
            "\"What is your go to home remedy for bad sinuses and a sore throat?\"\n",
            "\"Lemon, honey and ginger I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tkidding:  4.737432\n",
            "\tserious:  3.986889\n",
            "\n",
            "\"tipo cose considerate come i capisaldi del rap italiano, se ce ne sono\"\n",
            "\"Fedez I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tkidding:  2.7453537\n",
            "\tserious:  5.0033016\n",
            "\n",
            "\"No mention of the dark lord nestle.\"\n",
            "\"Oh there not that bad, it's not like they use child slavery to facilitate these mass extinctions. I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tkidding:  7.010345\n",
            "\tserious:  7.5164146\n",
            "\n",
            "\"Exakt\"\n",
            "\"Tja so viel Pech muss man erstmal haben xD I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tkidding:  3.7289834\n",
            "\tserious:  5.6929\n",
            "\n",
            "\"Tyson Fury on Twitter: \"Don't be fooled guys, just look at these Zionist trying to mess up my career, Wake up, world,\"\"\n",
            "\"I'm sure he has the final solution to this, that he'll elaborate on next time. I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tkidding:  6.3725166\n",
            "\tserious:  7.9114313\n",
            "\n",
            "\"Here we go again... Medical issue holding up Bruce trade\"\n",
            "\"Maybe the issue is potentially having 3 corner OFers? I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tkidding:  7.3335896\n",
            "\tserious:  8.112812\n",
            "\n",
            "\"I love this game more than any other Moba before And I really enjoy playing even if I lose. But then comes that moment when I am on reddit again be it in the Diablo or Hearthstone subreddit, that I realize at some point this will just die like all blizzard games that don't gemerate enough money anymore. With WoW being the only exception because it brings in cash all the time. And that makes me sad. And the same will happen with overwatch that's just the way blizz does things\"\n",
            "\"But everyone talks crap about it so it must suck! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tkidding:  7.481054\n",
            "\tserious:  7.5469556\n",
            "\n",
            "\"No, that's not it.\"\n",
            "\"Of course not, how would I know anything about growing up around young Maori kids. I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tkidding:  6.3719587\n",
            "\tserious:  6.845493\n",
            "\n",
            "\"Feel the new player experience!\"\n",
            "\"3x Br100 and still 2.18 KD... I'm fine with him learning on Koltyr. I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tkidding:  4.6987877\n",
            "\tserious:  6.2439456\n",
            "\n",
            "\"And I now need to make that my next hunt.\"\n",
            "\"Yeah they both really look fantastic I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tkidding:  5.5031624\n",
            "\tserious:  6.507118\n",
            "\n",
            "\"I would seriously love to hear the rationale for Gameday choosing ISU-KU over Duke-Louisville.\"\n",
            "\"Typical B12 / midwest bias on the part of ESPN! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tkidding:  7.179296\n",
            "\tserious:  6.887866\n",
            "\n",
            "\"\"std::string is the worst string class ever conceived\" why?\"\n",
            "\"It's bulky, it does many unneeded allocs and reallocs, etc. I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tkidding:  6.907236\n",
            "\tserious:  7.0148067\n",
            "\n",
            "\"How old are you if I may ask?\"\n",
            "\"I am 17, basically just turned I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tkidding:  1.8757905\n",
            "\tserious:  2.3610065\n",
            "\n",
            "\"I mean if it goes as far as Rick pulling a gun then I'd like Daryl to restrain him as opposed to Rick getting hit in the head with a rock by Michonne. It's probably the wrong attitude, and the moment will be awesome I've been wrong a number of times with this show.\"\n",
            "\"Put that way, the idea of Michonne suddenly taking a rock to Rick's head seems preposterous: very cartoonish, and unnecessary in a well-supplied town like this, with other resources for restraint and other people to help. I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tkidding:  7.7888694\n",
            "\tserious:  8.2172985\n",
            "\n",
            "\"You can also switch servers\"\n",
            "\"To Toontown Fellowship! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tkidding:  2.0585938\n",
            "\tserious:  3.8989797\n",
            "\n",
            "\"I too measure human worth by the number and complexity of extant orreries they created. Let's make this into a club!\"\n",
            "\"Yeah, and then beat someone with it! I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tkidding:  3.72371\n",
            "\tserious:  6.3349385\n",
            "\n",
            "\"Will only take 5 days\"\n",
            "\"Really, it's that easy? I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tkidding:  7.807542\n",
            "\tserious:  7.871636\n",
            "\n",
            "\"Wut, are you talking about the Nil greatsword powerstanced L2 move? That shit ain't OP.\"\n",
            "\"O.O DONT YOU KNOW OP = PRETTY TO LOOK AT I am [MASK]! \n",
            "Label is:  1\n",
            "Logits:\n",
            "\tkidding:  4.4596105\n",
            "\tserious:  4.434394\n",
            "\n",
            "\"Reality Check: 1 on 1 With President Obama, How Does He Justify A Kill List?\"\n",
            "\"He needs to get his own news network! I am [MASK]! \n",
            "Label is:  0\n",
            "Logits:\n",
            "\tkidding:  7.421427\n",
            "\tserious:  7.579418\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqiRZMMZhnYi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptZkpuIzi6th",
        "outputId": "412f97a5-3185-4da7-e607-07fd50b57ad9"
      },
      "source": [
        "print('cols:', len(combined))\n",
        "print('rows:', len(combined[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cols: 20\n",
            "rows: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbz27OryGcXL"
      },
      "source": [
        "## Testing on 20 examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "YhvrR7ZjEgQ8",
        "outputId": "95a11603-60f5-4324-a585-8ffa82b24971"
      },
      "source": [
        "# select a subset of the data\n",
        "s0 = df.label[df.label.eq(0)].sample(10).index\n",
        "s1 = df.label[df.label.eq(1)].sample(10).index \n",
        "df_subset = df.loc[s0.union(s1)].reset_index()\n",
        "df_subset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "      <th>comment</th>\n",
              "      <th>author</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>score</th>\n",
              "      <th>ups</th>\n",
              "      <th>downs</th>\n",
              "      <th>date</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>parent_comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4133</td>\n",
              "      <td>0</td>\n",
              "      <td>M E T A</td>\n",
              "      <td>Meowfia</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-11</td>\n",
              "      <td>2016-11-14 17:50:58</td>\n",
              "      <td>Then you send her the entirety of Moby Dick via text message</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>139031</td>\n",
              "      <td>1</td>\n",
              "      <td>But as long as they didn't succeed in killing anyone nobody would be able to prove they're NOT Skrulls, so I think they'd still be in the clear.</td>\n",
              "      <td>Thebxrabbit</td>\n",
              "      <td>politics</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-10</td>\n",
              "      <td>2016-10-30 02:48:56</td>\n",
              "      <td>Disagree. That's not an excuse. I mean, let's say I tell you that all Cuban people are secretly Skrulls so you drive to Little Havana and attack a bunch of them with a golf club. Obviously I'm not a good person and I should stop spreading misinformation, but that doesn't let you off the hook for your crazed golf-club rampage.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>148571</td>\n",
              "      <td>1</td>\n",
              "      <td>damn right, he was out greatest president!</td>\n",
              "      <td>MrKayMkay</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-15 11:08:32</td>\n",
              "      <td>Australians petition to put Steve Irwin on their money.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>162923</td>\n",
              "      <td>0</td>\n",
              "      <td>but then you would b**e** stuck on ps4 for another year or two, so that cant b**e** smarter.</td>\n",
              "      <td>McAwesome03</td>\n",
              "      <td>pcmasterrace</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-13 04:51:08</td>\n",
              "      <td>Yeah but I don't have enough money to spend $1k on a rig and keep my PS, although I'd love to. I guess I could wait even longer and just keep putting money aside and then buy one in a year or two :p, that might be the smarter option.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>166167</td>\n",
              "      <td>0</td>\n",
              "      <td>Bob Saget for Greg Giraldo</td>\n",
              "      <td>Snoochey</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-10</td>\n",
              "      <td>2016-10-24 09:07:44</td>\n",
              "      <td>If you could swap a living celebrity for a dead one, who would you choose?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index  ...                                                                                                                                                                                                                                                                                                                           parent_comment\n",
              "0  4133    ...  Then you send her the entirety of Moby Dick via text message                                                                                                                                                                                                                                                                           \n",
              "1  139031  ...  Disagree. That's not an excuse. I mean, let's say I tell you that all Cuban people are secretly Skrulls so you drive to Little Havana and attack a bunch of them with a golf club. Obviously I'm not a good person and I should stop spreading misinformation, but that doesn't let you off the hook for your crazed golf-club rampage.\n",
              "2  148571  ...  Australians petition to put Steve Irwin on their money.                                                                                                                                                                                                                                                                                \n",
              "3  162923  ...  Yeah but I don't have enough money to spend $1k on a rig and keep my PS, although I'd love to. I guess I could wait even longer and just keep putting money aside and then buy one in a year or two :p, that might be the smarter option.                                                                                              \n",
              "4  166167  ...  If you could swap a living celebrity for a dead one, who would you choose?                                                                                                                                                                                                                                                             \n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fThEN4QgGhk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f2bb54-528b-42e9-f87b-4fb8d25dd0e6"
      },
      "source": [
        "comments = list(df_subset['comment'].values)\n",
        "parent_comments = list(df_subset['parent_comment'].values)\n",
        "labels = list(df_subset['label'].values)\n",
        "\n",
        "print(comments)\n",
        "print(parent_comments)\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['M E T A', \"But as long as they didn't succeed in killing anyone nobody would be able to prove they're NOT Skrulls, so I think they'd still be in the clear.\", 'damn right, he was out greatest president!', 'but then you would b**e** stuck on ps4 for another year or two, so that cant b**e** smarter.', 'Bob Saget for Greg Giraldo', 'I already stated why.', 'smh but draymond allowed to call the refs pussies', '\"finishing *touch*\" I see what you did there...', 'God forbid we have brown people outnumber us!', 'oh yeah, the sexist tone', 'Snek is doing an anger over his name.', '\"unpredictability\" \"bigger picture\" \"feels like war\" ye, back to 2013, m8.', 'what grade level is this?', \"The Guardian doesn't censor, they shut down people who commit hate crimes.\", \"I hear that song on ESPN every single time it's on.\", \"Are you sure it's not because YOU'RE A SEXIST?\", 'Ah, what a joke!', '*Justice boner intensifies*', 'ITS ALMOST LIKE ITS /R/GAMING IN HERE.', 'We totally believe you.']\n",
            "['Then you send her the entirety of Moby Dick via text message', \"Disagree. That's not an excuse. I mean, let's say I tell you that all Cuban people are secretly Skrulls so you drive to Little Havana and attack a bunch of them with a golf club. Obviously I'm not a good person and I should stop spreading misinformation, but that doesn't let you off the hook for your crazed golf-club rampage.\", 'Australians petition to put Steve Irwin on their money.', \"Yeah but I don't have enough money to spend $1k on a rig and keep my PS, although I'd love to. I guess I could wait even longer and just keep putting money aside and then buy one in a year or two :p, that might be the smarter option.\", 'If you could swap a living celebrity for a dead one, who would you choose?', 'Well thats dumb. Why would it matter', 'Fan Ejected For Calling Out Refs - TSN', 'The tiny hand in the middle of the image is the perfect unintentional finishing touch.', 'When people say that Americans aren\\'t having enough kids, they almost always mean rich, white Americans aren\\'t having enough kids. Everyone else is reproducing too much. We\\'re not in danger of dying out. There are plenty of children being born. They just want the \"right\" people popping \\'em out!', 'Besides, he from the beginning tried to insinuate that he would not run a campaign based on attacking his opponent. Now, all late in the game he wants to come out tough. The way he refers to Hilary as \\'the secretary\\' is just funny, to me. The op statement just sounds like a desperate \"hey guys. Did you forget about me?\"', \"Trump won't let me play Xbox\", 'Fighting Underpopped - The Hard Way', 'I think I hate my English teacher. She always says stuff like \"Girls are smarter than boys,\" and calls on the girls more than us guys. I\\'ve brought it up multiple times, politely asking that she doesn\\'t make that comparison again, and to call on guys a bit more. (Maybe the reason they\\'re \"smarter\" is because you never call on them?) However, she always does it over and over. She says that \"It\\'s true, girls *are* smarter than guys!\" Studies *have* shown that girl\\'s brains adopt faster, but that doesn\\'t mean that they\\'re always smarter. When I ask her for proof, she says \"Well, why don\\'t you look it up on your own time?\" When I do, and bring in info/email her info and articles, she says \"Do you expect me to read all of that?\" Pure hatred for that woman.', \"This is gold. Perhaps, The Guardian should review themselves first before making this statement about censorship. I've got a great suggestion for them, stop using buzzwords such as racist and sexist to cut down a debate, where these words are inappropriately used and simply don't apply to the debate.\", 'silver scrapes is the anthem for league of legends :P', 'I had Alg2 with her last year.', 'Conquerer*', \"All you assholes who think you're better than anyone. So what if a fat person gets fast food, its their life. Guess what they pay taxes too, so the whole drain on the health care line is BS. You judge smokers? Fuck you, like you've never had a bad habit. Alcohol kills FAR more people a year than smoking. So if you're on your high horse, fuck you. I've judged you guilty of hypocrisy and want nothing to do with you. Other than that, cheers have a great fucking day :)\", 'omg! now you are complaining about him complaining about complaining about complaining! this is too much.', 'My french teachers never told me there was a less pain in the ass way. This is the reason I dropped french in high school.']\n",
            "[0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK_x7u7LbThD"
      },
      "source": [
        "# create a dataframe which has the logit and what phrase/word information so it's easier to analyze\n",
        "# and see which phrases and words we should use\n",
        "\n",
        "# columns: parent comment, comment, label, test phrase, word1, word2, word1 logit, word2 logit\n",
        "\n",
        "test_phrase1 = \" I am [MASK]!\"\n",
        "test_phrase2 = \" I am being [MASK]!\"\n",
        "test_phrase3 = \" OP is [MASK]!\"\n",
        "test_phrase4 = \" OP is [MASK].\"\n",
        "\n",
        "test_phrases = [test_phrase1, test_phrase2, test_phrase3, test_phrase4]\n",
        "\n",
        "# test_phrases = [test_phrase1]\n",
        "\n",
        "\n",
        "word1 = ['joking', 'ironic', 'satirical', 'exaggerating', 'sarcastic', 'kidding']\n",
        "word2 = ['serious', 'honest', 'blunt', 'transparent', 'unsarcastic']\n",
        "\n",
        "# word1 = ['joking', 'ironic', 'satirical', 'sarcastic' ,'kidding']\n",
        "# word2 = ['serious', 'honest']\n",
        "\n",
        "# word1 = ['joking', 'ironic']\n",
        "# word2 = ['serious']\n",
        "\n",
        "# pairs = (('joking', 'serious'), ('ironic', 'serious'), ('satirical', 'honest'), ('kidding', 'serious'))\n",
        "\n",
        "\n",
        "# pairs = [list(i) for i in zip(word1, word2)]\n",
        "pairs = list(itertools.product(word1, word2))\n",
        "\n",
        "df_parent_list = []\n",
        "df_comment_list = []\n",
        "df_label_list = []\n",
        "df_testphrase_list = []\n",
        "df_word1_list = []\n",
        "df_word2_list = []\n",
        "df_logit1_list = []\n",
        "df_logit2_list = []\n",
        "\n",
        "# test_comment = df.iloc[6, 1]\n",
        "# test_parent_comment = df.iloc[6, 9]\n",
        "# test_label = df.iloc[6, 0]\n",
        "# test_phrase = \" I am [MASK]!\"\n",
        "\n",
        "# phrase = test_phrase1\n",
        "\n",
        "for phrase in test_phrases:\n",
        "  for pair in pairs:\n",
        "  # for w1 in word1:\n",
        "  #   for w2 in word2:\n",
        "\n",
        "    comments_pet = [comment + phrase for comment in comments]\n",
        "    combined = [list(i) for i in zip(parent_comments, comments_pet)]\n",
        "\n",
        "    inputs = tokenizer(combined, padding=True, return_tensors=\"tf\", max_length=40)\n",
        "\n",
        "    mask_positions = np.where(inputs['input_ids'] == 103)\n",
        "\n",
        "    out = model(inputs)\n",
        "\n",
        "    pair_tokens = tokenizer.convert_tokens_to_ids(pair)\n",
        "\n",
        "\n",
        "    for example_nr, (context, example) in enumerate(combined):\n",
        "        df_parent_list.append(context)\n",
        "        df_comment_list.append(example)\n",
        "        df_label_list.append(labels[example_nr])\n",
        "        df_testphrase_list.append(phrase)\n",
        "        df_word1_list.append(pair[0])\n",
        "        df_word2_list.append(pair[1])\n",
        "        df_logit1_list.append(out[0][example_nr, mask_positions[1][example_nr]][pair_tokens[0]].numpy())\n",
        "        df_logit2_list.append(out[0][example_nr, mask_positions[1][example_nr]][pair_tokens[1]].numpy())\n",
        "\n",
        "        # print('\"' + context + '\"')\n",
        "        # print('\"' + example + \" \")\n",
        "        # print(\"Label is: \", labels[example_nr])\n",
        "        # print(\"Phrase\", phrase)\n",
        "        # print('Logits:')\n",
        "\n",
        "        # print('\\t' + pair[0] + ': ', out[0][example_nr, mask_positions[1][example_nr]][pair_tokens[0]].numpy())\n",
        "        # print('\\t' + pair[1] + ': ', out[0][example_nr, mask_positions[1][example_nr]][pair_tokens[1]].numpy())\n",
        "        # print()\n",
        "\n",
        "\n",
        "df_dict = {\"parent_comment\" : df_parent_list, \n",
        "            \"comment\" : df_comment_list, \n",
        "            \"test_phrase\" : df_testphrase_list, \n",
        "            \"label\" : df_label_list, \n",
        "            \"word1\" : df_word1_list, \n",
        "            \"word2\" : df_word2_list, \n",
        "            \"word1_logit\" : df_logit1_list, \n",
        "            \"word2_logit\" : df_logit2_list} \n",
        "\n",
        "cloze_df = pd.DataFrame(df_dict)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--1V-q02bTc8",
        "outputId": "a3360fa1-1fd7-4d96-a212-5d1f19d0bc42"
      },
      "source": [
        "print(len(comments))\n",
        "print(len(test_phrases))\n",
        "print(len(word1))\n",
        "print(len(word2))\n",
        "print(len(comments) * len(test_phrases) * len(word1) * len(word2))\n",
        "print(cloze_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "4\n",
            "6\n",
            "5\n",
            "2400\n",
            "(2400, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JKG-6erbTac",
        "outputId": "e4781330-d0db-4d16-d919-746d07951967"
      },
      "source": [
        "pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('joking', 'serious'),\n",
              " ('joking', 'honest'),\n",
              " ('joking', 'blunt'),\n",
              " ('joking', 'transparent'),\n",
              " ('joking', 'unsarcastic'),\n",
              " ('ironic', 'serious'),\n",
              " ('ironic', 'honest'),\n",
              " ('ironic', 'blunt'),\n",
              " ('ironic', 'transparent'),\n",
              " ('ironic', 'unsarcastic'),\n",
              " ('satirical', 'serious'),\n",
              " ('satirical', 'honest'),\n",
              " ('satirical', 'blunt'),\n",
              " ('satirical', 'transparent'),\n",
              " ('satirical', 'unsarcastic'),\n",
              " ('exaggerating', 'serious'),\n",
              " ('exaggerating', 'honest'),\n",
              " ('exaggerating', 'blunt'),\n",
              " ('exaggerating', 'transparent'),\n",
              " ('exaggerating', 'unsarcastic'),\n",
              " ('sarcastic', 'serious'),\n",
              " ('sarcastic', 'honest'),\n",
              " ('sarcastic', 'blunt'),\n",
              " ('sarcastic', 'transparent'),\n",
              " ('sarcastic', 'unsarcastic'),\n",
              " ('kidding', 'serious'),\n",
              " ('kidding', 'honest'),\n",
              " ('kidding', 'blunt'),\n",
              " ('kidding', 'transparent'),\n",
              " ('kidding', 'unsarcastic')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdqX9f1a6VRB",
        "outputId": "1ddc0514-89c0-4080-f046-09cbbffadde1"
      },
      "source": [
        "comments "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['M E T A',\n",
              " \"But as long as they didn't succeed in killing anyone nobody would be able to prove they're NOT Skrulls, so I think they'd still be in the clear.\",\n",
              " 'damn right, he was out greatest president!',\n",
              " 'but then you would b**e** stuck on ps4 for another year or two, so that cant b**e** smarter.',\n",
              " 'Bob Saget for Greg Giraldo',\n",
              " 'I already stated why.',\n",
              " 'smh but draymond allowed to call the refs pussies',\n",
              " '\"finishing *touch*\" I see what you did there...',\n",
              " 'God forbid we have brown people outnumber us!',\n",
              " 'oh yeah, the sexist tone',\n",
              " 'Snek is doing an anger over his name.',\n",
              " '\"unpredictability\" \"bigger picture\" \"feels like war\" ye, back to 2013, m8.',\n",
              " 'what grade level is this?',\n",
              " \"The Guardian doesn't censor, they shut down people who commit hate crimes.\",\n",
              " \"I hear that song on ESPN every single time it's on.\",\n",
              " \"Are you sure it's not because YOU'RE A SEXIST?\",\n",
              " 'Ah, what a joke!',\n",
              " '*Justice boner intensifies*',\n",
              " 'ITS ALMOST LIKE ITS /R/GAMING IN HERE.',\n",
              " 'We totally believe you.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "chDbowP86VPS",
        "outputId": "339f9a8c-94be-4a5c-90b3-c388b3416dbc"
      },
      "source": [
        "cloze_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parent_comment</th>\n",
              "      <th>comment</th>\n",
              "      <th>test_phrase</th>\n",
              "      <th>label</th>\n",
              "      <th>word1</th>\n",
              "      <th>word2</th>\n",
              "      <th>word1_logit</th>\n",
              "      <th>word2_logit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Then you send her the entirety of Moby Dick via text message</td>\n",
              "      <td>M E T A I am [MASK]!</td>\n",
              "      <td>I am [MASK]!</td>\n",
              "      <td>0</td>\n",
              "      <td>joking</td>\n",
              "      <td>serious</td>\n",
              "      <td>4.841153</td>\n",
              "      <td>6.183966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Disagree. That's not an excuse. I mean, let's say I tell you that all Cuban people are secretly Skrulls so you drive to Little Havana and attack a bunch of them with a golf club. Obviously I'm not a good person and I should stop spreading misinformation, but that doesn't let you off the hook for your crazed golf-club rampage.</td>\n",
              "      <td>But as long as they didn't succeed in killing anyone nobody would be able to prove they're NOT Skrulls, so I think they'd still be in the clear. I am [MASK]!</td>\n",
              "      <td>I am [MASK]!</td>\n",
              "      <td>1</td>\n",
              "      <td>joking</td>\n",
              "      <td>serious</td>\n",
              "      <td>5.038033</td>\n",
              "      <td>6.425537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Australians petition to put Steve Irwin on their money.</td>\n",
              "      <td>damn right, he was out greatest president! I am [MASK]!</td>\n",
              "      <td>I am [MASK]!</td>\n",
              "      <td>1</td>\n",
              "      <td>joking</td>\n",
              "      <td>serious</td>\n",
              "      <td>5.006810</td>\n",
              "      <td>6.432206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Yeah but I don't have enough money to spend $1k on a rig and keep my PS, although I'd love to. I guess I could wait even longer and just keep putting money aside and then buy one in a year or two :p, that might be the smarter option.</td>\n",
              "      <td>but then you would b**e** stuck on ps4 for another year or two, so that cant b**e** smarter. I am [MASK]!</td>\n",
              "      <td>I am [MASK]!</td>\n",
              "      <td>0</td>\n",
              "      <td>joking</td>\n",
              "      <td>serious</td>\n",
              "      <td>4.316962</td>\n",
              "      <td>5.850067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If you could swap a living celebrity for a dead one, who would you choose?</td>\n",
              "      <td>Bob Saget for Greg Giraldo I am [MASK]!</td>\n",
              "      <td>I am [MASK]!</td>\n",
              "      <td>0</td>\n",
              "      <td>joking</td>\n",
              "      <td>serious</td>\n",
              "      <td>5.143985</td>\n",
              "      <td>6.263018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2395</th>\n",
              "      <td>I had Alg2 with her last year.</td>\n",
              "      <td>Are you sure it's not because YOU'RE A SEXIST? OP is [MASK].</td>\n",
              "      <td>OP is [MASK].</td>\n",
              "      <td>1</td>\n",
              "      <td>kidding</td>\n",
              "      <td>unsarcastic</td>\n",
              "      <td>3.088112</td>\n",
              "      <td>1.234897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2396</th>\n",
              "      <td>Conquerer*</td>\n",
              "      <td>Ah, what a joke! OP is [MASK].</td>\n",
              "      <td>OP is [MASK].</td>\n",
              "      <td>1</td>\n",
              "      <td>kidding</td>\n",
              "      <td>unsarcastic</td>\n",
              "      <td>2.236543</td>\n",
              "      <td>1.045647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2397</th>\n",
              "      <td>All you assholes who think you're better than anyone. So what if a fat person gets fast food, its their life. Guess what they pay taxes too, so the whole drain on the health care line is BS. You judge smokers? Fuck you, like you've never had a bad habit. Alcohol kills FAR more people a year than smoking. So if you're on your high horse, fuck you. I've judged you guilty of hypocrisy and want nothing to do with you. Other than that, cheers have a great fucking day :)</td>\n",
              "      <td>*Justice boner intensifies* OP is [MASK].</td>\n",
              "      <td>OP is [MASK].</td>\n",
              "      <td>0</td>\n",
              "      <td>kidding</td>\n",
              "      <td>unsarcastic</td>\n",
              "      <td>-0.557331</td>\n",
              "      <td>1.914264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2398</th>\n",
              "      <td>omg! now you are complaining about him complaining about complaining about complaining! this is too much.</td>\n",
              "      <td>ITS ALMOST LIKE ITS /R/GAMING IN HERE. OP is [MASK].</td>\n",
              "      <td>OP is [MASK].</td>\n",
              "      <td>1</td>\n",
              "      <td>kidding</td>\n",
              "      <td>unsarcastic</td>\n",
              "      <td>1.185381</td>\n",
              "      <td>2.469634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2399</th>\n",
              "      <td>My french teachers never told me there was a less pain in the ass way. This is the reason I dropped french in high school.</td>\n",
              "      <td>We totally believe you. OP is [MASK].</td>\n",
              "      <td>OP is [MASK].</td>\n",
              "      <td>1</td>\n",
              "      <td>kidding</td>\n",
              "      <td>unsarcastic</td>\n",
              "      <td>1.598332</td>\n",
              "      <td>0.633750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2400 rows Ã— 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                             parent_comment  ... word2_logit\n",
              "0     Then you send her the entirety of Moby Dick via text message                                                                                                                                                                                                                                                                                                                                                                                                                           ...  6.183966  \n",
              "1     Disagree. That's not an excuse. I mean, let's say I tell you that all Cuban people are secretly Skrulls so you drive to Little Havana and attack a bunch of them with a golf club. Obviously I'm not a good person and I should stop spreading misinformation, but that doesn't let you off the hook for your crazed golf-club rampage.                                                                                                                                                ...  6.425537  \n",
              "2     Australians petition to put Steve Irwin on their money.                                                                                                                                                                                                                                                                                                                                                                                                                                ...  6.432206  \n",
              "3     Yeah but I don't have enough money to spend $1k on a rig and keep my PS, although I'd love to. I guess I could wait even longer and just keep putting money aside and then buy one in a year or two :p, that might be the smarter option.                                                                                                                                                                                                                                              ...  5.850067  \n",
              "4     If you could swap a living celebrity for a dead one, who would you choose?                                                                                                                                                                                                                                                                                                                                                                                                             ...  6.263018  \n",
              "...                                                                          ...                                                                                                                                                                                                                                                                                                                                                                                                             ...       ...  \n",
              "2395  I had Alg2 with her last year.                                                                                                                                                                                                                                                                                                                                                                                                                                                         ...  1.234897  \n",
              "2396  Conquerer*                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ...  1.045647  \n",
              "2397  All you assholes who think you're better than anyone. So what if a fat person gets fast food, its their life. Guess what they pay taxes too, so the whole drain on the health care line is BS. You judge smokers? Fuck you, like you've never had a bad habit. Alcohol kills FAR more people a year than smoking. So if you're on your high horse, fuck you. I've judged you guilty of hypocrisy and want nothing to do with you. Other than that, cheers have a great fucking day :)  ...  1.914264  \n",
              "2398  omg! now you are complaining about him complaining about complaining about complaining! this is too much.                                                                                                                                                                                                                                                                                                                                                                              ...  2.469634  \n",
              "2399  My french teachers never told me there was a less pain in the ass way. This is the reason I dropped french in high school.                                                                                                                                                                                                                                                                                                                                                             ...  0.633750  \n",
              "\n",
              "[2400 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn1zxO1zQ35O"
      },
      "source": [
        "# cloze_df[cloze_df[\"comment\"] == \"Foxnews is the most accurate reporting service next to the bible. OP is [MASK].\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTesKgXD6VNW"
      },
      "source": [
        "# see which word combo is predicting correctly\n",
        "# create a new column which is 1 if it's predicted correctly (logit is higher for the right word) and 0 if it's not\n",
        "\n",
        "predicted_labels = []\n",
        "for index, row in cloze_df.iterrows():\n",
        "  if row[\"label\"] == 0: # correct would mean word2 is higher\n",
        "    if row[\"word1_logit\"] <\trow[\"word2_logit\"]: \n",
        "      predicted_labels.append(1)\n",
        "    else: \n",
        "      predicted_labels.append(0)\n",
        "\n",
        "  else: \n",
        "    if row[\"word1_logit\"] >\trow[\"word2_logit\"]: \n",
        "      predicted_labels.append(1)\n",
        "    else: \n",
        "      predicted_labels.append(0)\n",
        "\n",
        "cloze_df[\"is_correct\"] = predicted_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqvJzDV96VK-",
        "outputId": "dc558d4a-303b-4e9b-f2dd-8263dc42d25e"
      },
      "source": [
        "# cloze_df[\"is_correct\"].value_counts()\n",
        "grp = cloze_df.groupby([\"test_phrase\", \"word1\", \"word2\", \"is_correct\"])[\"is_correct\"].count()\n",
        "pd.set_option(\"display.max_rows\", grp.index.shape[0])\n",
        "\n",
        "print(grp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_phrase          word1         word2        is_correct\n",
            " I am [MASK]!        exaggerating  blunt        0             14\n",
            "                                                1             6 \n",
            "                                   honest       0             10\n",
            "                                                1             10\n",
            "                                   serious      0             10\n",
            "                                                1             10\n",
            "                                   transparent  0             14\n",
            "                                                1             6 \n",
            "                                   unsarcastic  0             20\n",
            "                     ironic        blunt        0             10\n",
            "                                                1             10\n",
            "                                   honest       0             10\n",
            "                                                1             10\n",
            "                                   serious      0             10\n",
            "                                                1             10\n",
            "                                   transparent  0             10\n",
            "                                                1             10\n",
            "                                   unsarcastic  0             8 \n",
            "                                                1             12\n",
            "                     joking        blunt        0             10\n",
            "                                                1             10\n",
            "                                   honest       0             11\n",
            "                                                1             9 \n",
            "                                   serious      0             8 \n",
            "                                                1             12\n",
            "                                   transparent  0             10\n",
            "                                                1             10\n",
            "                                   unsarcastic  0             10\n",
            "                                                1             10\n",
            "                     kidding       blunt        0             10\n",
            "                                                1             10\n",
            "                                   honest       0             11\n",
            "                                                1             9 \n",
            "                                   serious      0             11\n",
            "                                                1             9 \n",
            "                                   transparent  0             10\n",
            "                                                1             10\n",
            "                                   unsarcastic  0             10\n",
            "                                                1             10\n",
            "                     sarcastic     blunt        0             10\n",
            "                                                1             10\n",
            "                                   honest       0             9 \n",
            "                                                1             11\n",
            "                                   serious      0             9 \n",
            "                                                1             11\n",
            "                                   transparent  0             15\n",
            "                                                1             5 \n",
            "                                   unsarcastic  0             8 \n",
            "                                                1             12\n",
            "                     satirical     blunt        0             7 \n",
            "                                                1             13\n",
            "                                   honest       0             10\n",
            "                                                1             10\n",
            "                                   serious      0             10\n",
            "                                                1             10\n",
            "                                   transparent  0             9 \n",
            "                                                1             11\n",
            "                                   unsarcastic  0             6 \n",
            "                                                1             14\n",
            " I am being [MASK]!  exaggerating  blunt        0             10\n",
            "                                                1             10\n",
            "                                   honest       0             10\n",
            "                                                1             10\n",
            "                                   serious      0             10\n",
            "                                                1             10\n",
            "                                   transparent  0             10\n",
            "                                                1             10\n",
            "                                   unsarcastic  0             20\n",
            "                     ironic        blunt        0             9 \n",
            "                                                1             11\n",
            "                                   honest       0             10\n",
            "                                                1             10\n",
            "                                   serious      0             10\n",
            "                                                1             10\n",
            "                                   transparent  0             11\n",
            "                                                1             9 \n",
            "                                   unsarcastic  0             10\n",
            "                                                1             10\n",
            "                     joking        blunt        0             7 \n",
            "                                                1             13\n",
            "                                   honest       0             10\n",
            "                                                1             10\n",
            "                                   serious      0             10\n",
            "                                                1             10\n",
            "                                   transparent  0             10\n",
            "                                                1             10\n",
            "                                   unsarcastic  0             10\n",
            "                                                1             10\n",
            "                     kidding       blunt        0             8 \n",
            "                                                1             12\n",
            "                                   honest       0             10\n",
            "                                                1             10\n",
            "                                   serious      0             10\n",
            "                                                1             10\n",
            "                                   transparent  0             12\n",
            "                                                1             8 \n",
            "                                   unsarcastic  0             10\n",
            "                                                1             10\n",
            "                     sarcastic     blunt        0             10\n",
            "                                                1             10\n",
            "                                   honest       0             11\n",
            "                                                1             9 \n",
            "                                   serious      0             10\n",
            "                                                1             10\n",
            "                                   transparent  0             10\n",
            "                                                1             10\n",
            "                                   unsarcastic  0             10\n",
            "                                                1             10\n",
            "                     satirical     blunt        0             8 \n",
            "                                                1             12\n",
            "                                   honest       0             10\n",
            "                                                1             10\n",
            "                                   serious      0             10\n",
            "                                                1             10\n",
            "                                   transparent  0             10\n",
            "                                                1             10\n",
            "                                   unsarcastic  0             10\n",
            "                                                1             10\n",
            " OP is [MASK]!       exaggerating  blunt        0             14\n",
            "                                                1             6 \n",
            "                                   honest       0             15\n",
            "                                                1             5 \n",
            "                                   serious      0             11\n",
            "                                                1             9 \n",
            "                                   transparent  0             15\n",
            "                                                1             5 \n",
            "                                   unsarcastic  0             20\n",
            "                     ironic        blunt        0             8 \n",
            "                                                1             12\n",
            "                                   honest       0             11\n",
            "                                                1             9 \n",
            "                                   serious      0             9 \n",
            "                                                1             11\n",
            "                                   transparent  0             12\n",
            "                                                1             8 \n",
            "                                   unsarcastic  0             5 \n",
            "                                                1             15\n",
            "                     joking        blunt        0             7 \n",
            "                                                1             13\n",
            "                                   honest       0             9 \n",
            "                                                1             11\n",
            "                                   serious      0             10\n",
            "                                                1             10\n",
            "                                   transparent  0             9 \n",
            "                                                1             11\n",
            "                                   unsarcastic  0             4 \n",
            "                                                1             16\n",
            "                     kidding       blunt        0             7 \n",
            "                                                1             13\n",
            "                                   honest       0             12\n",
            "                                                1             8 \n",
            "                                   serious      0             11\n",
            "                                                1             9 \n",
            "                                   transparent  0             11\n",
            "                                                1             9 \n",
            "                                   unsarcastic  0             6 \n",
            "                                                1             14\n",
            "                     sarcastic     blunt        0             10\n",
            "                                                1             10\n",
            "                                   honest       0             10\n",
            "                                                1             10\n",
            "                                   serious      0             9 \n",
            "                                                1             11\n",
            "                                   transparent  0             10\n",
            "                                                1             10\n",
            "                                   unsarcastic  0             7 \n",
            "                                                1             13\n",
            "                     satirical     blunt        0             10\n",
            "                                                1             10\n",
            "                                   honest       0             9 \n",
            "                                                1             11\n",
            "                                   serious      0             10\n",
            "                                                1             10\n",
            "                                   transparent  0             11\n",
            "                                                1             9 \n",
            "                                   unsarcastic  0             8 \n",
            "                                                1             12\n",
            " OP is [MASK].       exaggerating  blunt        0             14\n",
            "                                                1             6 \n",
            "                                   honest       0             14\n",
            "                                                1             6 \n",
            "                                   serious      0             12\n",
            "                                                1             8 \n",
            "                                   transparent  0             14\n",
            "                                                1             6 \n",
            "                                   unsarcastic  0             20\n",
            "                     ironic        blunt        0             8 \n",
            "                                                1             12\n",
            "                                   honest       0             12\n",
            "                                                1             8 \n",
            "                                   serious      0             11\n",
            "                                                1             9 \n",
            "                                   transparent  0             10\n",
            "                                                1             10\n",
            "                                   unsarcastic  0             7 \n",
            "                                                1             13\n",
            "                     joking        blunt        0             9 \n",
            "                                                1             11\n",
            "                                   honest       0             12\n",
            "                                                1             8 \n",
            "                                   serious      0             11\n",
            "                                                1             9 \n",
            "                                   transparent  0             11\n",
            "                                                1             9 \n",
            "                                   unsarcastic  0             5 \n",
            "                                                1             15\n",
            "                     kidding       blunt        0             7 \n",
            "                                                1             13\n",
            "                                   honest       0             9 \n",
            "                                                1             11\n",
            "                                   serious      0             10\n",
            "                                                1             10\n",
            "                                   transparent  0             8 \n",
            "                                                1             12\n",
            "                                   unsarcastic  0             4 \n",
            "                                                1             16\n",
            "                     sarcastic     blunt        0             11\n",
            "                                                1             9 \n",
            "                                   honest       0             14\n",
            "                                                1             6 \n",
            "                                   serious      0             10\n",
            "                                                1             10\n",
            "                                   transparent  0             12\n",
            "                                                1             8 \n",
            "                                   unsarcastic  0             6 \n",
            "                                                1             14\n",
            "                     satirical     blunt        0             10\n",
            "                                                1             10\n",
            "                                   honest       0             11\n",
            "                                                1             9 \n",
            "                                   serious      0             10\n",
            "                                                1             10\n",
            "                                   transparent  0             10\n",
            "                                                1             10\n",
            "                                   unsarcastic  0             8 \n",
            "                                                1             12\n",
            "Name: is_correct, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-tVNugrB5Pv"
      },
      "source": [
        "## Create a function to do everything above automatically\n",
        "gets batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYWJEXokC1i0"
      },
      "source": [
        "import random\n",
        "random.seed(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zG1NqkEGQVz",
        "outputId": "fa6274fe-0c76-4d20-f3a1-5e0ce6d8a8b3"
      },
      "source": [
        "# remove long parent comments and comments from the df so we don't run into memory issues later\n",
        "print(df.shape)\n",
        "\n",
        "df_shortsent = df[df['parent_comment'].str.split().str.len().lt(40)]\n",
        "df_shortsent = df_shortsent[df_shortsent['comment'].str.split().str.len().lt(40)]\n",
        "\n",
        "print(df_shortsent.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1010773, 10)\n",
            "(855340, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBL8RXbfAwx5"
      },
      "source": [
        "def create_cloze(df):\n",
        "                #  , num_examples):\n",
        "\n",
        "  # s0 = df.label[df.label.eq(0)].sample(num_examples).index\n",
        "  # s1 = df.label[df.label.eq(1)].sample(num_examples).index \n",
        "  # df_subset = df.loc[s0.union(s1)].reset_index()\n",
        "\n",
        "  # comments = list(df_subset['comment'].values)\n",
        "  # parent_comments = list(df_subset['parent_comment'].values)\n",
        "  # labels = list(df_subset['label'].values)\n",
        "\n",
        "  comments = list(df['comment'].values)\n",
        "  parent_comments = list(df['parent_comment'].values)\n",
        "  labels = list(df['label'].values)\n",
        "\n",
        "  test_phrase1 = \" I am [MASK]!\"\n",
        "  test_phrase2 = \" I am being [MASK]!\"\n",
        "  test_phrase3 = \" OP is [MASK]!\"\n",
        "  test_phrase4 = \" OP is [MASK].\"\n",
        "\n",
        "  test_phrases = [test_phrase1, test_phrase2, test_phrase3, test_phrase4]\n",
        "\n",
        "  word1 = ['joking', 'ironic', 'satirical', 'exaggerating', 'sarcastic', 'kidding']\n",
        "  word2 = ['serious', 'honest', 'blunt', 'transparent', 'unsarcastic']\n",
        "\n",
        "  pairs = list(itertools.product(word1, word2))\n",
        "\n",
        "  df_parent_list = []\n",
        "  df_comment_list = []\n",
        "  df_label_list = []\n",
        "  df_testphrase_list = []\n",
        "  df_word1_list = []\n",
        "  df_word2_list = []\n",
        "  df_logit1_list = []\n",
        "  df_logit2_list = []\n",
        "\n",
        "  for phrase in test_phrases:\n",
        "    for pair in pairs:\n",
        "      comments_pet = [comment + phrase for comment in comments]\n",
        "      combined = [list(i) for i in zip(parent_comments, comments_pet)]\n",
        "\n",
        "      inputs = tokenizer(combined, padding=True, return_tensors=\"tf\", max_length=40)\n",
        "      mask_positions = np.where(inputs['input_ids'] == 103)\n",
        "      out = model(inputs)\n",
        "      pair_tokens = tokenizer.convert_tokens_to_ids(pair)\n",
        "\n",
        "      for example_nr, (context, example) in enumerate(combined):\n",
        "          df_parent_list.append(context)\n",
        "          df_comment_list.append(example)\n",
        "          df_label_list.append(labels[example_nr])\n",
        "          df_testphrase_list.append(phrase)\n",
        "          df_word1_list.append(pair[0])\n",
        "          df_word2_list.append(pair[1])\n",
        "          df_logit1_list.append(out[0][example_nr, mask_positions[1][example_nr]][pair_tokens[0]].numpy())\n",
        "          df_logit2_list.append(out[0][example_nr, mask_positions[1][example_nr]][pair_tokens[1]].numpy())\n",
        "\n",
        "  df_dict = {\"parent_comment\" : df_parent_list, \n",
        "              \"comment\" : df_comment_list, \n",
        "              \"test_phrase\" : df_testphrase_list, \n",
        "              \"label\" : df_label_list, \n",
        "              \"word1\" : df_word1_list, \n",
        "              \"word2\" : df_word2_list, \n",
        "              \"word1_logit\" : df_logit1_list, \n",
        "              \"word2_logit\" : df_logit2_list} \n",
        "\n",
        "  cloze_df = pd.DataFrame(df_dict)\n",
        "\n",
        "  predicted_labels = []\n",
        "  for index, row in cloze_df.iterrows():\n",
        "    if row[\"label\"] == 0: # correct would mean word2 is higher\n",
        "      if row[\"word1_logit\"] <\trow[\"word2_logit\"]: \n",
        "        predicted_labels.append(1)\n",
        "      else: \n",
        "        predicted_labels.append(0)\n",
        "\n",
        "    else: \n",
        "      if row[\"word1_logit\"] >\trow[\"word2_logit\"]: \n",
        "        predicted_labels.append(1)\n",
        "      else: \n",
        "        predicted_labels.append(0)\n",
        "\n",
        "  cloze_df[\"is_correct\"] = predicted_labels\n",
        "\n",
        "\n",
        "  grp = cloze_df.groupby([\"test_phrase\", \"word1\", \"word2\", \"is_correct\"])[\"is_correct\"].count()\n",
        "  pd.set_option(\"display.max_rows\", grp.index.shape[0])\n",
        "\n",
        "  # print(grp)\n",
        "\n",
        "  # return cloze_df, grp\n",
        "  return (cloze_df, grp)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImhL7OC7B39z"
      },
      "source": [
        "# cloze_df_20, grp_20 = create_cloze(df, 10)\n",
        "# cloze_df_50, grp_50 = create_cloze(df, 25)\n",
        "# cloze_df_100, grp_100 = create_cloze(df, 50)\n",
        "# cloze_df_200, grp_200 = create_cloze(df, 100)\n",
        "# cloze_df_500, grp_500 = create_cloze(df, 250)\n",
        "# cloze_df_1000, grp_1000 = create_cloze(df, 500)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F-odbb6FW7s",
        "outputId": "737e4310-2f8d-4bef-d7cf-22ce23f62720"
      },
      "source": [
        "# most we'll have for a half is 500\n",
        "# since we can't do a lot of examples at a time - let's run batches through the cloze function\n",
        "s0 = df_shortsent.label[df_shortsent.label.eq(0)].sample(500).index\n",
        "s1 = df_shortsent.label[df_shortsent.label.eq(1)].sample(500).index \n",
        "\n",
        "batches = {}\n",
        "ind = 0\n",
        "for x in range(0, 500, 50):\n",
        "  print(ind)\n",
        "  subset0 = s0[x : x+50]\n",
        "  subset1 = s1[x : x+50]\n",
        "  df_subset = df_shortsent.loc[subset0.union(subset1)].reset_index()\n",
        "  (batch_df, batch_grp) = create_cloze(df_subset)\n",
        "  name = \"batch\" + str(ind)\n",
        "  batches[name] = (batch_df, batch_grp)\n",
        "  ind += 1\n",
        "\n",
        "\n",
        "\n",
        "# df_0 = df.loc[s0.union(s1)].reset_index()\n",
        "# df_1 = df.loc[s0.union(s1)].reset_index()\n",
        "# df_subset = df.loc[s0.union(s1)].reset_index()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE_rtMeUM_2B",
        "outputId": "1e0ff6b5-10d8-4306-c155-3587d6b38c97"
      },
      "source": [
        "batches.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['batch0', 'batch1', 'batch2', 'batch3', 'batch4', 'batch5', 'batch6', 'batch7', 'batch8', 'batch9'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE9Z_d7tAplQ"
      },
      "source": [
        "## LM for each set of examples\n",
        "##Manually look at breakdown and decide pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQl7-GkrwOPp"
      },
      "source": [
        "### 50 examples per class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JuNV1wewQMC",
        "outputId": "cf895d3c-6df2-4aa6-ed05-cf9a9d08d8ac"
      },
      "source": [
        "data_50 = batches[\"batch0\"][0]\n",
        "data_50.groupby([\"test_phrase\", \"word1\", \"word2\", \"is_correct\"])[\"is_correct\"].count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "test_phrase          word1         word2        is_correct\n",
              " I am [MASK]!        exaggerating  blunt        0             46 \n",
              "                                                1             54 \n",
              "                                   honest       0             51 \n",
              "                                                1             49 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             46 \n",
              "                                                1             54 \n",
              "                                   unsarcastic  0             100\n",
              "                     ironic        blunt        0             50 \n",
              "                                                1             50 \n",
              "                                   honest       0             50 \n",
              "                                                1             50 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             53 \n",
              "                                                1             47 \n",
              "                                   unsarcastic  0             51 \n",
              "                                                1             49 \n",
              "                     joking        blunt        0             50 \n",
              "                                                1             50 \n",
              "                                   honest       0             46 \n",
              "                                                1             54 \n",
              "                                   serious      0             47 \n",
              "                                                1             53 \n",
              "                                   transparent  0             50 \n",
              "                                                1             50 \n",
              "                                   unsarcastic  0             49 \n",
              "                                                1             51 \n",
              "                     kidding       blunt        0             50 \n",
              "                                                1             50 \n",
              "                                   honest       0             47 \n",
              "                                                1             53 \n",
              "                                   serious      0             42 \n",
              "                                                1             58 \n",
              "                                   transparent  0             50 \n",
              "                                                1             50 \n",
              "                                   unsarcastic  0             50 \n",
              "                                                1             50 \n",
              "                     sarcastic     blunt        0             49 \n",
              "                                                1             51 \n",
              "                                   honest       0             48 \n",
              "                                                1             52 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             47 \n",
              "                                                1             53 \n",
              "                                   unsarcastic  0             55 \n",
              "                                                1             45 \n",
              "                     satirical     blunt        0             48 \n",
              "                                                1             52 \n",
              "                                   honest       0             50 \n",
              "                                                1             50 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             50 \n",
              "                                                1             50 \n",
              "                                   unsarcastic  0             51 \n",
              "                                                1             49 \n",
              " I am being [MASK]!  exaggerating  blunt        0             49 \n",
              "                                                1             51 \n",
              "                                   honest       0             50 \n",
              "                                                1             50 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             50 \n",
              "                                                1             50 \n",
              "                                   unsarcastic  0             100\n",
              "                     ironic        blunt        0             46 \n",
              "                                                1             54 \n",
              "                                   honest       0             50 \n",
              "                                                1             50 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             46 \n",
              "                                                1             54 \n",
              "                                   unsarcastic  0             52 \n",
              "                                                1             48 \n",
              "                     joking        blunt        0             43 \n",
              "                                                1             57 \n",
              "                                   honest       0             50 \n",
              "                                                1             50 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             36 \n",
              "                                                1             64 \n",
              "                                   unsarcastic  0             51 \n",
              "                                                1             49 \n",
              "                     kidding       blunt        0             48 \n",
              "                                                1             52 \n",
              "                                   honest       0             50 \n",
              "                                                1             50 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             40 \n",
              "                                                1             60 \n",
              "                                   unsarcastic  0             51 \n",
              "                                                1             49 \n",
              "                     sarcastic     blunt        0             50 \n",
              "                                                1             50 \n",
              "                                   honest       0             50 \n",
              "                                                1             50 \n",
              "                                   serious      0             47 \n",
              "                                                1             53 \n",
              "                                   transparent  0             50 \n",
              "                                                1             50 \n",
              "                                   unsarcastic  0             50 \n",
              "                                                1             50 \n",
              "                     satirical     blunt        0             47 \n",
              "                                                1             53 \n",
              "                                   honest       0             50 \n",
              "                                                1             50 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             47 \n",
              "                                                1             53 \n",
              "                                   unsarcastic  0             52 \n",
              "                                                1             48 \n",
              " OP is [MASK]!       exaggerating  blunt        0             47 \n",
              "                                                1             53 \n",
              "                                   honest       0             41 \n",
              "                                                1             59 \n",
              "                                   serious      0             47 \n",
              "                                                1             53 \n",
              "                                   transparent  0             45 \n",
              "                                                1             55 \n",
              "                                   unsarcastic  0             100\n",
              "                     ironic        blunt        0             50 \n",
              "                                                1             50 \n",
              "                                   honest       0             49 \n",
              "                                                1             51 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             47 \n",
              "                                                1             53 \n",
              "                                   unsarcastic  0             53 \n",
              "                                                1             47 \n",
              "                     joking        blunt        0             45 \n",
              "                                                1             55 \n",
              "                                   honest       0             49 \n",
              "                                                1             51 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             50 \n",
              "                                                1             50 \n",
              "                                   unsarcastic  0             52 \n",
              "                                                1             48 \n",
              "                     kidding       blunt        0             57 \n",
              "                                                1             43 \n",
              "                                   honest       0             49 \n",
              "                                                1             51 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             43 \n",
              "                                                1             57 \n",
              "                                   unsarcastic  0             53 \n",
              "                                                1             47 \n",
              "                     sarcastic     blunt        0             47 \n",
              "                                                1             53 \n",
              "                                   honest       0             49 \n",
              "                                                1             51 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             51 \n",
              "                                                1             49 \n",
              "                                   unsarcastic  0             48 \n",
              "                                                1             52 \n",
              "                     satirical     blunt        0             48 \n",
              "                                                1             52 \n",
              "                                   honest       0             50 \n",
              "                                                1             50 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             51 \n",
              "                                                1             49 \n",
              "                                   unsarcastic  0             50 \n",
              "                                                1             50 \n",
              " OP is [MASK].       exaggerating  blunt        0             49 \n",
              "                                                1             51 \n",
              "                                   honest       0             46 \n",
              "                                                1             54 \n",
              "                                   serious      0             47 \n",
              "                                                1             53 \n",
              "                                   transparent  0             45 \n",
              "                                                1             55 \n",
              "                                   unsarcastic  0             100\n",
              "                     ironic        blunt        0             47 \n",
              "                                                1             53 \n",
              "                                   honest       0             51 \n",
              "                                                1             49 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             49 \n",
              "                                                1             51 \n",
              "                                   unsarcastic  0             52 \n",
              "                                                1             48 \n",
              "                     joking        blunt        0             49 \n",
              "                                                1             51 \n",
              "                                   honest       0             46 \n",
              "                                                1             54 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             48 \n",
              "                                                1             52 \n",
              "                                   unsarcastic  0             50 \n",
              "                                                1             50 \n",
              "                     kidding       blunt        0             45 \n",
              "                                                1             55 \n",
              "                                   honest       0             41 \n",
              "                                                1             59 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             43 \n",
              "                                                1             57 \n",
              "                                   unsarcastic  0             49 \n",
              "                                                1             51 \n",
              "                     sarcastic     blunt        0             42 \n",
              "                                                1             58 \n",
              "                                   honest       0             48 \n",
              "                                                1             52 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             47 \n",
              "                                                1             53 \n",
              "                                   unsarcastic  0             48 \n",
              "                                                1             52 \n",
              "                     satirical     blunt        0             48 \n",
              "                                                1             52 \n",
              "                                   honest       0             49 \n",
              "                                                1             51 \n",
              "                                   serious      0             50 \n",
              "                                                1             50 \n",
              "                                   transparent  0             51 \n",
              "                                                1             49 \n",
              "                                   unsarcastic  0             52 \n",
              "                                                1             48 \n",
              "Name: is_correct, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "cWaudBBBwjm3",
        "outputId": "355009fd-0b71-43d7-a5f0-c20f7e54ef3c"
      },
      "source": [
        "final_pairs50 = (('joking', 'transparent'), ('kidding', 'transparent'), ('joking', 'blunt'), ('ironic', 'transparent'), ('ironic', 'blunt'))\n",
        "\n",
        "final_phrase50 = \" I am being [MASK]!\"\n",
        "\n",
        "final_df_50 = pd.DataFrame()\n",
        "\n",
        "temp_df = batches[\"batch0\"][0]\n",
        "filtered_batch_df_50 = temp_df[(temp_df['test_phrase'] == final_phrase) & (((temp_df['word1'] == \"joking\") & (temp_df['word2'] == \"transparent\")) | \n",
        "                                                                 ((temp_df['word1'] == \"kidding\") & (temp_df['word2'] == \"transparent\")) | \n",
        "                                                                 ((temp_df['word1'] == \"joking\") & (temp_df['word2'] == \"blunt\")) |\n",
        "                                                                 ((temp_df['word1'] == \"ironic\") & (temp_df['word2'] == \"transparent\")) |\n",
        "                                                                 ((temp_df['word1'] == \"ironic\") & (temp_df['word2'] == \"blunt\"))\n",
        "                                                                 )]\n",
        "  # print(filtered_batch_df.shape)                                                               \n",
        "final_df_50 = final_df_50.append(filtered_batch_df_50)\n",
        "\n",
        "print(cloze_df.shape)\n",
        "print(final_df_50.shape)\n",
        "final_df_50.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2400, 9)\n",
            "(500, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parent_comment</th>\n",
              "      <th>comment</th>\n",
              "      <th>test_phrase</th>\n",
              "      <th>label</th>\n",
              "      <th>word1</th>\n",
              "      <th>word2</th>\n",
              "      <th>word1_logit</th>\n",
              "      <th>word2_logit</th>\n",
              "      <th>is_correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3200</th>\n",
              "      <td>Hell just thinking that if you want to level a character from level 1, thats 97 levels you have to earn before you will even see your artifact.</td>\n",
              "      <td>It took 8 years for my characters to see their Artifact weapons. I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>0</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>5.980262</td>\n",
              "      <td>4.633221</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3201</th>\n",
              "      <td>People are willing to pay $7.99 a month for one program?</td>\n",
              "      <td>Shit dude I pay 2.50 an episode to Google for shows I like I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>0</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>1.961727</td>\n",
              "      <td>2.252184</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3202</th>\n",
              "      <td>member when I didn't give a fuck</td>\n",
              "      <td>Member when i downvoted your ass? I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>1</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>6.115184</td>\n",
              "      <td>6.269764</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3203</th>\n",
              "      <td>I feel like this might be cheating but, firearms instructor?</td>\n",
              "      <td>Yeah I know how you feel... I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>0</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>5.217716</td>\n",
              "      <td>5.733661</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3204</th>\n",
              "      <td>Yahoo Email Surveillance: the Next Front in the Fight Against Mass Surveillance</td>\n",
              "      <td>I'm sure google and bing would never do such a thing at the government's behest either. I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>1</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>4.892037</td>\n",
              "      <td>4.933511</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                       parent_comment  ... is_correct\n",
              "3200  Hell just thinking that if you want to level a character from level 1, thats 97 levels you have to earn before you will even see your artifact.  ...  0        \n",
              "3201  People are willing to pay $7.99 a month for one program?                                                                                         ...  1        \n",
              "3202  member when I didn't give a fuck                                                                                                                 ...  0        \n",
              "3203  I feel like this might be cheating but, firearms instructor?                                                                                     ...  1        \n",
              "3204  Yahoo Email Surveillance: the Next Front in the Fight Against Mass Surveillance                                                                  ...  0        \n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCj0cYaRwSt8"
      },
      "source": [
        "### 100 examples per class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6eNvwcKpgnt",
        "outputId": "14d74b21-d0fc-4313-bac7-9b983db0c269"
      },
      "source": [
        "data_100 = pd.DataFrame()\n",
        "for b in list(batches.keys())[:2]:\n",
        "  data_100 = data_100.append(batches[b][0])\n",
        "\n",
        "data_100.groupby([\"test_phrase\", \"word1\", \"word2\", \"is_correct\"])[\"is_correct\"].count()\n",
        "\n",
        "# look for words/phrases where the is_correct 1 is > 100"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "test_phrase          word1         word2        is_correct\n",
              " I am [MASK]!        exaggerating  blunt        0             98 \n",
              "                                                1             102\n",
              "                                   honest       0             103\n",
              "                                                1             97 \n",
              "                                   serious      0             101\n",
              "                                                1             99 \n",
              "                                   transparent  0             96 \n",
              "                                                1             104\n",
              "                                   unsarcastic  0             200\n",
              "                     ironic        blunt        0             98 \n",
              "                                                1             102\n",
              "                                   honest       0             100\n",
              "                                                1             100\n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             102\n",
              "                                                1             98 \n",
              "                                   unsarcastic  0             96 \n",
              "                                                1             104\n",
              "                     joking        blunt        0             100\n",
              "                                                1             100\n",
              "                                   honest       0             99 \n",
              "                                                1             101\n",
              "                                   serious      0             98 \n",
              "                                                1             102\n",
              "                                   transparent  0             100\n",
              "                                                1             100\n",
              "                                   unsarcastic  0             98 \n",
              "                                                1             102\n",
              "                     kidding       blunt        0             100\n",
              "                                                1             100\n",
              "                                   honest       0             100\n",
              "                                                1             100\n",
              "                                   serious      0             90 \n",
              "                                                1             110\n",
              "                                   transparent  0             100\n",
              "                                                1             100\n",
              "                                   unsarcastic  0             100\n",
              "                                                1             100\n",
              "                     sarcastic     blunt        0             99 \n",
              "                                                1             101\n",
              "                                   honest       0             97 \n",
              "                                                1             103\n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             90 \n",
              "                                                1             110\n",
              "                                   unsarcastic  0             107\n",
              "                                                1             93 \n",
              "                     satirical     blunt        0             100\n",
              "                                                1             100\n",
              "                                   honest       0             100\n",
              "                                                1             100\n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             102\n",
              "                                                1             98 \n",
              "                                   unsarcastic  0             95 \n",
              "                                                1             105\n",
              " I am being [MASK]!  exaggerating  blunt        0             99 \n",
              "                                                1             101\n",
              "                                   honest       0             100\n",
              "                                                1             100\n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             101\n",
              "                                                1             99 \n",
              "                                   unsarcastic  0             200\n",
              "                     ironic        blunt        0             90 \n",
              "                                                1             110\n",
              "                                   honest       0             100\n",
              "                                                1             100\n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             94 \n",
              "                                                1             106\n",
              "                                   unsarcastic  0             101\n",
              "                                                1             99 \n",
              "                     joking        blunt        0             96 \n",
              "                                                1             104\n",
              "                                   honest       0             100\n",
              "                                                1             100\n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             93 \n",
              "                                                1             107\n",
              "                                   unsarcastic  0             101\n",
              "                                                1             99 \n",
              "                     kidding       blunt        0             94 \n",
              "                                                1             106\n",
              "                                   honest       0             100\n",
              "                                                1             100\n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             90 \n",
              "                                                1             110\n",
              "                                   unsarcastic  0             101\n",
              "                                                1             99 \n",
              "                     sarcastic     blunt        0             100\n",
              "                                                1             100\n",
              "                                   honest       0             102\n",
              "                                                1             98 \n",
              "                                   serious      0             96 \n",
              "                                                1             104\n",
              "                                   transparent  0             100\n",
              "                                                1             100\n",
              "                                   unsarcastic  0             100\n",
              "                                                1             100\n",
              "                     satirical     blunt        0             96 \n",
              "                                                1             104\n",
              "                                   honest       0             100\n",
              "                                                1             100\n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             103\n",
              "                                                1             97 \n",
              "                                   unsarcastic  0             102\n",
              "                                                1             98 \n",
              " OP is [MASK]!       exaggerating  blunt        0             100\n",
              "                                                1             100\n",
              "                                   honest       0             100\n",
              "                                                1             100\n",
              "                                   serious      0             99 \n",
              "                                                1             101\n",
              "                                   transparent  0             98 \n",
              "                                                1             102\n",
              "                                   unsarcastic  0             200\n",
              "                     ironic        blunt        0             94 \n",
              "                                                1             106\n",
              "                                   honest       0             102\n",
              "                                                1             98 \n",
              "                                   serious      0             101\n",
              "                                                1             99 \n",
              "                                   transparent  0             90 \n",
              "                                                1             110\n",
              "                                   unsarcastic  0             96 \n",
              "                                                1             104\n",
              "                     joking        blunt        0             105\n",
              "                                                1             95 \n",
              "                                   honest       0             109\n",
              "                                                1             91 \n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             107\n",
              "                                                1             93 \n",
              "                                   unsarcastic  0             103\n",
              "                                                1             97 \n",
              "                     kidding       blunt        0             112\n",
              "                                                1             88 \n",
              "                                   honest       0             101\n",
              "                                                1             99 \n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             90 \n",
              "                                                1             110\n",
              "                                   unsarcastic  0             106\n",
              "                                                1             94 \n",
              "                     sarcastic     blunt        0             98 \n",
              "                                                1             102\n",
              "                                   honest       0             103\n",
              "                                                1             97 \n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             105\n",
              "                                                1             95 \n",
              "                                   unsarcastic  0             99 \n",
              "                                                1             101\n",
              "                     satirical     blunt        0             98 \n",
              "                                                1             102\n",
              "                                   honest       0             100\n",
              "                                                1             100\n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             104\n",
              "                                                1             96 \n",
              "                                   unsarcastic  0             100\n",
              "                                                1             100\n",
              " OP is [MASK].       exaggerating  blunt        0             99 \n",
              "                                                1             101\n",
              "                                   honest       0             99 \n",
              "                                                1             101\n",
              "                                   serious      0             103\n",
              "                                                1             97 \n",
              "                                   transparent  0             96 \n",
              "                                                1             104\n",
              "                                   unsarcastic  0             200\n",
              "                     ironic        blunt        0             94 \n",
              "                                                1             106\n",
              "                                   honest       0             102\n",
              "                                                1             98 \n",
              "                                   serious      0             104\n",
              "                                                1             96 \n",
              "                                   transparent  0             91 \n",
              "                                                1             109\n",
              "                                   unsarcastic  0             97 \n",
              "                                                1             103\n",
              "                     joking        blunt        0             101\n",
              "                                                1             99 \n",
              "                                   honest       0             98 \n",
              "                                                1             102\n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             100\n",
              "                                                1             100\n",
              "                                   unsarcastic  0             99 \n",
              "                                                1             101\n",
              "                     kidding       blunt        0             90 \n",
              "                                                1             110\n",
              "                                   honest       0             91 \n",
              "                                                1             109\n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             96 \n",
              "                                                1             104\n",
              "                                   unsarcastic  0             94 \n",
              "                                                1             106\n",
              "                     sarcastic     blunt        0             94 \n",
              "                                                1             106\n",
              "                                   honest       0             99 \n",
              "                                                1             101\n",
              "                                   serious      0             100\n",
              "                                                1             100\n",
              "                                   transparent  0             103\n",
              "                                                1             97 \n",
              "                                   unsarcastic  0             99 \n",
              "                                                1             101\n",
              "                     satirical     blunt        0             100\n",
              "                                                1             100\n",
              "                                   honest       0             100\n",
              "                                                1             100\n",
              "                                   serious      0             101\n",
              "                                                1             99 \n",
              "                                   transparent  0             101\n",
              "                                                1             99 \n",
              "                                   unsarcastic  0             102\n",
              "                                                1             98 \n",
              "Name: is_correct, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "0EUcg2EswUYL",
        "outputId": "88e25a02-ec28-479d-d018-ab935981e238"
      },
      "source": [
        "final_pairs100 = (('joking', 'transparent'), ('kidding', 'transparent'), ('joking', 'blunt'), ('ironic', 'transparent'), ('ironic', 'blunt'))\n",
        "\n",
        "final_phrase100 = \" I am being [MASK]!\"\n",
        "\n",
        "final_df_100 = pd.DataFrame()\n",
        "\n",
        "for b in list(batches.keys())[:2]:\n",
        "  temp_df = batches[b][0]\n",
        "  filtered_batch_df_100 = temp_df[(temp_df['test_phrase'] == final_phrase) & (((temp_df['word1'] == \"joking\") & (temp_df['word2'] == \"transparent\")) | \n",
        "                                                                 ((temp_df['word1'] == \"kidding\") & (temp_df['word2'] == \"transparent\")) | \n",
        "                                                                 ((temp_df['word1'] == \"joking\") & (temp_df['word2'] == \"blunt\")) |\n",
        "                                                                 ((temp_df['word1'] == \"ironic\") & (temp_df['word2'] == \"transparent\")) |\n",
        "                                                                 ((temp_df['word1'] == \"ironic\") & (temp_df['word2'] == \"blunt\"))\n",
        "                                                                 )]\n",
        "  # print(filtered_batch_df.shape)                                                               \n",
        "  final_df_100 = final_df_100.append(filtered_batch_df_100)\n",
        "\n",
        "print(cloze_df.shape)\n",
        "print(final_df_100.shape)\n",
        "final_df_100.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2400, 9)\n",
            "(1000, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parent_comment</th>\n",
              "      <th>comment</th>\n",
              "      <th>test_phrase</th>\n",
              "      <th>label</th>\n",
              "      <th>word1</th>\n",
              "      <th>word2</th>\n",
              "      <th>word1_logit</th>\n",
              "      <th>word2_logit</th>\n",
              "      <th>is_correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3200</th>\n",
              "      <td>I'm fair disappointed that I couldn't use this meme</td>\n",
              "      <td>Yeah because the Irish Presidency is such a powerful and important position I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>1</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>1.247952</td>\n",
              "      <td>1.797580</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3201</th>\n",
              "      <td>Well, I guess we can't \"brigade\" it and earn ourselves some two day bans. Thanks, Reddit!</td>\n",
              "      <td>Never understood the purpose of banning people from an anonymous site they can register an unverifiable account to within 30 seconds of their ban. I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>0</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>6.342561</td>\n",
              "      <td>6.009890</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3202</th>\n",
              "      <td>And looked how that worked out</td>\n",
              "      <td>Sorry, forgot the I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>1</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>5.774253</td>\n",
              "      <td>6.336384</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3203</th>\n",
              "      <td>Lots of baking with my mum.</td>\n",
              "      <td>What kind of things do you guys bake? I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>0</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>6.223651</td>\n",
              "      <td>6.827527</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3204</th>\n",
              "      <td>She hates DC.</td>\n",
              "      <td>Can't imagine why. I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>1</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>4.137773</td>\n",
              "      <td>5.107919</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                 parent_comment  ... is_correct\n",
              "3200  I'm fair disappointed that I couldn't use this meme                                        ...  0        \n",
              "3201  Well, I guess we can't \"brigade\" it and earn ourselves some two day bans. Thanks, Reddit!  ...  0        \n",
              "3202  And looked how that worked out                                                             ...  0        \n",
              "3203  Lots of baking with my mum.                                                                ...  1        \n",
              "3204  She hates DC.                                                                              ...  0        \n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMY31u9MwUnz"
      },
      "source": [
        "### 250 examples per class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKsyxZjeq0iG",
        "outputId": "e2ece2b0-a475-42fa-b1b3-32dc2d94d987"
      },
      "source": [
        "data_250 = pd.DataFrame()\n",
        "for b in list(batches.keys())[:5]:\n",
        "  data_250 = data_250.append(batches[b][0])\n",
        "\n",
        "data_250.groupby([\"test_phrase\", \"word1\", \"word2\", \"is_correct\"])[\"is_correct\"].count()\n",
        "\n",
        "# look for words/phrases where the is_correct 1 is > 250"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "test_phrase          word1         word2        is_correct\n",
              " I am [MASK]!        exaggerating  blunt        0             265\n",
              "                                                1             235\n",
              "                                   honest       0             252\n",
              "                                                1             248\n",
              "                                   serious      0             251\n",
              "                                                1             249\n",
              "                                   transparent  0             247\n",
              "                                                1             253\n",
              "                                   unsarcastic  0             500\n",
              "                     ironic        blunt        0             250\n",
              "                                                1             250\n",
              "                                   honest       0             251\n",
              "                                                1             249\n",
              "                                   serious      0             250\n",
              "                                                1             250\n",
              "                                   transparent  0             257\n",
              "                                                1             243\n",
              "                                   unsarcastic  0             239\n",
              "                                                1             261\n",
              "                     joking        blunt        0             250\n",
              "                                                1             250\n",
              "                                   honest       0             259\n",
              "                                                1             241\n",
              "                                   serious      0             239\n",
              "                                                1             261\n",
              "                                   transparent  0             249\n",
              "                                                1             251\n",
              "                                   unsarcastic  0             249\n",
              "                                                1             251\n",
              "                     kidding       blunt        0             250\n",
              "                                                1             250\n",
              "                                   honest       0             254\n",
              "                                                1             246\n",
              "                                   serious      0             236\n",
              "                                                1             264\n",
              "                                   transparent  0             251\n",
              "                                                1             249\n",
              "                                   unsarcastic  0             250\n",
              "                                                1             250\n",
              "                     sarcastic     blunt        0             255\n",
              "                                                1             245\n",
              "                                   honest       0             247\n",
              "                                                1             253\n",
              "                                   serious      0             250\n",
              "                                                1             250\n",
              "                                   transparent  0             233\n",
              "                                                1             267\n",
              "                                   unsarcastic  0             249\n",
              "                                                1             251\n",
              "                     satirical     blunt        0             246\n",
              "                                                1             254\n",
              "                                   honest       0             250\n",
              "                                                1             250\n",
              "                                   serious      0             250\n",
              "                                                1             250\n",
              "                                   transparent  0             255\n",
              "                                                1             245\n",
              "                                   unsarcastic  0             242\n",
              "                                                1             258\n",
              " I am being [MASK]!  exaggerating  blunt        0             248\n",
              "                                                1             252\n",
              "                                   honest       0             250\n",
              "                                                1             250\n",
              "                                   serious      0             250\n",
              "                                                1             250\n",
              "                                   transparent  0             251\n",
              "                                                1             249\n",
              "                                   unsarcastic  0             500\n",
              "                     ironic        blunt        0             263\n",
              "                                                1             237\n",
              "                                   honest       0             250\n",
              "                                                1             250\n",
              "                                   serious      0             250\n",
              "                                                1             250\n",
              "                                   transparent  0             247\n",
              "                                                1             253\n",
              "                                   unsarcastic  0             253\n",
              "                                                1             247\n",
              "                     joking        blunt        0             245\n",
              "                                                1             255\n",
              "                                   honest       0             250\n",
              "                                                1             250\n",
              "                                   serious      0             250\n",
              "                                                1             250\n",
              "                                   transparent  0             237\n",
              "                                                1             263\n",
              "                                   unsarcastic  0             252\n",
              "                                                1             248\n",
              "                     kidding       blunt        0             245\n",
              "                                                1             255\n",
              "                                   honest       0             250\n",
              "                                                1             250\n",
              "                                   serious      0             250\n",
              "                                                1             250\n",
              "                                   transparent  0             238\n",
              "                                                1             262\n",
              "                                   unsarcastic  0             252\n",
              "                                                1             248\n",
              "                     sarcastic     blunt        0             250\n",
              "                                                1             250\n",
              "                                   honest       0             262\n",
              "                                                1             238\n",
              "                                   serious      0             238\n",
              "                                                1             262\n",
              "                                   transparent  0             250\n",
              "                                                1             250\n",
              "                                   unsarcastic  0             250\n",
              "                                                1             250\n",
              "                     satirical     blunt        0             257\n",
              "                                                1             243\n",
              "                                   honest       0             250\n",
              "                                                1             250\n",
              "                                   serious      0             250\n",
              "                                                1             250\n",
              "                                   transparent  0             261\n",
              "                                                1             239\n",
              "                                   unsarcastic  0             254\n",
              "                                                1             246\n",
              " OP is [MASK]!       exaggerating  blunt        0             243\n",
              "                                                1             257\n",
              "                                   honest       0             258\n",
              "                                                1             242\n",
              "                                   serious      0             245\n",
              "                                                1             255\n",
              "                                   transparent  0             261\n",
              "                                                1             239\n",
              "                                   unsarcastic  0             500\n",
              "                     ironic        blunt        0             250\n",
              "                                                1             250\n",
              "                                   honest       0             266\n",
              "                                                1             234\n",
              "                                   serious      0             251\n",
              "                                                1             249\n",
              "                                   transparent  0             250\n",
              "                                                1             250\n",
              "                                   unsarcastic  0             235\n",
              "                                                1             265\n",
              "                     joking        blunt        0             255\n",
              "                                                1             245\n",
              "                                   honest       0             263\n",
              "                                                1             237\n",
              "                                   serious      0             250\n",
              "                                                1             250\n",
              "                                   transparent  0             254\n",
              "                                                1             246\n",
              "                                   unsarcastic  0             243\n",
              "                                                1             257\n",
              "                     kidding       blunt        0             278\n",
              "                                                1             222\n",
              "                                   honest       0             270\n",
              "                                                1             230\n",
              "                                   serious      0             250\n",
              "                                                1             250\n",
              "                                   transparent  0             249\n",
              "                                                1             251\n",
              "                                   unsarcastic  0             267\n",
              "                                                1             233\n",
              "                     sarcastic     blunt        0             242\n",
              "                                                1             258\n",
              "                                   honest       0             248\n",
              "                                                1             252\n",
              "                                   serious      0             250\n",
              "                                                1             250\n",
              "                                   transparent  0             261\n",
              "                                                1             239\n",
              "                                   unsarcastic  0             246\n",
              "                                                1             254\n",
              "                     satirical     blunt        0             246\n",
              "                                                1             254\n",
              "                                   honest       0             252\n",
              "                                                1             248\n",
              "                                   serious      0             250\n",
              "                                                1             250\n",
              "                                   transparent  0             256\n",
              "                                                1             244\n",
              "                                   unsarcastic  0             248\n",
              "                                                1             252\n",
              " OP is [MASK].       exaggerating  blunt        0             244\n",
              "                                                1             256\n",
              "                                   honest       0             255\n",
              "                                                1             245\n",
              "                                   serious      0             260\n",
              "                                                1             240\n",
              "                                   transparent  0             242\n",
              "                                                1             258\n",
              "                                   unsarcastic  0             500\n",
              "                     ironic        blunt        0             242\n",
              "                                                1             258\n",
              "                                   honest       0             266\n",
              "                                                1             234\n",
              "                                   serious      0             259\n",
              "                                                1             241\n",
              "                                   transparent  0             252\n",
              "                                                1             248\n",
              "                                   unsarcastic  0             247\n",
              "                                                1             253\n",
              "                     joking        blunt        0             251\n",
              "                                                1             249\n",
              "                                   honest       0             251\n",
              "                                                1             249\n",
              "                                   serious      0             250\n",
              "                                                1             250\n",
              "                                   transparent  0             254\n",
              "                                                1             246\n",
              "                                   unsarcastic  0             242\n",
              "                                                1             258\n",
              "                     kidding       blunt        0             237\n",
              "                                                1             263\n",
              "                                   honest       0             249\n",
              "                                                1             251\n",
              "                                   serious      0             248\n",
              "                                                1             252\n",
              "                                   transparent  0             239\n",
              "                                                1             261\n",
              "                                   unsarcastic  0             239\n",
              "                                                1             261\n",
              "                     sarcastic     blunt        0             241\n",
              "                                                1             259\n",
              "                                   honest       0             263\n",
              "                                                1             237\n",
              "                                   serious      0             251\n",
              "                                                1             249\n",
              "                                   transparent  0             259\n",
              "                                                1             241\n",
              "                                   unsarcastic  0             251\n",
              "                                                1             249\n",
              "                     satirical     blunt        0             251\n",
              "                                                1             249\n",
              "                                   honest       0             259\n",
              "                                                1             241\n",
              "                                   serious      0             250\n",
              "                                                1             250\n",
              "                                   transparent  0             252\n",
              "                                                1             248\n",
              "                                   unsarcastic  0             251\n",
              "                                                1             249\n",
              "Name: is_correct, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "6lLeizIov4rg",
        "outputId": "4137b520-0db9-4574-8b38-dfb15ecca15f"
      },
      "source": [
        "# final_pairs250 = (('joking', 'transparent'), ('kidding', 'transparent'), ('joking', 'blunt'), ('ironic', 'transparent'), ('ironic', 'blunt'))\n",
        "final_pairs250 = (('joking', 'transparent'), ('kidding', 'transparent'), ('sarcastic', 'serious'), ('ironic', 'transparent'), ('joking', 'blunt'))\n",
        "\n",
        "final_phrase250 = \" I am being [MASK]!\"\n",
        "\n",
        "final_df_250 = pd.DataFrame()\n",
        "\n",
        "for b in list(batches.keys())[:5]:\n",
        "  temp_df = batches[b][0]\n",
        "  filtered_batch_df_250 = temp_df[(temp_df['test_phrase'] == final_phrase) & (((temp_df['word1'] == \"joking\") & (temp_df['word2'] == \"transparent\")) | \n",
        "                                                                 ((temp_df['word1'] == \"kidding\") & (temp_df['word2'] == \"transparent\")) | \n",
        "                                                                 ((temp_df['word1'] == \"joking\") & (temp_df['word2'] == \"blunt\")) |\n",
        "                                                                 ((temp_df['word1'] == \"ironic\") & (temp_df['word2'] == \"transparent\")) |\n",
        "                                                                 ((temp_df['word1'] == \"ironic\") & (temp_df['word2'] == \"blunt\"))\n",
        "                                                                 )]\n",
        "  # print(filtered_batch_df.shape)                                                               \n",
        "  final_df_250 = final_df_250.append(filtered_batch_df_250)\n",
        "\n",
        "print(cloze_df.shape)\n",
        "print(final_df_250.shape)\n",
        "final_df_250.head()"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2400, 9)\n",
            "(2500, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parent_comment</th>\n",
              "      <th>comment</th>\n",
              "      <th>test_phrase</th>\n",
              "      <th>label</th>\n",
              "      <th>word1</th>\n",
              "      <th>word2</th>\n",
              "      <th>word1_logit</th>\n",
              "      <th>word2_logit</th>\n",
              "      <th>is_correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3200</th>\n",
              "      <td>I'm fair disappointed that I couldn't use this meme</td>\n",
              "      <td>Yeah because the Irish Presidency is such a powerful and important position I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>1</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>1.247952</td>\n",
              "      <td>1.797580</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3201</th>\n",
              "      <td>Well, I guess we can't \"brigade\" it and earn ourselves some two day bans. Thanks, Reddit!</td>\n",
              "      <td>Never understood the purpose of banning people from an anonymous site they can register an unverifiable account to within 30 seconds of their ban. I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>0</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>6.342561</td>\n",
              "      <td>6.009890</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3202</th>\n",
              "      <td>And looked how that worked out</td>\n",
              "      <td>Sorry, forgot the I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>1</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>5.774253</td>\n",
              "      <td>6.336384</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3203</th>\n",
              "      <td>Lots of baking with my mum.</td>\n",
              "      <td>What kind of things do you guys bake? I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>0</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>6.223651</td>\n",
              "      <td>6.827527</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3204</th>\n",
              "      <td>She hates DC.</td>\n",
              "      <td>Can't imagine why. I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>1</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>4.137773</td>\n",
              "      <td>5.107919</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                 parent_comment  ... is_correct\n",
              "3200  I'm fair disappointed that I couldn't use this meme                                        ...  0        \n",
              "3201  Well, I guess we can't \"brigade\" it and earn ourselves some two day bans. Thanks, Reddit!  ...  0        \n",
              "3202  And looked how that worked out                                                             ...  0        \n",
              "3203  Lots of baking with my mum.                                                                ...  1        \n",
              "3204  She hates DC.                                                                              ...  0        \n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zujNAzeywYtr"
      },
      "source": [
        "### 500 examples per class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP_jWOAbrnbo",
        "outputId": "558d5b3f-354e-4411-c89d-78b7ac6ef751"
      },
      "source": [
        "data_500 = pd.DataFrame()\n",
        "for b in list(batches.keys()):\n",
        "  data_500 = data_500.append(batches[b][0])\n",
        "\n",
        "data_500.groupby([\"test_phrase\", \"word1\", \"word2\", \"is_correct\"])[\"is_correct\"].count()\n",
        "\n",
        "# look for words/phrases where the is_correct 1 is > 500"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "test_phrase          word1         word2        is_correct\n",
              " I am [MASK]!        exaggerating  blunt        0             526 \n",
              "                                                1             474 \n",
              "                                   honest       0             497 \n",
              "                                                1             503 \n",
              "                                   serious      0             501 \n",
              "                                                1             499 \n",
              "                                   transparent  0             508 \n",
              "                                                1             492 \n",
              "                                   unsarcastic  0             1000\n",
              "                     ironic        blunt        0             493 \n",
              "                                                1             507 \n",
              "                                   honest       0             499 \n",
              "                                                1             501 \n",
              "                                   serious      0             500 \n",
              "                                                1             500 \n",
              "                                   transparent  0             510 \n",
              "                                                1             490 \n",
              "                                   unsarcastic  0             490 \n",
              "                                                1             510 \n",
              "                     joking        blunt        0             500 \n",
              "                                                1             500 \n",
              "                                   honest       0             514 \n",
              "                                                1             486 \n",
              "                                   serious      0             483 \n",
              "                                                1             517 \n",
              "                                   transparent  0             499 \n",
              "                                                1             501 \n",
              "                                   unsarcastic  0             499 \n",
              "                                                1             501 \n",
              "                     kidding       blunt        0             500 \n",
              "                                                1             500 \n",
              "                                   honest       0             508 \n",
              "                                                1             492 \n",
              "                                   serious      0             471 \n",
              "                                                1             529 \n",
              "                                   transparent  0             500 \n",
              "                                                1             500 \n",
              "                                   unsarcastic  0             501 \n",
              "                                                1             499 \n",
              "                     sarcastic     blunt        0             518 \n",
              "                                                1             482 \n",
              "                                   honest       0             494 \n",
              "                                                1             506 \n",
              "                                   serious      0             500 \n",
              "                                                1             500 \n",
              "                                   transparent  0             490 \n",
              "                                                1             510 \n",
              "                                   unsarcastic  0             503 \n",
              "                                                1             497 \n",
              "                     satirical     blunt        0             494 \n",
              "                                                1             506 \n",
              "                                   honest       0             499 \n",
              "                                                1             501 \n",
              "                                   serious      0             500 \n",
              "                                                1             500 \n",
              "                                   transparent  0             508 \n",
              "                                                1             492 \n",
              "                                   unsarcastic  0             496 \n",
              "                                                1             504 \n",
              " I am being [MASK]!  exaggerating  blunt        0             497 \n",
              "                                                1             503 \n",
              "                                   honest       0             500 \n",
              "                                                1             500 \n",
              "                                   serious      0             500 \n",
              "                                                1             500 \n",
              "                                   transparent  0             501 \n",
              "                                                1             499 \n",
              "                                   unsarcastic  0             1000\n",
              "                     ironic        blunt        0             507 \n",
              "                                                1             493 \n",
              "                                   honest       0             500 \n",
              "                                                1             500 \n",
              "                                   serious      0             500 \n",
              "                                                1             500 \n",
              "                                   transparent  0             503 \n",
              "                                                1             497 \n",
              "                                   unsarcastic  0             504 \n",
              "                                                1             496 \n",
              "                     joking        blunt        0             494 \n",
              "                                                1             506 \n",
              "                                   honest       0             500 \n",
              "                                                1             500 \n",
              "                                   serious      0             500 \n",
              "                                                1             500 \n",
              "                                   transparent  0             492 \n",
              "                                                1             508 \n",
              "                                   unsarcastic  0             503 \n",
              "                                                1             497 \n",
              "                     kidding       blunt        0             494 \n",
              "                                                1             506 \n",
              "                                   honest       0             500 \n",
              "                                                1             500 \n",
              "                                   serious      0             499 \n",
              "                                                1             501 \n",
              "                                   transparent  0             484 \n",
              "                                                1             516 \n",
              "                                   unsarcastic  0             504 \n",
              "                                                1             496 \n",
              "                     sarcastic     blunt        0             500 \n",
              "                                                1             500 \n",
              "                                   honest       0             523 \n",
              "                                                1             477 \n",
              "                                   serious      0             477 \n",
              "                                                1             523 \n",
              "                                   transparent  0             500 \n",
              "                                                1             500 \n",
              "                                   unsarcastic  0             500 \n",
              "                                                1             500 \n",
              "                     satirical     blunt        0             497 \n",
              "                                                1             503 \n",
              "                                   honest       0             500 \n",
              "                                                1             500 \n",
              "                                   serious      0             500 \n",
              "                                                1             500 \n",
              "                                   transparent  0             500 \n",
              "                                                1             500 \n",
              "                                   unsarcastic  0             503 \n",
              "                                                1             497 \n",
              " OP is [MASK]!       exaggerating  blunt        0             479 \n",
              "                                                1             521 \n",
              "                                   honest       0             505 \n",
              "                                                1             495 \n",
              "                                   serious      0             498 \n",
              "                                                1             502 \n",
              "                                   transparent  0             512 \n",
              "                                                1             488 \n",
              "                                   unsarcastic  0             1000\n",
              "                     ironic        blunt        0             488 \n",
              "                                                1             512 \n",
              "                                   honest       0             512 \n",
              "                                                1             488 \n",
              "                                   serious      0             501 \n",
              "                                                1             499 \n",
              "                                   transparent  0             500 \n",
              "                                                1             500 \n",
              "                                   unsarcastic  0             493 \n",
              "                                                1             507 \n",
              "                     joking        blunt        0             500 \n",
              "                                                1             500 \n",
              "                                   honest       0             524 \n",
              "                                                1             476 \n",
              "                                   serious      0             500 \n",
              "                                                1             500 \n",
              "                                   transparent  0             521 \n",
              "                                                1             479 \n",
              "                                   unsarcastic  0             497 \n",
              "                                                1             503 \n",
              "                     kidding       blunt        0             543 \n",
              "                                                1             457 \n",
              "                                   honest       0             524 \n",
              "                                                1             476 \n",
              "                                   serious      0             500 \n",
              "                                                1             500 \n",
              "                                   transparent  0             521 \n",
              "                                                1             479 \n",
              "                                   unsarcastic  0             511 \n",
              "                                                1             489 \n",
              "                     sarcastic     blunt        0             506 \n",
              "                                                1             494 \n",
              "                                   honest       0             510 \n",
              "                                                1             490 \n",
              "                                   serious      0             500 \n",
              "                                                1             500 \n",
              "                                   transparent  0             547 \n",
              "                                                1             453 \n",
              "                                   unsarcastic  0             512 \n",
              "                                                1             488 \n",
              "                     satirical     blunt        0             493 \n",
              "                                                1             507 \n",
              "                                   honest       0             499 \n",
              "                                                1             501 \n",
              "                                   serious      0             500 \n",
              "                                                1             500 \n",
              "                                   transparent  0             514 \n",
              "                                                1             486 \n",
              "                                   unsarcastic  0             495 \n",
              "                                                1             505 \n",
              " OP is [MASK].       exaggerating  blunt        0             483 \n",
              "                                                1             517 \n",
              "                                   honest       0             506 \n",
              "                                                1             494 \n",
              "                                   serious      0             516 \n",
              "                                                1             484 \n",
              "                                   transparent  0             485 \n",
              "                                                1             515 \n",
              "                                   unsarcastic  0             1000\n",
              "                     ironic        blunt        0             481 \n",
              "                                                1             519 \n",
              "                                   honest       0             517 \n",
              "                                                1             483 \n",
              "                                   serious      0             510 \n",
              "                                                1             490 \n",
              "                                   transparent  0             497 \n",
              "                                                1             503 \n",
              "                                   unsarcastic  0             492 \n",
              "                                                1             508 \n",
              "                     joking        blunt        0             487 \n",
              "                                                1             513 \n",
              "                                   honest       0             495 \n",
              "                                                1             505 \n",
              "                                   serious      0             500 \n",
              "                                                1             500 \n",
              "                                   transparent  0             496 \n",
              "                                                1             504 \n",
              "                                   unsarcastic  0             490 \n",
              "                                                1             510 \n",
              "                     kidding       blunt        0             473 \n",
              "                                                1             527 \n",
              "                                   honest       0             493 \n",
              "                                                1             507 \n",
              "                                   serious      0             495 \n",
              "                                                1             505 \n",
              "                                   transparent  0             475 \n",
              "                                                1             525 \n",
              "                                   unsarcastic  0             489 \n",
              "                                                1             511 \n",
              "                     sarcastic     blunt        0             495 \n",
              "                                                1             505 \n",
              "                                   honest       0             515 \n",
              "                                                1             485 \n",
              "                                   serious      0             499 \n",
              "                                                1             501 \n",
              "                                   transparent  0             521 \n",
              "                                                1             479 \n",
              "                                   unsarcastic  0             508 \n",
              "                                                1             492 \n",
              "                     satirical     blunt        0             496 \n",
              "                                                1             504 \n",
              "                                   honest       0             508 \n",
              "                                                1             492 \n",
              "                                   serious      0             500 \n",
              "                                                1             500 \n",
              "                                   transparent  0             503 \n",
              "                                                1             497 \n",
              "                                   unsarcastic  0             502 \n",
              "                                                1             498 \n",
              "Name: is_correct, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "3axT_yhQ6VIZ",
        "outputId": "87882a23-135b-41aa-f6bd-0ce37ee03f14"
      },
      "source": [
        "# final_pairs500 = (('joking', 'transparent'), ('kidding', 'transparent'), ('joking', 'blunt'), ('ironic', 'transparent'), ('ironic', 'blunt'))\n",
        "\n",
        "final_pairs500 = (('joking', 'transparent'), ('kidding', 'transparent'), ('joking', 'blunt'), ('sarcastic', 'serious'), ('kidding', 'blunt'))\n",
        "\n",
        "final_phrase_500 = \" I am being [MASK]!\"\n",
        "\n",
        "\n",
        "\n",
        "# run and get the logits dataframe with one phrase and the chosen pairs\n",
        "# final_df = cloze_df[(cloze_df['test_phrase'] == final_phrase) & (((cloze_df['word1'] == \"joking\") & (cloze_df['word2'] == \"transparent\")) | \n",
        "#                                                                  ((cloze_df['word1'] == \"kidding\") & (cloze_df['word2'] == \"transparent\")) | \n",
        "#                                                                  ((cloze_df['word1'] == \"joking\") & (cloze_df['word2'] == \"blunt\")) |\n",
        "#                                                                  ((cloze_df['word1'] == \"ironic\") & (cloze_df['word2'] == \"transparent\")) |\n",
        "#                                                                  ((cloze_df['word1'] == \"ironic\") & (cloze_df['word2'] == \"blunt\"))\n",
        "#                                                                  )]\n",
        "final_df_500 = pd.DataFrame()\n",
        "for b in batches.keys():\n",
        "  temp_df = batches[b][0]\n",
        "  filtered_batch_df_500 = temp_df[(temp_df['test_phrase'] == final_phrase) & (((temp_df['word1'] == \"joking\") & (temp_df['word2'] == \"transparent\")) | \n",
        "                                                                 ((temp_df['word1'] == \"kidding\") & (temp_df['word2'] == \"transparent\")) | \n",
        "                                                                 ((temp_df['word1'] == \"joking\") & (temp_df['word2'] == \"blunt\")) |\n",
        "                                                                 ((temp_df['word1'] == \"ironic\") & (temp_df['word2'] == \"transparent\")) |\n",
        "                                                                 ((temp_df['word1'] == \"ironic\") & (temp_df['word2'] == \"blunt\"))\n",
        "                                                                 )]\n",
        "  # print(filtered_batch_df.shape)                                                               \n",
        "  final_df_500 = final_df_500.append(filtered_batch_df_500)\n",
        "\n",
        "print(cloze_df.shape)\n",
        "print(final_df_500.shape)\n",
        "final_df_500.head()\n",
        "\n"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2400, 9)\n",
            "(5000, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parent_comment</th>\n",
              "      <th>comment</th>\n",
              "      <th>test_phrase</th>\n",
              "      <th>label</th>\n",
              "      <th>word1</th>\n",
              "      <th>word2</th>\n",
              "      <th>word1_logit</th>\n",
              "      <th>word2_logit</th>\n",
              "      <th>is_correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3200</th>\n",
              "      <td>I'm fair disappointed that I couldn't use this meme</td>\n",
              "      <td>Yeah because the Irish Presidency is such a powerful and important position I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>1</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>1.247952</td>\n",
              "      <td>1.797580</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3201</th>\n",
              "      <td>Well, I guess we can't \"brigade\" it and earn ourselves some two day bans. Thanks, Reddit!</td>\n",
              "      <td>Never understood the purpose of banning people from an anonymous site they can register an unverifiable account to within 30 seconds of their ban. I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>0</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>6.342561</td>\n",
              "      <td>6.009890</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3202</th>\n",
              "      <td>And looked how that worked out</td>\n",
              "      <td>Sorry, forgot the I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>1</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>5.774253</td>\n",
              "      <td>6.336384</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3203</th>\n",
              "      <td>Lots of baking with my mum.</td>\n",
              "      <td>What kind of things do you guys bake? I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>0</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>6.223651</td>\n",
              "      <td>6.827527</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3204</th>\n",
              "      <td>She hates DC.</td>\n",
              "      <td>Can't imagine why. I am being [MASK]!</td>\n",
              "      <td>I am being [MASK]!</td>\n",
              "      <td>1</td>\n",
              "      <td>joking</td>\n",
              "      <td>blunt</td>\n",
              "      <td>4.137773</td>\n",
              "      <td>5.107919</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                 parent_comment  ... is_correct\n",
              "3200  I'm fair disappointed that I couldn't use this meme                                        ...  0        \n",
              "3201  Well, I guess we can't \"brigade\" it and earn ourselves some two day bans. Thanks, Reddit!  ...  0        \n",
              "3202  And looked how that worked out                                                             ...  0        \n",
              "3203  Lots of baking with my mum.                                                                ...  1        \n",
              "3204  She hates DC.                                                                              ...  0        \n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER9-PPNEyLYo"
      },
      "source": [
        "### Get Averages + Soft Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHWjfXr2GChR",
        "outputId": "49201f8b-cae8-41b3-b97f-b070a96b936d"
      },
      "source": [
        "# for each comment - average the logits that are given to it (will get an average for 0 and average for 1 - whichever is higher is our soft label) - put this in a df\n",
        "\n",
        "avg_log_50 = final_df_50.groupby([\"parent_comment\", \"comment\"])[\"word1_logit\", \"word2_logit\", \"label\"].mean()\n",
        "avg_log_100 = final_df_100.groupby([\"parent_comment\", \"comment\"])[\"word1_logit\", \"word2_logit\", \"label\"].mean()\n",
        "avg_log_250 = final_df_250.groupby([\"parent_comment\", \"comment\"])[\"word1_logit\", \"word2_logit\", \"label\"].mean()\n",
        "avg_log_500 = final_df_500.groupby([\"parent_comment\", \"comment\"])[\"word1_logit\", \"word2_logit\", \"label\"].mean()\n",
        "\n",
        "\n",
        "# avg_log"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXgI896-SmcU"
      },
      "source": [
        "avg_log_50 = avg_log_50.reset_index(inplace=False)\n",
        "avg_log_50 = avg_log_50.rename(columns={'word1_logit': 'word1_logit_avg', 'word2_logit': 'word2_logit_avg'})\n",
        "\n",
        "avg_log_100 = avg_log_100.reset_index(inplace=False)\n",
        "avg_log_100 = avg_log_100.rename(columns={'word1_logit': 'word1_logit_avg', 'word2_logit': 'word2_logit_avg'})\n",
        "\n",
        "avg_log_250 = avg_log_250.reset_index(inplace=False)\n",
        "avg_log_250 = avg_log_250.rename(columns={'word1_logit': 'word1_logit_avg', 'word2_logit': 'word2_logit_avg'})\n",
        "\n",
        "avg_log_500 = avg_log_500.reset_index(inplace=False)\n",
        "avg_log_500 = avg_log_500.rename(columns={'word1_logit': 'word1_logit_avg', 'word2_logit': 'word2_logit_avg'})\n",
        "\n",
        "# avg_log"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXNw1t9NTVa1"
      },
      "source": [
        "# find the soft label\n",
        "\n",
        "soft_labels = []\n",
        "for index, row in avg_log_50.iterrows():\n",
        "  if row[\"word1_logit_avg\"] <\trow[\"word2_logit_avg\"]: \n",
        "    soft_labels.append(0)\n",
        "  else: \n",
        "    soft_labels.append(1)\n",
        "avg_log_50[\"soft_labels\"] = soft_labels\n",
        "\n",
        "soft_labels = []\n",
        "for index, row in avg_log_100.iterrows():\n",
        "  if row[\"word1_logit_avg\"] <\trow[\"word2_logit_avg\"]: \n",
        "    soft_labels.append(0)\n",
        "  else: \n",
        "    soft_labels.append(1)\n",
        "avg_log_100[\"soft_labels\"] = soft_labels\n",
        "\n",
        "soft_labels = []\n",
        "for index, row in avg_log_250.iterrows():\n",
        "  if row[\"word1_logit_avg\"] <\trow[\"word2_logit_avg\"]: \n",
        "    soft_labels.append(0)\n",
        "  else: \n",
        "    soft_labels.append(1)\n",
        "avg_log_250[\"soft_labels\"] = soft_labels\n",
        "\n",
        "soft_labels = []\n",
        "for index, row in avg_log_500.iterrows():\n",
        "  if row[\"word1_logit_avg\"] <\trow[\"word2_logit_avg\"]: \n",
        "    soft_labels.append(0)\n",
        "  else: \n",
        "    soft_labels.append(1)\n",
        "avg_log_500[\"soft_labels\"] = soft_labels\n",
        "# avg_log"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "5XXjYgjn3iNE",
        "outputId": "8ed7e4a3-e9de-4bd8-f7bc-153bb5e7d043"
      },
      "source": [
        "# avg_log_500.head()"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parent_comment</th>\n",
              "      <th>comment</th>\n",
              "      <th>word1_logit_avg</th>\n",
              "      <th>word2_logit_avg</th>\n",
              "      <th>label</th>\n",
              "      <th>soft_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"Are you just a nasty girl with negative behavior who ate up for no good but drama and trouble?\" (Image 3 and 4 should be switched)</td>\n",
              "      <td>what sort of fucked up self destructive sociopath uses linkedin for dating stuff? I am being [MASK]!</td>\n",
              "      <td>5.390528</td>\n",
              "      <td>6.044638</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"Bitcoin has brought a surge of new talent to the financial technology space.\"</td>\n",
              "      <td>Wait a minute... There was financial technology before Bitcoin. I am being [MASK]!</td>\n",
              "      <td>5.443249</td>\n",
              "      <td>6.149345</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Don't you like a man looking at your boobs all day?\"</td>\n",
              "      <td>Mmm yeah I love men looking at my boobs all day, make's me feel like an object! I am being [MASK]!</td>\n",
              "      <td>4.477148</td>\n",
              "      <td>6.029073</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"Hey, do you know Tomb Raider?\" \"No\"</td>\n",
              "      <td>\"Oh great.. You're one of 'those' gamers, aren't you?\" I am being [MASK]!</td>\n",
              "      <td>6.566190</td>\n",
              "      <td>6.058030</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"Lightning follows the path of least resistance, which is why France suffers from the most lightning strikes.\"</td>\n",
              "      <td>Cue the reminder that the French Resistance was one of, if not the largest one in WW2 and made D-Day much less bloody by cutting Nazi supply lines and providing intelligence to the Allies. I am being [MASK]!</td>\n",
              "      <td>5.308903</td>\n",
              "      <td>5.473442</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                        parent_comment  ... soft_labels\n",
              "0  \"Are you just a nasty girl with negative behavior who ate up for no good but drama and trouble?\" (Image 3 and 4 should be switched)  ...  0         \n",
              "1  \"Bitcoin has brought a surge of new talent to the financial technology space.\"                                                       ...  0         \n",
              "2  \"Don't you like a man looking at your boobs all day?\"                                                                                ...  0         \n",
              "3  \"Hey, do you know Tomb Raider?\" \"No\"                                                                                                 ...  1         \n",
              "4  \"Lightning follows the path of least resistance, which is why France suffers from the most lightning strikes.\"                       ...  0         \n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF9wr6DNGCfJ",
        "outputId": "73066fb9-73ff-43b5-888e-6f422f384318"
      },
      "source": [
        "# compare soft label to actual label - error analysis on this\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"50 labels per class\")\n",
        "# accuracy\n",
        "acc = accuracy_score(avg_log_50[\"label\"], avg_log_50[\"soft_labels\"])\n",
        "print('Accuracy: %.3f' % acc)\n",
        "\n",
        "# precision\n",
        "precision = precision_score(avg_log_50[\"label\"], avg_log_50[\"soft_labels\"], average='binary', zero_division = 0)\n",
        "print('Precision: %.3f' % precision)\n",
        "\n",
        "# recall\n",
        "recall = recall_score(avg_log_50[\"label\"], avg_log_50[\"soft_labels\"], average='binary', zero_division = 0)\n",
        "print('Recall: %.3f' % recall)\n",
        "\n",
        "#f1\n",
        "score = f1_score(avg_log_50[\"label\"], avg_log_50[\"soft_labels\"], average='binary')\n",
        "print('F-Measure: %.3f' % score)\n",
        "\n",
        "target_names = ['not sarc', 'sarc']\n",
        "print(classification_report(avg_log_50[\"label\"], avg_log_50[\"soft_labels\"], target_names=target_names))\n",
        "\n",
        "\n",
        "\n",
        "print(\"100 labels per class\")\n",
        "# accuracy\n",
        "acc = accuracy_score(avg_log_100[\"label\"], avg_log_100[\"soft_labels\"])\n",
        "print('Accuracy: %.3f' % acc)\n",
        "\n",
        "# precision\n",
        "precision = precision_score(avg_log_100[\"label\"], avg_log_100[\"soft_labels\"], average='binary', zero_division = 0)\n",
        "print('Precision: %.3f' % precision)\n",
        "\n",
        "# recall\n",
        "recall = recall_score(avg_log_100[\"label\"], avg_log_100[\"soft_labels\"], average='binary', zero_division = 0)\n",
        "print('Recall: %.3f' % recall)\n",
        "\n",
        "#f1\n",
        "score = f1_score(avg_log_100[\"label\"], avg_log_100[\"soft_labels\"], average='binary')\n",
        "print('F-Measure: %.3f' % score)\n",
        "\n",
        "target_names = ['not sarc', 'sarc']\n",
        "print(classification_report(avg_log_100[\"label\"], avg_log_100[\"soft_labels\"], target_names=target_names))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"250 labels per class\")\n",
        "# accuracy\n",
        "acc = accuracy_score(avg_log_250[\"label\"], avg_log_250[\"soft_labels\"])\n",
        "print('Accuracy: %.3f' % acc)\n",
        "\n",
        "# precision\n",
        "precision = precision_score(avg_log_250[\"label\"], avg_log_250[\"soft_labels\"], average='binary', zero_division = 0)\n",
        "print('Precision: %.3f' % precision)\n",
        "\n",
        "# recall\n",
        "recall = recall_score(avg_log_250[\"label\"], avg_log_250[\"soft_labels\"], average='binary', zero_division = 0)\n",
        "print('Recall: %.3f' % recall)\n",
        "\n",
        "#f1\n",
        "score = f1_score(avg_log_250[\"label\"], avg_log_250[\"soft_labels\"], average='binary')\n",
        "print('F-Measure: %.3f' % score)\n",
        "\n",
        "target_names = ['not sarc', 'sarc']\n",
        "print(classification_report(avg_log_250[\"label\"], avg_log_250[\"soft_labels\"], target_names=target_names))\n",
        "\n",
        "\n",
        "\n",
        "print(\"500 labels per class\")\n",
        "# accuracy\n",
        "acc = accuracy_score(avg_log_500[\"label\"], avg_log_500[\"soft_labels\"])\n",
        "print('Accuracy: %.3f' % acc)\n",
        "\n",
        "# precision\n",
        "precision = precision_score(avg_log_500[\"label\"], avg_log_500[\"soft_labels\"], average='binary', zero_division = 0)\n",
        "print('Precision: %.3f' % precision)\n",
        "\n",
        "# recall\n",
        "recall = recall_score(avg_log_500[\"label\"], avg_log_500[\"soft_labels\"], average='binary', zero_division = 0)\n",
        "print('Recall: %.3f' % recall)\n",
        "\n",
        "#f1\n",
        "score = f1_score(avg_log_500[\"label\"], avg_log_500[\"soft_labels\"], average='binary')\n",
        "print('F-Measure: %.3f' % score)\n",
        "\n",
        "target_names = ['not sarc', 'sarc']\n",
        "print(classification_report(avg_log_500[\"label\"], avg_log_500[\"soft_labels\"], target_names=target_names))\n",
        "\n",
        "\n"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50 labels per class\n",
            "Accuracy: 0.530\n",
            "Precision: 0.548\n",
            "Recall: 0.340\n",
            "F-Measure: 0.420\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    not sarc       0.52      0.72      0.61        50\n",
            "        sarc       0.55      0.34      0.42        50\n",
            "\n",
            "    accuracy                           0.53       100\n",
            "   macro avg       0.54      0.53      0.51       100\n",
            "weighted avg       0.54      0.53      0.51       100\n",
            "\n",
            "100 labels per class\n",
            "Accuracy: 0.505\n",
            "Precision: 0.507\n",
            "Recall: 0.370\n",
            "F-Measure: 0.428\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    not sarc       0.50      0.64      0.56       100\n",
            "        sarc       0.51      0.37      0.43       100\n",
            "\n",
            "    accuracy                           0.51       200\n",
            "   macro avg       0.51      0.51      0.50       200\n",
            "weighted avg       0.51      0.51      0.50       200\n",
            "\n",
            "250 labels per class\n",
            "Accuracy: 0.496\n",
            "Precision: 0.494\n",
            "Recall: 0.312\n",
            "F-Measure: 0.382\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    not sarc       0.50      0.68      0.57       250\n",
            "        sarc       0.49      0.31      0.38       250\n",
            "\n",
            "    accuracy                           0.50       500\n",
            "   macro avg       0.50      0.50      0.48       500\n",
            "weighted avg       0.50      0.50      0.48       500\n",
            "\n",
            "500 labels per class\n",
            "Accuracy: 0.496\n",
            "Precision: 0.493\n",
            "Recall: 0.300\n",
            "F-Measure: 0.373\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    not sarc       0.50      0.69      0.58       500\n",
            "        sarc       0.49      0.30      0.37       500\n",
            "\n",
            "    accuracy                           0.50      1000\n",
            "   macro avg       0.50      0.50      0.48      1000\n",
            "weighted avg       0.50      0.50      0.48      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaIU2fW-37gz"
      },
      "source": [
        "## Bert with parent comment and comment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc8D0m_aDQfM"
      },
      "source": [
        "### Remove mask from comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkmaz4Fn39PK"
      },
      "source": [
        "avg_log_50[\"comment\"] = avg_log_50[\"comment\"].str.replace('I am being [MASK]!', '', regex=False)\n",
        "avg_log_100[\"comment\"] = avg_log_100[\"comment\"].str.replace('I am being [MASK]!', '', regex=False)\n",
        "avg_log_250[\"comment\"] = avg_log_250[\"comment\"].str.replace('I am being [MASK]!', '', regex=False)\n",
        "avg_log_500[\"comment\"] = avg_log_500[\"comment\"].str.replace('I am being [MASK]!', '', regex=False)\n"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "0z6Zi9QG7Vys",
        "outputId": "2bcbf3af-6208-4dd0-a2cb-b8e3352b75bd"
      },
      "source": [
        "avg_log_500.head()\n"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parent_comment</th>\n",
              "      <th>comment</th>\n",
              "      <th>word1_logit_avg</th>\n",
              "      <th>word2_logit_avg</th>\n",
              "      <th>label</th>\n",
              "      <th>soft_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"Are you just a nasty girl with negative behavior who ate up for no good but drama and trouble?\" (Image 3 and 4 should be switched)</td>\n",
              "      <td>what sort of fucked up self destructive sociopath uses linkedin for dating stuff?</td>\n",
              "      <td>5.390528</td>\n",
              "      <td>6.044638</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"Bitcoin has brought a surge of new talent to the financial technology space.\"</td>\n",
              "      <td>Wait a minute... There was financial technology before Bitcoin.</td>\n",
              "      <td>5.443249</td>\n",
              "      <td>6.149345</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Don't you like a man looking at your boobs all day?\"</td>\n",
              "      <td>Mmm yeah I love men looking at my boobs all day, make's me feel like an object!</td>\n",
              "      <td>4.477148</td>\n",
              "      <td>6.029073</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"Hey, do you know Tomb Raider?\" \"No\"</td>\n",
              "      <td>\"Oh great.. You're one of 'those' gamers, aren't you?\"</td>\n",
              "      <td>6.566190</td>\n",
              "      <td>6.058030</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"Lightning follows the path of least resistance, which is why France suffers from the most lightning strikes.\"</td>\n",
              "      <td>Cue the reminder that the French Resistance was one of, if not the largest one in WW2 and made D-Day much less bloody by cutting Nazi supply lines and providing intelligence to the Allies.</td>\n",
              "      <td>5.308903</td>\n",
              "      <td>5.473442</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                        parent_comment  ... soft_labels\n",
              "0  \"Are you just a nasty girl with negative behavior who ate up for no good but drama and trouble?\" (Image 3 and 4 should be switched)  ...  0         \n",
              "1  \"Bitcoin has brought a surge of new talent to the financial technology space.\"                                                       ...  0         \n",
              "2  \"Don't you like a man looking at your boobs all day?\"                                                                                ...  0         \n",
              "3  \"Hey, do you know Tomb Raider?\" \"No\"                                                                                                 ...  1         \n",
              "4  \"Lightning follows the path of least resistance, which is why France suffers from the most lightning strikes.\"                       ...  0         \n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USpDwXjS74wA"
      },
      "source": [
        "### Run bert using the soft labels - 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJvhV9QS77lt"
      },
      "source": [
        "parent_comments = avg_log_50.parent_comment.values\n",
        "comments = avg_log_50.comment.values\n",
        "labels = avg_log_50.soft_labels.values"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJYea1vJ8TaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7ed5f1-5c87-4e2d-e055-10938afe594c"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFCRXB8B92ND"
      },
      "source": [
        "combined = [list(i) for i in zip(parent_comments, comments)]"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbO8lRC0-jCr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2218597d-8991-4c6c-8bec-5649fa33a369"
      },
      "source": [
        "combined"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['\"Are you just a nasty girl with negative behavior who ate up for no good but drama and trouble?\" (Image 3 and 4 should be switched)',\n",
              "  'what sort of fucked up self destructive sociopath uses linkedin for dating stuff? '],\n",
              " ['\"Women in tech\" ? &amp;#3232;\\\\_&amp;#3232;',\n",
              "  'Well, at least we don\\'t have \"Men in Beauty Industry\". '],\n",
              " [\"90% of those mods have been added in the last 12-months. But that's probably just a coincidence.\",\n",
              "  \"No, it's a right-wing conspiracy! \"],\n",
              " ['ASDERHNIP ADRIAN SHEPHARD',\n",
              "  \"Hey hey, that's just fans trolling themselves! \"],\n",
              " [\"Also, there's nothing responsible about drinking Old Milwaukee...\",\n",
              "  'Financial responsibility. '],\n",
              " [\"Alternatives to TrueCrypt? I setup my cousin with TrueCrypt when he left for China 2 years ago. Now that TrueCrypt isn't available what would my sysadmin bros and gals recommend?\",\n",
              "  \"I've heard of this thing called cryptolocker, works flawlessly. \"],\n",
              " ['And another one. Holy shit this one got 440 likes.',\n",
              "  'Clearly is has a firm grasp on reality '],\n",
              " [\"At least teletubbies don't hurt your ears...\",\n",
              "  'But Stampy has coherent words instead of the gibberish the Teletubbies make. '],\n",
              " ['Awwwwww. You talked about us!', 'first time for everything. '],\n",
              " ['Bernicrat. Ugh.',\n",
              "  \"Y'know, the people who registered as a Democrat two weeks ago and call everyone who disagrees with them DINOs. \"],\n",
              " [\"Can you imagine, WWIII is started because of a stoner comedy? Fuck, that's a depressing thought.\",\n",
              "  'yeh, because north korea really has the capability to start world war 3 '],\n",
              " ['Champa? Huehuehuehue',\n",
              "  \"It's obviously the WT announcer omg how can you not tell! \"],\n",
              " ['Family First', 'Maybe it means lowest tobacco quantity in town? '],\n",
              " ['Flying Lotus is on the sticky twice...',\n",
              "  '2 nights of Flying Lotus confirmed. '],\n",
              " ['He might be grown in a physical sense but clearly not a mental one',\n",
              "  'Big man, mental midget '],\n",
              " [\"He won't advertise it. This is SSD is for the enterprise market. Linus advertises for gamers\",\n",
              "  'Yeah because every gamer I know has a website made by squarespace. '],\n",
              " ['Hell just thinking that if you want to level a character from level 1, thats 97 levels you have to earn before you will even see your artifact.',\n",
              "  'It took 8 years for my characters to see their Artifact weapons. '],\n",
              " [\"Hey, I didn't write it.\", 'Where do I send my letter of complaint? '],\n",
              " ['How do babies get fed?', 'Shit. '],\n",
              " ['How is this world news?',\n",
              "  \"It isn't, but it is something that makes Muslims look bad, so why shouldn't we post it on this sub? \"],\n",
              " ['I actually hope demo gets a small loch buff that has the grenades explode on any contact rather the dissipating on the ground.',\n",
              "  'If that happened, nobody would play soldier anymore '],\n",
              " ['I always find it irritating when some uses being young as an insult.(The insult itself is childish)',\n",
              "  'quiet kid, the adults are talking... '],\n",
              " ['I compiled a quick list of all the women that are now banned from buying birth control: * * *',\n",
              "  'A war on women if I ever saw one. '],\n",
              " ['I feel like this might be cheating but, firearms instructor?',\n",
              "  'Yeah I know how you feel... '],\n",
              " ['I got one of them fancy shmancy 144Hz monitors, gotta get those extra frames for the smoothness!',\n",
              "  'Texture quality has minimum performance differences as long as you have the memory to support it. '],\n",
              " [\"I live in C-K. The anti-wind people are loud, obnoxious and truly form like sheep. I really think it's mostly people concerned about their property value (apparently they think turbines are sinfully-ugly)\",\n",
              "  'Because smokestacks belching out carcinogens are just *gorgeous* '],\n",
              " ['I love this app, when im playing rocket league and i put full blast some Kendrick lamar',\n",
              "  'What did you think of untitled unmastered? '],\n",
              " ['I regret posting this list now it missed out on some great movies.',\n",
              "  \"No sweat, not your problem that the list isn't complete! \"],\n",
              " ['I remember when i was like \"why i should buy items? better safe money for next game\"',\n",
              "  'I was so confused when I started lvl 1 again in my second game lol '],\n",
              " [\"I respect your idea. This doesn't affect me much because I never used the Blue Eye Orb, but your idea isn't all that bad if you can get a decent backing.\",\n",
              "  'Thanks man :) '],\n",
              " ['I thought Disney owned them now.',\n",
              "  'Disney did own them but they are asking the SEALs to train their H1b replacements. '],\n",
              " ['I thought exercise bikes were for hanging laundry on?',\n",
              "  'Laundry is for treadmills, coats go on the exercise bike! '],\n",
              " ['I was just about to say, who names kid \"the cones of dunshire\"?',\n",
              "  'Ben Wyatt '],\n",
              " ['I would be careful with the bolts so close to the edge of the carbon... there are industry standard spacings for a reason. Delamination in a crash is highly likely here.',\n",
              "  'Which bolts in particular are you referring to? '],\n",
              " [\"I'm firing blanks.\", 'r/childfree '],\n",
              " [\"I'm furious that I cannot vote for Bernie in my home state (stupid New York registration rules). So I decided to canvass for 12 hours today. Halfway done. Round two tomorrow! Let's take New York!\",\n",
              "  'Awesome '],\n",
              " [\"I'm infantry, is it just like, the general shit for patrolling or is that particular section of the handbook something I need to hit on.\",\n",
              "  'Chapter 11. '],\n",
              " ['In the 5 precincts with the highest percentage of African American registrants, HRC won all the delegates. 76-0',\n",
              "  'Obviously nobody told them that Bernie marched with MLK. '],\n",
              " ['Is Cannabis a performance enhancing drug?', 'Weed is a downer, so no. '],\n",
              " ['Jayna James', 'I remember this website very well '],\n",
              " ['Justices May Weigh Cases of Alabama Judges Overriding Juries',\n",
              "  \"I think Alabama's practice is terrifying and anti-democratic, but this court isn't going to strike it down. \"],\n",
              " ['Let me guess the \"gun death rate\" used in those sources includes suicides.',\n",
              "  'I gave you the sorce, why bother to guess? '],\n",
              " ['Lol. Ignorant Spud fans are the best.', \"Wow, I'm hurt \"],\n",
              " ['Look around. There are tons of good cops',\n",
              "  \"Yeah, they're protecting us from all those criminal black fellows! \"],\n",
              " [\"Maybe in 2 seasons depending on how Walker-Peters develops physically as he'd be a natural replacement but he's just too small to play right now.\",\n",
              "  'Ya all are forgetting thay super talent Yidlin '],\n",
              " ['Maybe they do want to cash out, but they dont wanna to qs their stuff?',\n",
              "  'Yeah no ban it '],\n",
              " ['Meh, we should just do it the way the rest of the civilized world does it.',\n",
              "  \"Well yeah, but just because literally every other industrialized nation does it doesn't mean it works, and every single data point shows they are better and more efficient over all doesn't mean it's better. \"],\n",
              " [\"Mike Pence: 'I Don't Understand' Michelle Obama's Criticism of Donald Trump\",\n",
              "  \"How could you, you're an idiot sir \"],\n",
              " ['No', 'I should have put an '],\n",
              " [\"No i feel you. I'd like to part that out but then it'll probably just sit around doing nothing. Lol\",\n",
              "  'Can you sell the white mods on its own '],\n",
              " [\"No, we aren't supposed to tip here.\", 'Not just the tip? '],\n",
              " ['OOC: Das is really just a dragon quest master',\n",
              "  \"OOC: Yup, that's all it is. \"],\n",
              " ['Obviously. PKK does more damage to Turkey than ISIS.',\n",
              "  'turkey does more damage to turkey than they both '],\n",
              " [\"Oh baby, do you work at Morton's? Because I can see the salt.\",\n",
              "  'Probably ARG with this type of banlist. '],\n",
              " ['Or a mammoth clitoris. Some women are afflicted with dickclit.',\n",
              "  'Well, that\\'ll be a good thought to have during \"dinner.\" '],\n",
              " ['People are willing to pay $7.99 a month for one program?',\n",
              "  'Shit dude I pay 2.50 an episode to Google for shows I like '],\n",
              " ['Press X to pay respects', 'Fucking peasant. '],\n",
              " ['Remember: always bring enough ear protection for everyone around',\n",
              "  'puppy murderer! '],\n",
              " [\"Right, there's no other context to suggest that at all, just that the poster supports hillary.\",\n",
              "  'I should add a '],\n",
              " [\"Rudy Giuliani: Before Obama came along we didn't have any successful radical Islamic terrorist attack inside the United States.\",\n",
              "  'Obama was born long before 9/11 so it must be his fault. '],\n",
              " ['Said no man ever', 'This is real. '],\n",
              " ['Sec. Kerry says Trump did not contact State Dept. prior to calls',\n",
              "  'Oh well, surely the Republicans will keep to their previous positions and crucify him for violating state department protocols. '],\n",
              " ['Self driving flying car cant wait',\n",
              "  \"I can't wait to hear all the crash stories on the news like the ones about tesla *NO SIR I DID NOT PRESS THE GAS AND DRIVE INTO THAT BUS* \"],\n",
              " [\"She's divorced for a reason. Rum while you can and don't look back.\",\n",
              "  'How will rum fix this? '],\n",
              " ['So FDA...admits it...\"Although the FDA recognized that completely switching to e-cigarettes may reduce the risk of tobacco-related disease for individuals currently smoking conventional cigarettes, it found that e-cigarettes still pose a number of significant health and safety risks\"',\n",
              "  'FDA, the bullshit meter just broke. '],\n",
              " [\"So much harder to bullshit anyone with smart phones and the internet at everyone's fingertips. Nowdays, you get called out immediately.\",\n",
              "  'As evidenced by the immense ammount of fact-checking in political debates these days. '],\n",
              " ['Tfw when many whites are descendants of slaves too, i.e irish',\n",
              "  'They hate water as well. '],\n",
              " ['That was a woman?',\n",
              "  'I thought it was just a random Indian guy that for some reason was speaking russian '],\n",
              " [\"The Bush administration lost 22 million emails about the Plame affair, and 5 million emails regarding Regent U (Pat Robertson seminary) attorneys' illegal firing of federal attorneys.\",\n",
              "  'Oh, well then I guess this is ok. '],\n",
              " [\"The Wild's tribute to Prince.\",\n",
              "  'Not sure if tribute to Prince or advertisement for Future '],\n",
              " ['The mere fact that Hilary is the democratic candidate is going to present a roadblock to that. The people who support Bernie tend to see Hilary as a cynical liar.',\n",
              "  \"Well, they're not wrong... \"],\n",
              " ['Third times a charm - finally worked. Thanks for your help',\n",
              "  'Glad it all worked out for you. '],\n",
              " ['This is getting kind of frustrating. I really want the Shovel Knight amiibo but I feel like his launch is going to be super abrupt when it does get announced.',\n",
              "  'The announcement will be that the release date is the same day. '],\n",
              " ['This makes me want to see the Haas car decked out in flames, racing stripes and tribal designs. And some truck nuts hanging off the back for good measure.',\n",
              "  'Need for speed underground was my favorite racing game as well '],\n",
              " ['Tips are your paycheck. FTFY', 'not my problem. '],\n",
              " [\"We won the game Weber would have been suspended for. I don't see why people are still upset over this.\",\n",
              "  'Reason has no place in this! '],\n",
              " ['Well duh some people act like that album is the best hip hop album in history.',\n",
              "  'We all know that Kanye\\'s \"My Beautiful Dark Twisted Fantasy\" is objectively better, anyway. '],\n",
              " ['Welsh Rarebit I made after work.', 'Needs more cheese '],\n",
              " ['What is IG?', 'Instagram. '],\n",
              " ['What was it supposed to say?',\n",
              "  'Not sure... probably NOT what it says now though. '],\n",
              " ['When is violence okay, if ever?',\n",
              "  'Protections of yourself and others, for sport and entertainment, and for fun (assuming everyone involved is on board). '],\n",
              " ['Where broadband is a utility, 100Mbps costs just $40 a month. Small Oregon city upgrades network to fiber, destroys competition.',\n",
              "  'The internet is still considered by many to be a luxury. '],\n",
              " ['Which of your opponents has the best chance to beat you at home this year?',\n",
              "  \"Well the Civil War is down south, so I can say with absolute confidence that the Ducks don't stand a very good chance of beating us at home this year. \"],\n",
              " ['Who Wins', 'The latest Batman is the most experienced of them all. '],\n",
              " ['Yahoo Email Surveillance: the Next Front in the Fight Against Mass Surveillance',\n",
              "  \"I'm sure google and bing would never do such a thing at the government's behest either. \"],\n",
              " ['Yay every ones favourite SHITTY am I the only one? Posts these are actually stupid of course your not the only fucking one',\n",
              "  \"lol I didn't mean it, my bad. \"],\n",
              " [\"Yeah don't care about the whole philosophical 1 2 3 you are doing here. It's just word salad to me.\",\n",
              "  \"This guy's a fucking goldmine of bad philosophy. \"],\n",
              " ['Yeah man. Killing gender-switching animals in the jangle with your bare hands is tough business.',\n",
              "  'Well yeah, you have to pay attention so you can stop hitting it when it turns in to a woman. '],\n",
              " ['Yes both of them were black', 'Well, this is completely shocking '],\n",
              " [\"You can't just say that and not tell us which actor! Come on man!\",\n",
              "  'Yahoo Serious, has to be. '],\n",
              " ['both of those instances would not include a healer',\n",
              "  'If a mercy is out of position, you go for it. '],\n",
              " ['does that look like Syria to you, OP? Do these guys look like IDF ?!',\n",
              "  'There are songs about the sprawling green hills of Syria. '],\n",
              " ['icb with 4 ft asiis lol, doesnt matter how big ur inv is still not a icb',\n",
              "  ';) '],\n",
              " ['if i have to get raped by the TSA every day i go to work ill probably quit my job',\n",
              "  'Man, I usually have to pay extra to get what the TSA offers for free. '],\n",
              " ['josh pan - real life', 'Hab u seen an alien? '],\n",
              " [\"member when I didn't give a fuck\", 'Member when i downvoted your ass? '],\n",
              " [\"my rapist doesn't know he's a rapist - slutwalk\",\n",
              "  \"*don't open the comments* *DO NOT OPEN THE COMMENTS* ....shit \"],\n",
              " ['probably should use a dull knife when practicing this shit',\n",
              "  'But a sharp knife is safer! '],\n",
              " ['so far yeah, only know of Iron Man and Captain Marvel so far',\n",
              "  \"The game's gonna have only Ryu, Rocketman, Ironman and Miss Marvel, confirmed. \"],\n",
              " [\"what if i'm a gaymer can I still buy it with the dlc?\",\n",
              "  'You just are unable to access the Heaven Map ']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEymwN3_88Py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d4c575-8f4a-4575-caf8-82e03611b358"
      },
      "source": [
        "batch_encoded_dict = tokenizer.batch_encode_plus(\n",
        "                      combined,\n",
        "                      add_special_tokens = True,\n",
        "                      max_length = 50,\n",
        "                      pad_to_max_length = True,\n",
        "                      return_token_type_ids=True,\n",
        "                      return_attention_mask = True,\n",
        "                      return_tensors = 'tf'\n",
        "                     )"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wacey4z2_fX4"
      },
      "source": [
        "labels = tf.convert_to_tensor(labels)"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk7htrCj_lMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb5a0a2-cde1-4f13-ea04-64bbc5c3c7a3"
      },
      "source": [
        "labels"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
              "array([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68QZ3NT-_op_"
      },
      "source": [
        "def example_to_features(input_ids, attention_masks, token_type_ids, y):\n",
        "  return {\"input_ids\": input_ids,\n",
        "          \"token_type_ids\": token_type_ids,\n",
        "          \"attention_mask\": attention_masks}, y\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((batch_encoded_dict['input_ids'], batch_encoded_dict['token_type_ids'], batch_encoded_dict['attention_mask'], \n",
        "                                              labels)).map(example_to_features)"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZlO7KAj_u79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f75c5cdc-5d30-4d7c-d8cb-4f67e1b57c86"
      },
      "source": [
        "# Train on 50% of the dataset, Validate on 25% of the dataset, Test on 25% of the dataset\n",
        "# Calculate the number of samples to include in each set\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.5 * dataset_size)\n",
        "val_size = int(0.25 * dataset_size)\n",
        "test_size = int(0.25 * dataset_size)\n",
        "dataset = dataset.shuffle(dataset_size)\n",
        "train_dataset = dataset.take(train_size)\n",
        "test_dataset = dataset.skip(train_size)\n",
        "val_dataset = dataset.skip(test_size)\n",
        "test_dataset = dataset.take(test_size)\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "print('{:>5,} test samples'.format(test_size))"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   50 training samples\n",
            "   25 validation samples\n",
            "   25 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1qZ_bObA5kP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c204095e-12ab-4ac0-ccf3-d3466f4d56da"
      },
      "source": [
        "from transformers import TFBertForSequenceClassification, TFTrainer, TFTrainingArguments, BertConfig\n",
        "\n",
        "config = BertConfig.from_pretrained('bert-base-cased', hidden_dropout_prob=0.5, num_labels=2)"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.5,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTsIkyiWBLcM"
      },
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    acc = np.sum(preds_flat == labels_flat) / len(labels_flat)\n",
        "    return {'accuracy': acc}"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThRGcY5JBNzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e070cb49-288d-41b3-e7da-fdff1241c1bf"
      },
      "source": [
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    num_train_epochs=2,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    learning_rate=5e-5,              # learning rate\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01               # strength of weight decay\n",
        ")\n",
        "\n",
        "with training_args.strategy.scope():\n",
        "    # Load TFBertForSequenceClassification, the pretrained BERT model with a single \n",
        "    # linear classification layer on top. \n",
        "    model = TFBertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-cased\", # Use the 12-layer BERT model, with cased vocab.\n",
        "        config = config   \n",
        "    )\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    compute_metrics=compute_metrics     # function that computes metrics \n",
        ")"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Tensorflow: setting up strategy\n",
            "loading weights file https://huggingface.co/bert-base-cased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/01800f4158e284e2447020e0124bc3f6aea3ac49848e744594f7cce8ee5ac0a4.a7137b2090d9302d722735af604b4c142ec9d1bfc31be7cbbe230aea9d5cfb76.h5\n",
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnLCvJbSBcpv"
      },
      "source": [
        "import transformers\n",
        "\n",
        "transformers.logging.set_verbosity_info()"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dt25OIpBg4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38f5232d-a304-4820-f9f5-2a61722144c3"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50\n",
            "  Num Epochs = 2.0\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Steps per epoch = 4\n",
            "  Total optimization steps = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Training took: 0:00:12.660386\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuG1a13pBjCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30aba90e-6287-4624-fb4a-36201d9ac5f8"
      },
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples in dataset = 25\n",
            "  Num examples in used in evaluation = 64\n",
            "  Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:5 out of the last 9 calls to <function TFTrainer.distributed_prediction_steps at 0x7f279aaeb710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.6800963282585144, 'eval_accuracy': 0.640625, 'epoch': 2.0, 'step': 8}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_accuracy': 0.640625, 'eval_loss': 0.6800963282585144}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhAIIGq9BmKd"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izkrGYnDDhE8"
      },
      "source": [
        "### Run bert using the soft labels - 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll78gLNNDhE9"
      },
      "source": [
        "parent_comments = avg_log_100.parent_comment.values\n",
        "comments = avg_log_100.comment.values\n",
        "labels = avg_log_100.soft_labels.values"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgh8eAJ8DhFJ"
      },
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TUQUXEODhFK"
      },
      "source": [
        "combined = [list(i) for i in zip(parent_comments, comments)]"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnM7oWSsDhFK"
      },
      "source": [
        "# combined"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEZxHsy6DhFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b37a0771-dec7-4a1e-d440-b5d3d9263f3d"
      },
      "source": [
        "batch_encoded_dict = tokenizer.batch_encode_plus(\n",
        "                      combined,\n",
        "                      add_special_tokens = True,\n",
        "                      max_length = 50,\n",
        "                      pad_to_max_length = True,\n",
        "                      return_token_type_ids=True,\n",
        "                      return_attention_mask = True,\n",
        "                      return_tensors = 'tf'\n",
        "                     )"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOp0M61JDhFK"
      },
      "source": [
        "labels = tf.convert_to_tensor(labels)"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkgV0nUxDhFK",
        "outputId": "f77ea2d9-daca-4723-c2dd-56eba68064e4"
      },
      "source": [
        "labels"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(200,), dtype=int64, numpy=\n",
              "array([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
              "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN0LMJ8cDhFL"
      },
      "source": [
        "def example_to_features(input_ids, attention_masks, token_type_ids, y):\n",
        "  return {\"input_ids\": input_ids,\n",
        "          \"token_type_ids\": token_type_ids,\n",
        "          \"attention_mask\": attention_masks}, y\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((batch_encoded_dict['input_ids'], batch_encoded_dict['token_type_ids'], batch_encoded_dict['attention_mask'], \n",
        "                                              labels)).map(example_to_features)"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfZOdESIDhFL",
        "outputId": "40fd4e49-c670-4034-c682-19e9e816dcef"
      },
      "source": [
        "# Train on 50% of the dataset, Validate on 25% of the dataset, Test on 25% of the dataset\n",
        "# Calculate the number of samples to include in each set\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.5 * dataset_size)\n",
        "val_size = int(0.25 * dataset_size)\n",
        "test_size = int(0.25 * dataset_size)\n",
        "dataset = dataset.shuffle(dataset_size)\n",
        "train_dataset = dataset.take(train_size)\n",
        "test_dataset = dataset.skip(train_size)\n",
        "val_dataset = dataset.skip(test_size)\n",
        "test_dataset = dataset.take(test_size)\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "print('{:>5,} test samples'.format(test_size))"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  100 training samples\n",
            "   50 validation samples\n",
            "   50 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7CCuRh2DhFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c24e8f-5ec1-4e4c-91bc-4f48cfe70a20"
      },
      "source": [
        "# from transformers import TFBertForSequenceClassification, TFTrainer, TFTrainingArguments, BertConfig\n",
        "\n",
        "config = BertConfig.from_pretrained('bert-base-cased', hidden_dropout_prob=0.5, num_labels=2)"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.5,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duDVARqHDhFL"
      },
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    acc = np.sum(preds_flat == labels_flat) / len(labels_flat)\n",
        "    return {'accuracy': acc}"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zezb5UdmDhFL",
        "outputId": "49229525-6b22-4f84-d41a-0f846a316209"
      },
      "source": [
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    num_train_epochs=2,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    learning_rate=5e-5,              # learning rate\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01               # strength of weight decay\n",
        ")\n",
        "\n",
        "with training_args.strategy.scope():\n",
        "    # Load TFBertForSequenceClassification, the pretrained BERT model with a single \n",
        "    # linear classification layer on top. \n",
        "    model = TFBertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-cased\", # Use the 12-layer BERT model, with cased vocab.\n",
        "        config = config   \n",
        "    )\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    compute_metrics=compute_metrics     # function that computes metrics \n",
        ")"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Tensorflow: setting up strategy\n",
            "loading weights file https://huggingface.co/bert-base-cased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/01800f4158e284e2447020e0124bc3f6aea3ac49848e744594f7cce8ee5ac0a4.a7137b2090d9302d722735af604b4c142ec9d1bfc31be7cbbe230aea9d5cfb76.h5\n",
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32JUYDvjDhFM"
      },
      "source": [
        "# import transformers\n",
        "\n",
        "transformers.logging.set_verbosity_info()"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exPhOCQqDhFM",
        "outputId": "8ff003dd-eb4b-4b69-9886-522d779a2bec"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 100\n",
            "  Num Epochs = 2.0\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Steps per epoch = 7\n",
            "  Total optimization steps = 14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Training took: 0:00:13.234917\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzyG-v9DDhFM",
        "outputId": "785ff629-d718-4eaf-e3aa-a22ffd58d2ee"
      },
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples in dataset = 50\n",
            "  Num examples in used in evaluation = 64\n",
            "  Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:6 out of the last 10 calls to <function TFTrainer.distributed_prediction_steps at 0x7f279ac567a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.7347804307937622, 'eval_accuracy': 0.515625, 'epoch': 2.0, 'step': 14}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_accuracy': 0.515625, 'eval_loss': 0.7347804307937622}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmose0PSDhFM"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4R92BDxEcLN"
      },
      "source": [
        "### Run bert using the soft labels - 250"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeU999w7EcLY"
      },
      "source": [
        "parent_comments = avg_log_250.parent_comment.values\n",
        "comments = avg_log_250.comment.values\n",
        "labels = avg_log_250.soft_labels.values"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h5FMNthEcLZ",
        "outputId": "c69ded16-fb15-4576-b16d-14221161ea6d"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUE6My7-EcLa"
      },
      "source": [
        "combined = [list(i) for i in zip(parent_comments, comments)]"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGP14ERjEcLb",
        "outputId": "2740ec73-1211-42f2-c38d-5a878aff7ce0"
      },
      "source": [
        "batch_encoded_dict = tokenizer.batch_encode_plus(\n",
        "                      combined,\n",
        "                      add_special_tokens = True,\n",
        "                      max_length = 50,\n",
        "                      pad_to_max_length = True,\n",
        "                      return_token_type_ids=True,\n",
        "                      return_attention_mask = True,\n",
        "                      return_tensors = 'tf'\n",
        "                     )"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H48ygTGsEcLb"
      },
      "source": [
        "labels = tf.convert_to_tensor(labels)"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD5lX4CwEcLb",
        "outputId": "10f3d29b-f4ea-4709-9f4a-cd1b719a0fc4"
      },
      "source": [
        "labels"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(500,), dtype=int64, numpy=\n",
              "array([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
              "       0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h2RSBWMEcLc"
      },
      "source": [
        "def example_to_features(input_ids, attention_masks, token_type_ids, y):\n",
        "  return {\"input_ids\": input_ids,\n",
        "          \"token_type_ids\": token_type_ids,\n",
        "          \"attention_mask\": attention_masks}, y\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((batch_encoded_dict['input_ids'], batch_encoded_dict['token_type_ids'], batch_encoded_dict['attention_mask'], \n",
        "                                              labels)).map(example_to_features)"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdW6rTnSEcLc",
        "outputId": "ad4367e4-cb2d-4e80-8115-505e8079efda"
      },
      "source": [
        "# Train on 50% of the dataset, Validate on 25% of the dataset, Test on 25% of the dataset\n",
        "# Calculate the number of samples to include in each set\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.5 * dataset_size)\n",
        "val_size = int(0.25 * dataset_size)\n",
        "test_size = int(0.25 * dataset_size)\n",
        "dataset = dataset.shuffle(dataset_size)\n",
        "train_dataset = dataset.take(train_size)\n",
        "test_dataset = dataset.skip(train_size)\n",
        "val_dataset = dataset.skip(test_size)\n",
        "test_dataset = dataset.take(test_size)\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "print('{:>5,} test samples'.format(test_size))"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  250 training samples\n",
            "  125 validation samples\n",
            "  125 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnJGroKLEcLc",
        "outputId": "267a426c-f743-403a-b9b3-98a269e34297"
      },
      "source": [
        "config = BertConfig.from_pretrained('bert-base-cased', hidden_dropout_prob=0.5, num_labels=2)"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.5,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsRXMsE_EcLd"
      },
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    acc = np.sum(preds_flat == labels_flat) / len(labels_flat)\n",
        "    return {'accuracy': acc}"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdH09ntxEcLd",
        "outputId": "0580b42a-82cf-46f6-e167-75e845dd2680"
      },
      "source": [
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    num_train_epochs=2,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    learning_rate=5e-5,              # learning rate\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01               # strength of weight decay\n",
        ")\n",
        "\n",
        "with training_args.strategy.scope():\n",
        "    # Load TFBertForSequenceClassification, the pretrained BERT model with a single \n",
        "    # linear classification layer on top. \n",
        "    model = TFBertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-cased\", # Use the 12-layer BERT model, with cased vocab.\n",
        "        config = config   \n",
        "    )\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    compute_metrics=compute_metrics     # function that computes metrics \n",
        ")"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Tensorflow: setting up strategy\n",
            "loading weights file https://huggingface.co/bert-base-cased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/01800f4158e284e2447020e0124bc3f6aea3ac49848e744594f7cce8ee5ac0a4.a7137b2090d9302d722735af604b4c142ec9d1bfc31be7cbbe230aea9d5cfb76.h5\n",
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdRIZZOSEcLd"
      },
      "source": [
        "transformers.logging.set_verbosity_info()"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkwT-zqMEcLd",
        "outputId": "931a785d-3a2c-403c-afe1-9d06a09fcfbf"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 250\n",
            "  Num Epochs = 2.0\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Steps per epoch = 16\n",
            "  Total optimization steps = 32\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Training took: 0:00:15.662529\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjg9fmHoEcLe",
        "outputId": "bc6338df-665d-41b3-a4a3-f64302e82ee0"
      },
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples in dataset = 125\n",
            "  Num examples in used in evaluation = 128\n",
            "  Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function TFTrainer.distributed_prediction_steps at 0x7f2799a563b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.681489884853363, 'eval_accuracy': 0.625, 'epoch': 2.0, 'step': 32}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_accuracy': 0.625, 'eval_loss': 0.681489884853363}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6l63CrZEcLe"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubnSL37jEfu_"
      },
      "source": [
        "### Run bert using the soft labels - 500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y55u7HLdEfu_"
      },
      "source": [
        "parent_comments = avg_log_500.parent_comment.values\n",
        "comments = avg_log_500.comment.values\n",
        "labels = avg_log_500.soft_labels.values"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEBzCqltEfu_",
        "outputId": "64bb8861-62f2-4e9f-daa8-084f03d702a0"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOuvSiAiEfvA"
      },
      "source": [
        "combined = [list(i) for i in zip(parent_comments, comments)]"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg1QzQ12EfvB",
        "outputId": "431e468e-a092-431e-a5fa-f9c086d7e81d"
      },
      "source": [
        "batch_encoded_dict = tokenizer.batch_encode_plus(\n",
        "                      combined,\n",
        "                      add_special_tokens = True,\n",
        "                      max_length = 50,\n",
        "                      pad_to_max_length = True,\n",
        "                      return_token_type_ids=True,\n",
        "                      return_attention_mask = True,\n",
        "                      return_tensors = 'tf'\n",
        "                     )"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYdwzMwaEfvC"
      },
      "source": [
        "labels = tf.convert_to_tensor(labels)"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfjfqUZcEfvC",
        "outputId": "eb22e0fb-4dd5-4aec-897a-9150fbc493fd"
      },
      "source": [
        "labels"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1000,), dtype=int64, numpy=\n",
              "array([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
              "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
              "       1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
              "       1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
              "       0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
              "       1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
              "       0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4gnTb-nEfvC"
      },
      "source": [
        "def example_to_features(input_ids, attention_masks, token_type_ids, y):\n",
        "  return {\"input_ids\": input_ids,\n",
        "          \"token_type_ids\": token_type_ids,\n",
        "          \"attention_mask\": attention_masks}, y\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((batch_encoded_dict['input_ids'], batch_encoded_dict['token_type_ids'], batch_encoded_dict['attention_mask'], \n",
        "                                              labels)).map(example_to_features)"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMKLnVTEEfvC",
        "outputId": "366a38f3-4c1a-4f2f-d8dd-b79e69623a1f"
      },
      "source": [
        "# Train on 50% of the dataset, Validate on 25% of the dataset, Test on 25% of the dataset\n",
        "# Calculate the number of samples to include in each set\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.5 * dataset_size)\n",
        "val_size = int(0.25 * dataset_size)\n",
        "test_size = int(0.25 * dataset_size)\n",
        "dataset = dataset.shuffle(dataset_size)\n",
        "train_dataset = dataset.take(train_size)\n",
        "test_dataset = dataset.skip(train_size)\n",
        "val_dataset = dataset.skip(test_size)\n",
        "test_dataset = dataset.take(test_size)\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "print('{:>5,} test samples'.format(test_size))"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  500 training samples\n",
            "  250 validation samples\n",
            "  250 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBteFZdyEfvC",
        "outputId": "43fe0451-ddf3-4ecc-e01c-18b5bc1626c7"
      },
      "source": [
        "config = BertConfig.from_pretrained('bert-base-cased', hidden_dropout_prob=0.5, num_labels=2)"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.5,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2kjdhNLEfvD"
      },
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    acc = np.sum(preds_flat == labels_flat) / len(labels_flat)\n",
        "    return {'accuracy': acc}"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVHfANOOEfvD",
        "outputId": "2338d8b1-c0a5-45db-8d06-1d5c5295a689"
      },
      "source": [
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    num_train_epochs=2,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    learning_rate=5e-5,              # learning rate\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01               # strength of weight decay\n",
        ")\n",
        "\n",
        "with training_args.strategy.scope():\n",
        "    # Load TFBertForSequenceClassification, the pretrained BERT model with a single \n",
        "    # linear classification layer on top. \n",
        "    model = TFBertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-cased\", # Use the 12-layer BERT model, with cased vocab.\n",
        "        config = config   \n",
        "    )\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    compute_metrics=compute_metrics     # function that computes metrics \n",
        ")"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Tensorflow: setting up strategy\n",
            "loading weights file https://huggingface.co/bert-base-cased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/01800f4158e284e2447020e0124bc3f6aea3ac49848e744594f7cce8ee5ac0a4.a7137b2090d9302d722735af604b4c142ec9d1bfc31be7cbbe230aea9d5cfb76.h5\n",
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6OdywNFEfvD"
      },
      "source": [
        "transformers.logging.set_verbosity_info()"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUHI1BfBEfvD",
        "outputId": "3f8667a1-477a-45a5-df50-50286a70a969"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 2.0\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Steps per epoch = 32\n",
            "  Total optimization steps = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Training took: 0:00:19.229913\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESRBduusEfvE",
        "outputId": "948bb569-9541-489a-ad10-f45cbb2fabf6"
      },
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples in dataset = 250\n",
            "  Num examples in used in evaluation = 256\n",
            "  Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function TFTrainer.distributed_prediction_steps at 0x7f279addc5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5999228954315186, 'eval_accuracy': 0.7265625, 'epoch': 2.0, 'step': 64}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_accuracy': 0.7265625, 'eval_loss': 0.5999228954315186}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjbvWDuuEfvE"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5RiELlcNZ2S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p50BElLGtNRS"
      },
      "source": [
        "## Error Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUxsDEnxvJ_L"
      },
      "source": [
        "### Get predictions for 500 examples per class (1000 total)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTyPM_I0tOcH",
        "outputId": "1ef233aa-860a-48e3-f388-d4cbe0d1a0a3"
      },
      "source": [
        "pred_labels_1000 = trainer.predict(test_dataset).label_ids"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples in dataset = 250\n",
            "  Batch size = 64\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD8axfumuS49",
        "outputId": "88991a54-210b-4929-d3b4-15dbc41d6d0f"
      },
      "source": [
        "pred_labels_1000"
      ],
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLm82jnZvUwY"
      },
      "source": [
        "test_np = list(test_dataset.as_numpy_iterator())\n",
        "true_labels_1000 = []\n",
        "for elem in test_np:\n",
        "  true_labels_1000.append(elem[1])"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg3GJL1Dvc1n",
        "outputId": "64de7726-61a1-4837-e8ef-50ac09c2e85f"
      },
      "source": [
        "# accuracy\n",
        "acc = accuracy_score(true_labels_1000, pred_labels_1000)\n",
        "print('Accuracy: %.3f' % acc)\n",
        "\n",
        "# precision\n",
        "precision = precision_score(true_labels_1000, pred_labels_1000, average='binary', zero_division = 0)\n",
        "print('Precision: %.3f' % precision)\n",
        "\n",
        "# recall\n",
        "recall = recall_score(true_labels_1000, pred_labels_1000, average='binary', zero_division = 0)\n",
        "print('Recall: %.3f' % recall)\n",
        "\n",
        "#f1\n",
        "score = f1_score(true_labels_1000, pred_labels_1000, average='binary')\n",
        "print('F-Measure: %.3f' % score)\n",
        "\n",
        "target_names = ['not sarc', 'sarc']\n",
        "print(classification_report(true_labels_1000, pred_labels_1000, target_names=target_names))\n"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.568\n",
            "Precision: 0.256\n",
            "Recall: 0.309\n",
            "F-Measure: 0.280\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    not sarc       0.72      0.66      0.69       182\n",
            "        sarc       0.26      0.31      0.28        68\n",
            "\n",
            "    accuracy                           0.57       250\n",
            "   macro avg       0.49      0.49      0.49       250\n",
            "weighted avg       0.59      0.57      0.58       250\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jUTfOXCyvb2"
      },
      "source": [
        "### Specific Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dISM94kry_8d"
      },
      "source": [
        "#### Untokenize the comments and put them into a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDahuHiQ0QX1"
      },
      "source": [
        "untokenized_1000 = []\n",
        "for elem in test_np:\n",
        "  elem_input_ids = elem[0][\"input_ids\"]\n",
        "  elem_tokens = tokenizer.convert_ids_to_tokens(elem_input_ids)\n",
        "  untokenized_1000.append(\" \".join(elem_tokens))"
      ],
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byrHReNbzOWr",
        "outputId": "04607bc4-82b8-4b01-c189-a02aafc303e1"
      },
      "source": [
        "untokenized_1000[:5]"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] A Different Kind of Up ##grade ( Power ##line adapt ##ers for Wire ##d Internet ) [SEP] what exactly are these ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] * Ha ##zed ? * You miss ##pelled indicted . [SEP] Probably not if it wasn ' t his intent to la ##und ##er . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Damn . I loved Cloud Atlas and would love to see a 4 hour version . That film had a lot of flaws , but it was so ambitious the flaws felt ne ##gli ##gible . [SEP] Nah man I want another re ##boo ##t [SEP] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Just like they ruined the Elder Sc ##rolls series , the GT ##A series , the Fall ##out series , the Witch ##er series . . [SEP] I hated how Counter ##st ##rik ##e and Team fortress ruined my half ##life and q ##ua ##ke ##world gameplay ! [SEP]',\n",
              " '[CLS] People are willing to pay $ 7 . 99 a month for one program ? [SEP] Shit dude I pay 2 . 50 an episode to Google for shows I like [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 283
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RiVtWVJ1Nje"
      },
      "source": [
        "fp = []\n",
        "fn = []\n",
        "tp = []\n",
        "tn = []\n",
        "\n",
        "for ind, (l1, l2) in enumerate(zip(true_labels_1000, pred_labels_1000)):\n",
        "  if l1 == 0 and l2 == 1: #predicted as sarcastic but actually not\n",
        "    fp.append(ind)\n",
        "  elif l1 == 1 and l2 == 0: # predicted as not but actually are sarcastic\n",
        "    fn.append(ind)\n",
        "  elif l1 == 1 and l2 == 1: # predicted as sarc and correct\n",
        "    tp.append(ind)\n",
        "  else: # predicted as not and it's correct\n",
        "    tn.append(ind)"
      ],
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZkuBrbxy2s0"
      },
      "source": [
        "#### Examples that were predicted as sarcastic but are actually not sarcastic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15QEmyrM0v_J",
        "outputId": "efab6a8d-01f1-48b7-971b-76d009e3b663"
      },
      "source": [
        "[untokenized_1000[i] for i in fp]"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"[CLS] * Ha ##zed ? * You miss ##pelled indicted . [SEP] Probably not if it wasn ' t his intent to la ##und ##er . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] People are willing to pay $ 7 . 99 a month for one program ? [SEP] Shit dude I pay 2 . 50 an episode to Google for shows I like [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Exactly this has nothing to do with tax ##payers . Just a somewhat dubious but very successful company . [SEP] One that \" dubious ##ly \" and \" allegedly \" might possibly be engaging in things like front - running and other similar ass ##orted * ah ##em [SEP]',\n",
              " '[CLS] Going to Kris ##py K ##rem ##e when the \" Hot \" sign is on . . . [SEP] They have an app to let you know when the hot ##light is on by the way . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] I am here to educate you GE is bad and you should feel bad for voting \" yes \" . Stop ruin ##ing perfectly good games for your convenience . [SEP] I feel so educated now , thank you . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] 15 year old homes ##cho ##ole ##d son of a Pastor guns down his entire family [SEP] I blame video games , not the fact that this kid had easy access to high powered weapons . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] This band hired someone off of c ##rai ##gs ##list for $ 25 to dance during their set . [SEP] Need ##s more re ##ver ##b [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] What kind of music can you just not stand , and why ? [SEP] Nothing mainstream , because I ' m a hips ##ter [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] So progressive , so tolerant . [SEP] If she had just put a Bernie Sanders stick ##er on the laptop , then there wouldn ' t have been a problem ; you see , this is her fault [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] LA , Winnipeg , Phoenix , Nashville , Minnesota and Hartford ? [SEP] Wrong on Winnipeg , Minnesota , and Hartford . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] W ##H ##Y IS TO ##N ##Y ROM ##O SL ##EP ##T ON E ##VE ##R ##Y Y ##EA ##R ? [SEP] If he doesn ' t want to be slept on he shouldn ##t look so com ##fy [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Hmm don ' t see K ##hal ##dor in here telling everybody they are wrong . . . . [SEP] K ##hal ##dor have low mm ##r on hot ##s ##log ##s so obviously its in ##ac ##curate [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Bern ##ic ##rat . U ##gh . [SEP] Y ' know , the people who registered as a Democrat two weeks ago and call everyone who disagree ##s with them D ##IN ##Os . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Girls having standards and expectations . It ' s getting old , ladies . [SEP] Only one I agree with [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Tony Abbott warns Indonesia on borders [SEP] Gotta make sure those little brown people know there place , and that they are no match for a pair of stunning ##ly fashioned red b ##ud ##gie smug ##gler ##s . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Down ##vo ##ting is supposed to be for comments that don ' t contribute to the discussion , not comments you disagree with . [SEP] What about down ##vo ##ting O ##P for making a comment on a post I don ' t like ? [SEP] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] 144 ##f ##ps is like watching real motion to me [SEP] 144 ##f ##ps peasant , 256 ##f ##ps master race [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] So apparently b ##ots do this now [SEP] Report b ##ot for grief ##ing [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Oh happy dagger , this is thy sheath . [SEP] There r ##ust and let me die . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] 3 . 5 / 4 * ft ##fy [SEP] A ##y ##y [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] What ' s in your gym bag ? [SEP] G ##ym shoes , water bottle , dip belt , notebook , pen . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] You are awarded $ 10 , 000 , 000 , but you must spend it with one sub ##red ##dit . Which do you choose ? [SEP] r / p ##c ##master ##race [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] But the demo ##crats are already doing those three things . [SEP] Glad you called them out because they are definitely the only party doing s ##hady things . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Jay ##na James [SEP] I remember this website very well [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] why is small ball a bad thing ? Why is it something that needs to be \" fixed \" ? [SEP] Because basketball shouldn \\' t be about skill , it should be about being tall , like in the old days . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] A ##pp ##roach them , drop your pants , and ask for a full cavity search ? [SEP] But I wanna search their ca ##vi ##ties [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] It is true , and not made up . My age and gender are stated accurately . Please re - read . [SEP] Yes , I believe you because it is well known that nobody ever lies on the Internet . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] You must not work a job that receives g ##rat ##uit ##ies . If you did , you would not want this sunshine on all your income that you don ' t report for tax purposes , the [SEP] I only get paid in bit ##co ##ins . [SEP]\",\n",
              " '[CLS] It was just a joke , obviously she couldn ##t hold an opinion to save her life [SEP] Meanwhile no one even knows who govern ##s au ##st ##ria . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Any advice for a high school student looking at EC ##E as a career ? [SEP] Go to college and graduate from college . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] face ##pal ##m [SEP] yeah not knowing that fights ##tick ##s work on p ##c is definitely something to face ##pal ##m about . . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] If there is one thing we have learned from marijuana , it ' s that states are not o ##b ##liga ##ted to even attempt to follow [SEP] Yeah , because legal marijuana and legal slavery are ^ one ^ and ^ the ^ same . . . [SEP]\",\n",
              " \"[CLS] Hancock ' s reaction if you kill Finn as soon as you walk into Good N ##ei ##gh ##bo ##ur [SEP] With high Cha ##rism ##a and In ##ti ##mi ##dation , can you a ##gg ##ro him , then p ##ac ##ify him , and command him [SEP]\",\n",
              " \"[CLS] Steam ##pu ##nk Nix ##ie Tu ##be Clock - 1860 ' s looks with 1960 ' s technology . Full ##y As ##se ##mble ##d ! [SEP] I wonder how much electricity it needs to run . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Bulls ##hit . You got this from Super ##bad [SEP] You caught me , the shame . . . . . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] We can meet at the pub mentioned at 7 if everyone ' s down [SEP] 7 at the Hampshire Hotel sounds good see you there [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Minnesota says cigarette smuggling is a growing problem [SEP] Well , let ' s make sure we start banning e - c ##ig ##s so we can keep problems like this going . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] We use the black cards from cards against humanity [SEP] So do you re ##cu ##rs ##ively keep playing cards to fill the blank ##s ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] lose weight [SEP] This is literally smile with your eyes and don ' t be a fat ##ass . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Everything is made in China or k ##ore ##a [SEP] Even Cut ##co ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] that means you harvested 1 million stone in this wipe alone [SEP] That ' s T ##W ##O T ##H ##O ##US ##AN ##D nodes . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Sc ##ri ##bble [SEP] o ma ##h god mob ##il link p ##ls d ##lt na ##ow [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Anyone realize how low the score is compared to other games [SEP] If you want low scoring , check out the SD ##SU - C ##in ##cy game on es ##p ##n ##2 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] 6 rockets fired , 5 land short in Gaza . [SEP] maybe they are testing to see if their tunnels are rocket proof . . . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] It hurts me to say this , but Baylor . Sur ##prise team of the year . [SEP] You broke the circle ##jer ##k ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] I didn ' t get and I was born and raised speaking English in an English country . [SEP] Ye ##a but you didn ' t get it for other reasons [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] How do I beat him at L ##10 ? They are already ma ##xed . . . [SEP] Try DD armor ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] The Li ##bert ##arian dream realized . [SEP] Well the people who die won ##t buy more product , so the market will fix the problem without the na ##nny state . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] V ##ar ##lam ##ov speaks to Russian media , says the A ##v ##s are \" waiting for Ra ##du ##lov . He \\' [SEP] He \\' s just what the A ##v ##s need , a highly skilled forward with no defensive awareness and a pen ##chan [SEP]',\n",
              " \"[CLS] I wish they ' d just remove idol ##s straight out . Like , who the hell decided it ' d be a good idea ? What was wrong with . . . oh [SEP] damn I wish they removed to ##tem ##s too so annoying to get [SEP]\",\n",
              " \"[CLS] Have you log ##ged on to anyone else ' s ps ##4 ? [SEP] Lo ##l , I ' ve answered this . . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] The Wild ' s tribute to Prince . [SEP] Not sure if tribute to Prince or advertisement for Future [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] I guess if I were bi it would be \" so - so ho ##mo \" . [SEP] Just the tip . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] In November , San Francisco voters [ increased minimum wage to $ 15 per hour by 2018 . is not a financially viable business [SEP] But surely the l ##iber ##tarian ##s told us this wouldn ' t happen , because everyone will have more spending power , [SEP]\",\n",
              " \"[CLS] Down ##loading all that may have some serious ram ##ifications . [SEP] You ' re really D ##IM ##M ##wi ##tted aren ' t you ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] My CPU can ' t handle Cities : Sky ##lines once I get to a certain point , I plan on getting the i [SEP] Op , your AP ##U can , I used to have an A ##8 - 650 ##0 myself , it ran Cities Sky [SEP]\",\n",
              " \"[CLS] Not sure what this has to do with the subject at hand . [SEP] probably nothing because it ' s been deleted . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Mir ##ill ##is Action ! Game Record ##er 24 Hour Give ##away - Support ##s AM ##D A ##pp & am ##p ; N ##vid ##ia C ##U ##DA for high F ##PS recording and low disk usage . [SEP] Ch ##eers man : ) [SEP] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Give ##away - 12 Indie Gala 6 bundle ##s [SEP] here to enter , also ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Because its a bullshit story designed to r ##ile folks up and distract from real issues . [SEP] The willingness of our current president and probable future presidential candidate to lie to the American people and congress surely doesn ' t effect other issues . [SEP] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Almost $ 5 million in free R ##ift ##s , so it would seem so . [SEP] we found the reason for the rid ##ico ##ulus price [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKECuAFrzK_Y"
      },
      "source": [
        "#### Examples that were predicted as not sarcastic but are actually sarcastic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akOdnvVGyFqa",
        "outputId": "12f5c340-b669-4df6-d9a1-ea1b253cef95"
      },
      "source": [
        "[untokenized_1000[i] for i in fn]"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] Damn . I loved Cloud Atlas and would love to see a 4 hour version . That film had a lot of flaws , but it was so ambitious the flaws felt ne ##gli ##gible . [SEP] Nah man I want another re ##boo ##t [SEP] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Just like they ruined the Elder Sc ##rolls series , the GT ##A series , the Fall ##out series , the Witch ##er series . . [SEP] I hated how Counter ##st ##rik ##e and Team fortress ruined my half ##life and q ##ua ##ke ##world gameplay ! [SEP]',\n",
              " \"[CLS] You ' re going to spa ##m 4 ##chan with gay p ##orn . How , exactly , will anyone be able to notice ? [SEP] Holy shit they ' ve been raiding / b / for years ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Let ' s form a cult [SEP] I ' ll be the mascot . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] What was the most fucked up thing that you witnessed in a school hallway ? [SEP] Some guy fucking with a au ##tist ##ic kid , unlike most schools most people here would step In the administration literally thought it was a gang fight of literally 7 on [SEP]',\n",
              " '[CLS] So when a Christian points to prop ##he ##cies that they say came true does that mean Christianity is true ? [SEP] Well no , because Islam is true obviously [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] That ' s racist [SEP] Like how the re ##fs hate s ##love ##nian ##s ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] I may or may not have worn my mother ' s panties , shoved her v ##ib ##rator in my ass and turned it on , and then mast ##ur ##bate ##d . [SEP] When did you first start actually murdering those women ? [SEP] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Test [SEP] [ * butt ##s ^ butt ##s ^ butt ##s ^ butt ##s ^ butt ##s ^ butt ##s * ] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Take two teams and combine their roster ##s , then trim it down to 25 men . Which two teams create the best fused team ? [SEP] Mets and Blue j ##ays . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Big mistake ? Like invading Iraq big ? [SEP] Or skip ##ping Fall ##out 4 big ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] I said \" oh my god \" out loud by accident . I \\' m alone . I was just very overwhelmed by how fucking adorable he was being . [SEP] I was smiling so hard ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] I noticed that too . You ' re * only * 30 ! Dude I already told you you ' re too g ##d young . That doesn [SEP] But my book says all women over 29 are co ##uga ##rs and totally want younger dude ##s ! [SEP]\",\n",
              " '[CLS] Never heard of a hung ##over K ##ling ##on . ; ) [SEP] There are sober k ##ling ##ons ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] 5 Ranger ve ##ts vs . 5 Bo ##S p ##ala ##din ##s . [SEP] What weapons do the Rangers have and are we talking Out ##cast ##s , East Coast , or New Vegas Pa ##lad ##ins ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] R ##IT painting class for absolute begin ##ners ? this b ##ob r ##oss twitch stream get ##tin me h ##y ##pt as fuck [SEP] You could just watch a bunch of Bob Ross videos [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] H ##ype ##d but expecting us to blow it . Atlanta sports j ##s [SEP] No dude , this year ' s different [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] G ##rap ##hing Dani ##ca Patrick ' s Average Running Po ##si ##tion By Year , 2012 - 2015 . [SEP] But as D ##W always says , she is finally starting to catch on and is R ##EA ##LL ##Y improving in stock cars ! [SEP] [PAD] [PAD]\",\n",
              " \"[CLS] What ' s the leaf ##iest part about this ? The thumb ##nail or the title ? [SEP] Ty ##coon Shit ##ema does it again [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Uh , well double * w ##oos ##h * is suppose . [SEP] You must be who ##oshi ##ng his whose , but then I realized I who ##osh ##ed you who ##sh on his who ##sh . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] u / creative ##lya ##bs ##ent draws red ##dit ##ors over at r / red ##dit ##gets ##dra ##wn . I think s / he deserves a L ##OT [SEP] I would love for someone to draw me , but I never get picked in these things . [SEP]',\n",
              " \"[CLS] So , none of you people ever made friends with your cow ##or ##kers and made terrible sexual jokes with them ? I always [SEP] No , you ' re a horrible human being and should rot in hell for acknowledging that women have reproductive organs between the [SEP]\",\n",
              " '[CLS] St ##uff ##s . I don \\' t know if people wherever you all live use ( or mi ##suse ) it this way but a lot of people say \" stuff ##s \" instead [SEP] Uh ##h ye ##a it means more than one stuff obviously . [SEP]',\n",
              " \"[CLS] Wow this post is terrible . Its not because we ' re des ##ens ##iti ##zed to violence you wit . It ' s because animals don ' t have the intelligence level to fully understand why something is happening to them [SEP] Please tell me you forgot [SEP]\",\n",
              " \"[CLS] If no one ' s time is more important than life and limb , wouldn ' t the via ##duct have been closed years ago ? [SEP] We ' d also have lower highway speed limits . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Hell just thinking that if you want to level a character from level 1 , that ##s 97 levels you have to earn before you will even see your artifact . [SEP] It took 8 years for my characters to see their Art ##if ##act weapons . [SEP] [PAD]',\n",
              " '[CLS] This author seems to think \" you have to be like me to be successful , so here \\' s how to be me . \" [SEP] but this guy is 23 years old , and it seems like he has * everything * figured out ! [SEP] [PAD]',\n",
              " '[CLS] Local paper does piece on Grandma Shirley [SEP] Wow , that is awesome . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Co ##mbo decks mostly . B ##urs ##t Priest , freeze mage , etc . I also enjoy a ##gg ##ro . [SEP] Check out Sc ##ap ##esh ##ift and Storm [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] When I heard that the Taliban and IS ##IS announced ji ##had on each other [SEP] The middle east ' s future sure does seem r ##osy [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Even Tomb Rai ##der 2013 looks better . [SEP] Even Half life 2 episode 2 ( 2007 ) looks better [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] What are some of the most awesome / beautifully illustrated panels displaying action or fighting you ' ve seen in comics ? [SEP] In ##ser ##t any splash page from J ##H Williams III . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] The Joy of Top Lane by D ##yr ##us [SEP] Get em just like so , god b ##less . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] # P ##OT ##US ##45 S ##IG ##NE ##D M ##Y S ##ON ' S ABS ##EN ##T NO ##TE F ##OR SC ##H ##O ##OL ! # MA ##GA [SEP] is that a white su ##p ##rem ##ac ##ist t shirt [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] How do babies get fed ? [SEP] Shit . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Ya I can ##t recall one WR ##P ##G that has pre ##pu ##bes ##can ##t girls with thigh skirts and pan ##ty shots while the girl is all go ##og ##ley eyed for your every move . [SEP] The Last of Us ? [SEP] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Yes . 21 episodes for this season . [SEP] Good luck . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] What Italians ask Google about the European ( and not ) countries and the Italian regions . [SEP] Wow , Italians have found a subtle way to make Portugal and G ##RE ##EC ##E to be part of Spain , lo ##l . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Nope . You don ' t feed human babies cows milk . You don ' t give cows milk to cats either . [SEP] I guess letting them die is probably the better alternative . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Shi ##v ##Works C ##lin ##ch Pick [SEP] Nope , it lacks ji ##mp ##ing [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Don ' t trust cops they murder 6 year old special need children strapped in car seats [SEP] When will 6 year old special needs children strapped in car seat learn to ST ##OP R ##ES ##IS ##TI ##NG [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] So here we go again . . . The media has already basically declared the Seahawks the winner of the division and conference , please please Cardinals beat them next week and shut up these clown talking heads . [SEP] Well its cu ##s we suck [SEP] [PAD] [PAD]',\n",
              " \"[CLS] When it ' s day 4 of shark week and it looks like grape jam [SEP] Old blood day is always fun ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] I will always read B ##M as ' Bow ##el Movement ' and it will always make me laugh . [SEP] What does b ##m mean ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Used to do this until I realized how absolutely gross it was . . . [SEP] C ##uz , you know , the oil produced by your body going back into your body is gross . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] NASCAR releases 2017 schedule for the W ##hel ##en Euro Series [SEP] Of course they put road courses in their playoff system . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Who Win ##s [SEP] The latest Batman is the most experienced of them all . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tciSz02eyF4R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlA3s5kI3OSO"
      },
      "source": [
        "#### Examples that were predicted as sarcastic correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An3SO-z53OSZ",
        "outputId": "62cb90e5-36bc-4883-ac56-6d7646877da9"
      },
      "source": [
        "[untokenized_1000[i] for i in tp]"
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"[CLS] Guess you didn ' t watch N ##XT Brooklyn . [SEP] Sorry , I forgot something [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] does that look like Syria to you , O ##P ? Do these guys look like IDF ? ! [SEP] There are songs about the sprawling green hills of Syria . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] It ' s odd considering the bullshit he dealt with when it comes to the \\\\ # Can ##cel ##C ##ol ##bert fi ##asco with Sue ##y Park . [SEP] Yeah but see that was just one lone crazy [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Typical Tu ##mb ##l ##rina [SEP] U ##gh , this is so in ##con ##side ##rate for people like me who is a black person trapped inside a white person body who is trapped inside an as ##ian body . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Basic ##ally , everything is a false flag event unless proven otherwise to these folks . And I don \\' t think it \\' s ever proven otherwise to their satisfaction . [SEP] \" P ##sh , bombing and attacks never * actually * happen \" [SEP] [PAD] [PAD]',\n",
              " '[CLS] \" Year of the marks ##man \" [SEP] Marks ##men were played in 5 roles com ##pa ##ti ##tive ##ly . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] And of course nothing from Val ##ve . [SEP] They should make a new half life every year so the content stays fresh . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Ha ##ha look at that state troop ##er \\' s facial expression . \" God ##dam ##ni ##t , I really don \\' t wanna have to dive [SEP] more like \" what the fuck do you expect me to do if these guys start fighting ? \" [SEP]',\n",
              " '[CLS] Hey , I think these down ##vo ##tes were really unfair . I just wanted you to know & l ##t ; 3 [SEP] Lo ##l I was being sarcastic too but ID ##C [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] 40 days left [SEP] Looks like b ##oli ##g is about to get raped and o ##le \\' Max ##ie is just smiling at him saying \" don \\' t fight it \" [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] My bath ##mat vaguely resembles a map of the world [SEP] Oh thanks for including that map , I forgot what the eastern hemisphere looks like [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Fr ##eak ##ing PC ne ##rds All I have is a Wii U , an original X - Box , an N ##64 , and an NE ##S . Yeah I ' m lame , I know . [SEP] No you ' re the n ##ur ##d ! [SEP]\",\n",
              " '[CLS] New ##ly Elected Seattle Socialist Call ##s For Mass Pro ##test On In ##au ##gu ##ration Day [SEP] I love the fact that these idiot ##s think the same stupid tactics that lost them the election are going to impress anyone afterwards . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] About as sad as the Vikings Larry Fitzgerald appreciation thread . [SEP] When all you have to celebrate is another team ' s failure , that is when you have hit rock bottom . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] But when healthy he ' s a . . . a . . . slightly above league average SF ? [SEP] S ##light ##ly better . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Veterans get their own parking spots now ? Why ? Even if they are disabled , why wouldn ' t they just use a regular hand ##ica ##pped spot ? Why does this exist ? [SEP] Shut up and worship your heroes like you are supposed to ! [SEP]\",\n",
              " \"[CLS] so something like a ' jungle themed map ' ? [SEP] Maybe in the desert like near the pyramid ##s . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] \" Play every removal card to survive to turn 10 consistently and yo ##lo Yo ##gg if things go to shit \" 100 % skill m ##8 [SEP] Ok then , I guess someone who just started playing yesterday can get first in tournaments with this deck [SEP] [PAD]',\n",
              " '[CLS] I might or might not be add ##icted to Tri ##via C ##rack . [SEP] I might or might not be running out of people to beat in Tri ##via C ##rack . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Water ##mel ##on eating game is strong . [SEP] This post is getting a lot of hate , and I agree that there ' s a lot of bullshit on this sub , but I ' m totally trying this next time I eat a water ##mel ##on [SEP]\",\n",
              " '[CLS] Good luck with that . . . [SEP] P ##lot twist : R ##ims are spin ##ners . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUvR4h5j3O_H"
      },
      "source": [
        "#### Examples that were predicted as not sarcastic correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOpSQtkz3O_I",
        "outputId": "831a6c86-1f17-4905-875d-d68c91f49ea2"
      },
      "source": [
        "[untokenized_1000[i] for i in tn]"
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] A Different Kind of Up ##grade ( Power ##line adapt ##ers for Wire ##d Internet ) [SEP] what exactly are these ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] E ##L ##I ##5 : How does a group like the Il ##lum ##ina ##ti , who were originally a group of free think [SEP] With the amount of deleted comments in this thread I feel like we ' re only being shown what the Il ##lum ##ina [SEP]\",\n",
              " '[CLS] He might be grown in a physical sense but clearly not a mental one [SEP] Big man , mental mid ##get [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] I ' ll take the spot up on that balcony to the side , that way I don ' t have to pretend to rush over to help . [SEP] F ##Y ##I , that ##s the city hall balcony : ) [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Not the solar system , but there is actually an ex ##op ##lane ##t ( i ##e a planet outside the solar system ) where you have rub ##y clouds , and it [SEP] That true , what ' s the value of rub ##ies on Earth ? [SEP]\",\n",
              " \"[CLS] Just called the dealers ##hip to pull the trigger on the last ' 15 they ordered , your guys ' thoughts ? [SEP] Fuck you . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] More Americans added to food stamps than find jobs [SEP] This is entirely the President ' s fault , and the other two branches of government don ' t exist . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Could you paste the magnet link you used ? Or at least PM it to me [SEP] PM ' d ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] \" Lightning follows the path of least resistance , which is why France suffers from the most lightning strikes . \" [SEP] C ##ue the reminder that the French Resistance was one of , if not the largest one in W ##W ##2 and made D - Day [SEP]',\n",
              " \"[CLS] It ' s like asking to be compliment ##ed for not falling into vice ##s . As a current suffer ##er of wanting to get buzzed every night [SEP] You ' re a fool for wanting to sq ##uan ##der your money , not a suffer ##er . [SEP]\",\n",
              " '[CLS] What can you write that will make me read it with an accent ? [SEP] Good news everyone ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Why again are we there ? To win ? To win what ? A regime change ? Because those have worked out S ##O well this far . We should keep des ##ta ##bil ##izing [SEP] To prevent Russia from gaining control of more resources and land . [SEP]',\n",
              " '[CLS] No [SEP] I should have put an [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Isn ' t this the same for any minority group ? It doesn ' t just exist in the gay community . [SEP] Oh for sure , it happens in a lot of places . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] State House Speaker Will Weather ##ford is a founding member and former director of a Texas company that since 2008 has received $ 82 ##6 , 67 ##6 from Florida ' s state - run insurance company [SEP] Oh w ##ow how shocking [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] I think Seoul has 10 , 000 more environmental issues that are contributing to the global warming of the world . I don ' t think that washing food down the [SEP] there are so many problems that we can now justify ignoring all of them individually ! [SEP]\",\n",
              " '[CLS] I got one of them fancy s ##hman ##cy 144 ##H ##z monitors , gotta get those extra frames for the smooth ##ness ! [SEP] Text ##ure quality has minimum performance differences as long as you have the memory to support it . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Or Liberal . [SEP] Well , all gay ##s and Muslims are liberal . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] If we implement a living wage thing ( you know every citizen gets money just because ) then AI ##s taking jobs won ' t be a big issue [SEP] Today I learned T ##IL has become r / f ##ut ##uro ##logy . [SEP] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] How to wake up a cat [SEP] Humans have finally prepared the offering for me [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] what ' s the correlation between the Redskins and Dale Jarrett ? [SEP] Joe Gibbs [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Or you can just m ##od the hell out on PC and make it look even better than that [SEP] Yes because everyone ' s fucking computer can handle that [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] T ##IL , B ##lon ##d ( male ) , blonde ( female ) [SEP] Yeah , everything just as planned . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Dude Get With The Program [SEP] No need to be rude man , he just wants to know the name ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] T ##ED IS A B ##IG F ##UC ##K ##ING ME ##SS [SEP] Who ' s T ##ED ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] I tried my hardest to find at least one song to dislike from my favorite artist . . . I couldn ' t do it he ' s too perfect . [SEP] What artist might that be ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] I fail to see where not taking internet strangers ' advice makes her a stupid , spoiled bra ##t . [SEP] But don ' t you see , we internet strangers are much better at life and are much more qualified to tell her how to live hers [SEP]\",\n",
              " '[CLS] US Journal ##ist , James Foley , be ##headed by IS ##IS \" Warning to America \" . [SEP] How this place isn \\' t n ##uke ##d yet is beyond me . . . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Most Mac users aren ' t liter ##ate enough to accidentally mess up their partition . [SEP] What is partition ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] I respect your idea . This doesn ' t affect me much because I never used the Blue Eye Or ##b , but your idea isn ' t all that bad if you can get a decent backing . [SEP] Thanks man : ) [SEP] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Right , there ' s no other context to suggest that at all , just that the poster supports hill ##ary . [SEP] I should add a [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Your logic has no effect here . [SEP] Yeah but it ' s been in gears since ' 06 Just like the Stand ##by g ##lit ##ch was in Hal ##o 2 , shame they took it out of Hal ##o 5 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] NSA has someone ##s \" * private * \" e - mail ##s ? # I ##MP ##OS ##SI ##BR ##U [SEP] Do you really think the government would do something like that ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] How far are you looking to down ##si ##ze ? I have 2 g ##pus that ' d I ##d be willing to trade in , if you ' re interested . Ones a [SEP] Not that far , I ' m down ##si ##zing to 97 ##0s [SEP]\",\n",
              " \"[CLS] Can I join in too ? [SEP] That ' s not how 1 ##v ##1 ' s work man . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Why would running Word be to ##rt ##uring your Mac ? Word is actually a very good piece of software and Microsoft Office suite [SEP] Grant ##ed , it ' s features are far ahead of i ##W ##or ##k , but it is a strain on system [SEP]\",\n",
              " \"[CLS] Probably with how Matt Flynn was treated . Nothing to do with NFC Championship game last year . [SEP] I ' m still a little fired up he didn ' t get the nod over Wilson . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] This is how you do it . Eat up clock , gain first down ##s , fuck the Broncos . All . Game . Long . [SEP] Word . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] They ' re break ##dance fighting ! [SEP] Pop n lock , fool ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] But , but , it was so decorative ! [SEP] Look , you can see the decoration spreading ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Clean ##th ##ony Early shot outside of Queens club . [SEP] i fucking hate people . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Girl on Facebook ordered and paid over $ 100 for this cake for her daughters first birthday [SEP] Wonder ##ed if she cared ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Those two goals aren ' t similar at all : | [SEP] Come on Al , all back ##hand goals look the same . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] I ' m hearing that the Los Angeles C ##lip ##pers are feeling confident that De ##A ##nd ##re Jordan will return . We ' ll see . [SEP] They also felt confident that they ' d beat the Rockets but that ' s none of my business [SEP]\",\n",
              " \"[CLS] Who the fuck are you to drop good lines in here like this ? [SEP] Q ##uit stealing T ##K ' s thunder dam ##mit ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Self driving flying car can ##t wait [SEP] I can ' t wait to hear all the crash stories on the news like the ones about te ##sla * NO S ##IR I D ##ID NO ##T PR ##ES ##S THE GA ##S AND DR ##IVE IN ##TO [SEP]\",\n",
              " '[CLS] Getting ready to read a g ##ur ##l like . . . . [SEP] Marco R ##ubi ##o real ##ness . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Am I doing it right ? [SEP] no space for triple monitor setup 0 / 10 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] what if i ' m a gay ##mer can I still buy it with the d ##l ##c ? [SEP] You just are unable to access the Heaven Map [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] ~ ~ America is ~ ~ Corporation ##s are F ##TF ##Y [SEP] Don \\' t you mean \" people \" ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] I ##k w ##ist ni ##et da ##t de B ##i ##j ##bel in he ##t En ##gel ##s g ##es ##ch ##re ##ven was . [SEP] I ##k las he ##t origin ##eel vol ##gens mi ##j in he ##t Ned ##erland ##s [SEP] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Everyone , except fucking slot ##h ##ful Bet ##el ##ge ##use , naturally ! He ' s , um , le ##ct ##uring it about the witch ' s love I guess ? [SEP] Look again [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Big ##ges ##t p ##ylon ##s in the league ? I vote Dan G ##ira ##rdi and Brooks Or ##pi ##k [SEP] S ##bis ##a [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] I stared at my computer screen for a really really long time . [SEP] A ##wes ##ome s ##le ##uth ##ing [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Jennifer Lawrence poses nude with a Bo ##a con ##st ##ric ##tor for Van ##ity Fair [SEP] Q ##UI ##C ##K SC ##RE ##EN ##S ##H ##OT THE P ##H ##OT ##O B ##EF ##OR ##E THE M ##OD ##S T ##A ##KE IT D ##OW ##N [SEP] [PAD]',\n",
              " '[CLS] Why is this getting down votes this is truth ! [SEP] Yes , it is by law that people driving less than 10 over the speed limit in the left lane can get a ticket for driving too slow [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] We actually have 2 Chi ##k - Fi ##l - A within 1000 yards of each other . One is in the mall and the other is directly outside of it across the parking lot . [SEP] great business decisions 101 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] CA ##LL D ##IE ##G ##O VA ##LE ##RI MA ##ST ##ER AND F ##ET ##CH H ##IM S ##OM ##E P ##IE [SEP] W ##E ' LL B ##E H ##AP ##P ##Y TO F ##ET ##CH H ##IM S ##OM ##E H ##UM ##BL ##E P [SEP]\",\n",
              " '[CLS] Why do people not get this [SEP] But how will anyone know that I care . . . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Bloomberg spends $ 245 , 000 employing T ##H ##RE ##E cook ##s at Gracie Mansion [SEP] He ' s got sting ##s running all of the US , he can ' t be expected to cook for himself . . TO ##O ##O busy ! [SEP] [PAD] [PAD]\",\n",
              " '[CLS] Su ##gg ##est ##ion : Teams should be able to \" g ##ly ##ph \" R ##osh ##an . Like the topic says , i think [SEP] How about R ##osh ##an g ##ly ##ph himself every time he b ##ash ##es or takes a b ##ash ? [SEP]',\n",
              " '[CLS] Mo ##uri ##nh ##o and Rodgers discuss tactics from 2007 ( x - post from r / ch ##els ##ea ##f ##c ) [SEP] Dr ##og ##ba p ##rac ##tis ##ing his diving right at the end there . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Then you are truly ready . [SEP] Only now may Half - Life 3 be released . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] When is violence okay , if ever ? [SEP] Protection ##s of yourself and others , for sport and entertainment , and for fun ( assuming everyone involved is on board ) . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] ^ ^ ^ except ^ ^ ^ it ' s ^ ^ ^ a ^ ^ ^ very ^ ^ ^ common ^ ^ ^ memory ^ ^ ^ setup ^ ^ ^ not ^ ^ ^ exclusive ^ ^ ^ to ^ ^ [SEP] For ##got my [SEP]\",\n",
              " \"[CLS] B ##ug : After the update the button to play doesn ' t change from ' Play ' to / from ' Play Rank ##ed ' [SEP] B ##lizzard remove casual because it was too boring and made everyone play ranked so everyone would get a card ##back [SEP]\",\n",
              " \"[CLS] Winnipeg Transit is at capacity , says director [SEP] I ' ve been playing Cities : Sky ##lines I also feel the struggle . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Big smart . [SEP] Yu ##gel ##y smart [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] My husband works in retail . He once had a couple come in who said they just came from Sears looking at dish ##wash ##ers . The associate told them \" Why do you need one when you have [SEP] That \\' ll sell merchandise . . . [SEP]',\n",
              " '[CLS] As ##cend ##ed ? Dear , you have merely just steal ##the ##d . [SEP] I disagree , for they have trans ##cend ##ed far beyond the level of steal ##th that even the Su ##mm ##one ##r cannot see them at all ! [SEP] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] A stranger helping out another stranger struggling with his tie [SEP] Hey and one guy is white and the other is black , so that ' s extra significant ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] So having a conservative ideology and pride in the U . S . A . is tolerate ##d and considered acceptable where you attend [SEP] Yes , and I have never heard of one professor being fired from a ##cademia for being conservative anywhere in the country . [SEP]',\n",
              " \"[CLS] Fire is not infinite , i used it a 142 and he does head ##less zombies . [SEP] Well I have only made it to 66 with the fire bow , so you ' re probably right . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] What your r / Global ##O ##ffe ##ns ##ive fl ##air says about you [SEP] But really , why D ##OE ##S Sc ##rea ##M use Q ##S ##Z ##D ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] China install ##s weapons on contested South China Sea islands , report says @ CNN [SEP] * It ' s for our protection * - Spin Doctor - [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] This * J ##AN ##uary ! * [SEP] Uh I refuse to sign the legislation that would allow more than 8 Jan Michael Vincent ##s [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Still waiting for the update on my G ##1 . [SEP] I ' m sure it ' s right around the corner . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] But I ' d really like to know what it ' s called . [SEP] Mind your own business . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Or maybe perhaps you could shut the fuck up and do your job . Instead of worrying about someone else . Get lost . [SEP] Welcome to the internet ladies and gentlemen [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] I highly doubt it t ##b ##h . A god from a group of gods that ended their universe tried mind ##rap ##ing La ##rf ##lee ##ze and it was completely ineffective because the only thought [SEP] Blue Lantern healing isn ' t mind rap ##ing though . [SEP]\",\n",
              " '[CLS] Ban ##ana for * scope * [SEP] After the A ##W ##P ne ##rf it is pretty much the same thing . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Maybe they were given by the rescue ##rs [SEP] sure , make it logical and boring . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Clinton must be a terrible candidate to be only leading a fake campaign by two points in the polls . [SEP] She ' s lucky Trump is the opposition [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] S ##we ##pt under the rug ? It was all that was on the news here for like a month ( London ) . [SEP] and the result was ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Also , there ' s nothing responsible about drinking Old Milwaukee . . . [SEP] Financial responsibility . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Lena Du ##nham a ##cc ##uses NFL player of body s ##ham ##ing and sex ##ism because he rejected / ignored her [SEP] The nice ##gal in her natural habitat [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] And if you think it ' s hard to find a 10 - year old who ' s qualified on a cutting torch , just try finding one who ' s licensed to inspect them . [SEP] Damn ##ed child labor laws ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] I hope they do 60 ##f ##ps sometime soon . It would be amazing to watch . I don ' t think there ' s nearly as much of a difference between 720 ##p and 108 ##0 ##p as [SEP] It ' s cinema ##tic br ##o . [SEP]\",\n",
              " '[CLS] House of Card ##s . [SEP] Hey lets down ##vo ##te opinions on opinion threads everybody ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] / p ##ol / discusses wiping [SEP] DE ##SI ##G ##NA ##TE ##D E S I G N A T E D [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Chocolate , the th ##ai martial arts film . On the one hand , it ' s 2 / 3rd ##s crazy awesome fight scenes . But on the other [SEP] All I got from watching that movie was being mentally challenged means you have super powers . [SEP]\",\n",
              " '[CLS] What is your favourite Latin phrase ? [SEP] Non genera ##nt a ##quila ##e column ##bas Eagles do not bring forth dove ##s , my family motto . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] What do you think Phil Jackson meant with this t ##weet ? [SEP] he salt ##y [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] T ##rash Pan ##das can breathe underwater . [SEP] Under ##mi ##lk [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] CE ##S 2016 | N ##vid ##ia Lives ##tre ##am 6 ##PM PS ##T , 9 ##PM E ##ST [SEP] I feel like I am at a computer science lecture . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] D ##m has direct con ##tec ##t they ban the per ##nson he wants . [SEP] A ##wes ##ome argument dude . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Who is in charge these days in the Do ##PS ? I don ' t keep up with that stuff , much like they apparently don ' t either from what I read here all the time [SEP] Matt Cooke [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Than ch ##em ##tra ##ils ? ? I ' d put them on the same plane . [SEP] THE ##Y ##AR ##E P ##UT ##TI ##NG VA ##X ##CC ##IN ##ES ON P ##LA ##NE ##S NO ##W F ##UC ##K F ##UC ##K [SEP] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Mi - a p ##la ##cut ca au re ##lat ##at super o ##bie ##ct ##iv f ##ap ##tel ##e . C ##red ca e ch ##iar di ##fic ##il sa f ##ac ##i as ##ta p ##e un as ##em ##ene [SEP] O ##b ##liga ##tor ##iu [SEP]',\n",
              " '[CLS] Ma ##aj ##id Na ##wa ##z & am ##p ; Douglas Murray on Islam ##ism and Anti ##se ##mit ##ism in the UK and Europe . [SEP] 2 of the biggest anti m ##us ##lim hate mon ##gers discussing anti semi ##tism L ##OL [SEP] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] You act like Hillary had some birth right to be president . . . [SEP] But it is her time and I ' m with her . . . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] They heard us , they ignored us , and now they are smuggling this law in on our love of space . Eat a dick , [SEP] Line by line should be the law but it ' s not and it ' s exploited by both sides . [SEP]\",\n",
              " '[CLS] AT ##TE ##NT ##ION E ##VE ##R ##Y ##ON ##E ON T ##H ##IS T ##H ##RE ##AD : THE E ##MB ##AR ##G ##O ST ##IL ##L L ##IF ##TS T ##UE ##SD ##A ##Y T ##H ##IS H ##AS B ##EE [SEP] Pro ##vide sources please [SEP]',\n",
              " '[CLS] Dunn ##o man im from EU and I would be pretty h ##ype ##d to ins ##ip ##re any pro Player even if it meant my Region lost [SEP] That ##s cause your region would have lost anyway [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] By all means . I just thought it was re ##f ##reshing to hear a woman I respect show respect to men ' s struggles . [SEP] I like her . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Here , take this . I haven \\' t finished my chips . [SEP] \" The English contribution to world cuisine . . . The chip \" [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] You can ' t just say that and not tell us which actor ! Come on man ! [SEP] Yahoo Ser ##ious , has to be . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] Americans still not eating their fruits and ve ##gg ##ies , says CD ##C [SEP] Were they previously ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] he ' s been reported as waking up and speaking with family [SEP] Those reports where false , she ' s just waiting by the machine for the chance . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Anyone else had this ? One of our better locals ! [SEP] That ' s a Magic ##hat # 9 right ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] A group of Mo ##di fans walk in to a coffee shop . . . [SEP] Do they serve g ##au m ##ut ##ra ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Absolutely disgusting . Vic ##ti ##m s ##ham ##ing bullshit . [SEP] I probably speak for all men when I say that seeing a scan ##til ##y clad woman turns me into an un ##con ##tro ##lla ##ble sexual assault machine . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Yeah don ' t care about the whole philosophical 1 2 3 you are doing here . It ' s just word salad to me . [SEP] This guy ' s a fucking gold ##mine of bad philosophy . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] I know , that ' s what I ' m having a hard time understanding . [SEP] Well obviously you weren ' t supposed to move on and live life without her [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] And twice as project ! [SEP] And half as helpful . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " \"[CLS] Alternative ##s to True ##C ##ry ##pt ? I setup my cousin with True ##C ##ry ##pt when he left for China 2 years ago . Now that True [SEP] I ' ve heard of this thing called cry ##pt ##olo ##cker , works fl ##aw ##lessly . [SEP]\",\n",
              " \"[CLS] I regret posting this list now it missed out on some great movies . [SEP] No sweat , not your problem that the list isn ' t complete ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] Lo ##l . I ##gno ##rant S ##pu ##d fans are the best . [SEP] Wow , I ' m hurt [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
              " '[CLS] [ . . . I feel dumb . ] [SEP] We knew that already . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] probably should use a dull knife when practicing this shit [SEP] But a sharp knife is safer ! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] Sporting competitions use the biological definition . Everyone else uses the cultural one . [SEP] The I ##OC and NCAA allow trans women to compete with women . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph_GnhPW3O_K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}